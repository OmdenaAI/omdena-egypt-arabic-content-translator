{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Try on arabic-words code\n### challenges:\n1. Lack of parallel corpus for english to arabic DS contenct\n\n2. Large dataset needed for finetuning\n\nlet's start with coursera, and gradually build on it.","metadata":{"id":"rnQZs0Ac6bE7"}},{"cell_type":"markdown","source":"# MT5 background\nWe'll be using MT5.\nI chose this model because it has a relatively small number of parameters (compared to other pre-trained seq2seq models, like M2M), so it would be compatible for training on Kaggle.\n\nmT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.\nThe mT5 model was introduced back in 2020 as the multilingual rightful heir of the T5 model. The m stands for multilingual.\n\nBoth mT5 and T5 were trained in similar fashion. The only difference was that mT5 was trained on multi-lingual data, and had vastly more token embeddings (250k). Both were initially trained on the objective of span-corruption: “consecutive spans of input tokens are replaced with a mask token and the model is trained to reconstruct the masked-out token”.\n\nThe dataset used for training the model had 6.3 Trillion tokens of 107 languages. ","metadata":{"id":"2VoBtJnK-Ong"}},{"cell_type":"markdown","source":"# Getting libraries\n\nTransformers and simpletransformers: Huggingface transformers is the most popular NLP library as to date. It requires minimal to no effort to fine-tune state-of-the-art transformer-based models on tasks such as classification, text generation and summarization. Simpletransformers is just a small library built on top of it to speed up prototyping and testing.","metadata":{}},{"cell_type":"code","source":"# !pip install transformers\n!pip install simpletransformers\n\n# #For tokenization\n# !pip install sentencepiece ","metadata":{"id":"zHfPrJ7e4zWk","outputId":"44d8aeeb-86af-4834-d5f4-2e759a0d0965","execution":{"iopub.status.busy":"2021-12-09T19:54:20.39071Z","iopub.execute_input":"2021-12-09T19:54:20.391031Z","iopub.status.idle":"2021-12-09T19:54:34.130482Z","shell.execute_reply.started":"2021-12-09T19:54:20.390935Z","shell.execute_reply":"2021-12-09T19:54:34.129669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:54:34.134102Z","iopub.execute_input":"2021-12-09T19:54:34.134327Z","iopub.status.idle":"2021-12-09T19:54:34.790432Z","shell.execute_reply.started":"2021-12-09T19:54:34.134299Z","shell.execute_reply":"2021-12-09T19:54:34.789556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport os\n# from google.colab import drive\nimport logging\n#The T5Model class is used for any NLP task performed with a T5 model or a mT5 model.\nfrom simpletransformers.t5 import T5Model, T5Args","metadata":{"id":"de_7ogPp6g65","execution":{"iopub.status.busy":"2021-12-09T19:54:34.792917Z","iopub.execute_input":"2021-12-09T19:54:34.793194Z","iopub.status.idle":"2021-12-09T19:54:42.636973Z","shell.execute_reply.started":"2021-12-09T19:54:34.793157Z","shell.execute_reply":"2021-12-09T19:54:42.636102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls '/kaggle/input'","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:54:42.639204Z","iopub.execute_input":"2021-12-09T19:54:42.639417Z","iopub.status.idle":"2021-12-09T19:54:43.304877Z","shell.execute_reply.started":"2021-12-09T19:54:42.639391Z","shell.execute_reply":"2021-12-09T19:54:43.304023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/mt5finetuning/train_1.csv')","metadata":{"id":"E0fXBW8UQk8O","execution":{"iopub.status.busy":"2021-12-09T19:54:43.306756Z","iopub.execute_input":"2021-12-09T19:54:43.307049Z","iopub.status.idle":"2021-12-09T19:54:44.158539Z","shell.execute_reply.started":"2021-12-09T19:54:43.307011Z","shell.execute_reply":"2021-12-09T19:54:44.157807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"id":"hq1H9R4KL-Iu","outputId":"1bd1d4d0-c49d-40bd-ee7c-940d598450fc","execution":{"iopub.status.busy":"2021-12-09T19:54:44.159723Z","iopub.execute_input":"2021-12-09T19:54:44.159979Z","iopub.status.idle":"2021-12-09T19:54:44.177878Z","shell.execute_reply.started":"2021-12-09T19:54:44.159947Z","shell.execute_reply":"2021-12-09T19:54:44.17698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"id":"tvNbl1dmMoMs","outputId":"35316756-917b-4271-bc63-1bbc10153f13","execution":{"iopub.status.busy":"2021-12-09T19:54:44.178989Z","iopub.execute_input":"2021-12-09T19:54:44.179375Z","iopub.status.idle":"2021-12-09T19:54:44.185212Z","shell.execute_reply.started":"2021-12-09T19:54:44.17934Z","shell.execute_reply":"2021-12-09T19:54:44.184493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum() / train.shape[0] *100","metadata":{"id":"8DJIxtU8MBjW","outputId":"562e162d-2a00-44dd-cc7c-52baefbc65bd","execution":{"iopub.status.busy":"2021-12-09T19:54:44.186716Z","iopub.execute_input":"2021-12-09T19:54:44.187207Z","iopub.status.idle":"2021-12-09T19:54:44.203389Z","shell.execute_reply.started":"2021-12-09T19:54:44.187169Z","shell.execute_reply":"2021-12-09T19:54:44.202688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### drop na","metadata":{"id":"ToBwtWuYGvea"}},{"cell_type":"code","source":"train.dropna(inplace=True)","metadata":{"id":"Ipom_4ZpL21e","execution":{"iopub.status.busy":"2021-12-09T19:54:44.204902Z","iopub.execute_input":"2021-12-09T19:54:44.205204Z","iopub.status.idle":"2021-12-09T19:54:44.219899Z","shell.execute_reply.started":"2021-12-09T19:54:44.205166Z","shell.execute_reply":"2021-12-09T19:54:44.218967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum() / train.shape[0] *100","metadata":{"id":"de_bnqUvMuZf","outputId":"e398787b-7837-48aa-cb70-b450d1ffd652","execution":{"iopub.status.busy":"2021-12-09T19:54:44.223735Z","iopub.execute_input":"2021-12-09T19:54:44.22436Z","iopub.status.idle":"2021-12-09T19:54:44.23737Z","shell.execute_reply.started":"2021-12-09T19:54:44.22433Z","shell.execute_reply":"2021-12-09T19:54:44.236564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain = train.drop_duplicates(subset=['AR', 'EN'])\n# train['Arabic_transcript'] = train.apply(lambda row: (row.Arabic_transcript).strip().lower(), axis=1)\ntrain['EN'] = train.apply(lambda row: row.EN.lower(), axis=1)\ntrain = train[[\"AR\", \"EN\"]]\n","metadata":{"id":"hSSwZpcPK8Ti","execution":{"iopub.status.busy":"2021-12-09T19:54:44.238807Z","iopub.execute_input":"2021-12-09T19:54:44.23909Z","iopub.status.idle":"2021-12-09T19:54:44.692608Z","shell.execute_reply.started":"2021-12-09T19:54:44.239052Z","shell.execute_reply":"2021-12-09T19:54:44.691868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finetuning mt5 was computationally challenging. I had to reduce the sequence length to 128, which means that each input sentence should't exceed 128 words (to avoid truncating sentences and performance degredation)","metadata":{}},{"cell_type":"markdown","source":"# Dataset format","metadata":{}},{"cell_type":"markdown","source":"The library requires dataset to be in the format of a Pandas dataframe, with three columns: input_text, target_text, and prefix. Prefix is a column used during the training of mT5 to specify the task the model should do (summarize, classify …). We won’t need it for our case, we create it and leave it blank “”.\n\nNote that we are casting all the data in the Dataframe as strings. This is because mT5 is a sequence-to-sequence model which expects all inputs and outputs to be text sequences. If we have numeric values (or any other non-string values), we’ll run into errors during training.","metadata":{}},{"cell_type":"code","source":"train.columns = ['target_text', 'input_text']","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:54:44.693925Z","iopub.execute_input":"2021-12-09T19:54:44.694172Z","iopub.status.idle":"2021-12-09T19:54:44.699015Z","shell.execute_reply.started":"2021-12-09T19:54:44.694141Z","shell.execute_reply":"2021-12-09T19:54:44.698356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### get number of sentences","metadata":{"id":"N_pEYCnENsgc"}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize, sent_tokenize\nimport nltk","metadata":{"id":"DcuzGdSwNupk","execution":{"iopub.status.busy":"2021-12-09T19:54:44.700585Z","iopub.execute_input":"2021-12-09T19:54:44.701153Z","iopub.status.idle":"2021-12-09T19:54:45.653714Z","shell.execute_reply.started":"2021-12-09T19:54:44.701116Z","shell.execute_reply":"2021-12-09T19:54:45.652911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')","metadata":{"id":"E_w47h55ORWj","outputId":"44c54077-949b-401d-d6c8-9ab78c14cdda","execution":{"iopub.status.busy":"2021-12-09T19:54:45.655031Z","iopub.execute_input":"2021-12-09T19:54:45.655275Z","iopub.status.idle":"2021-12-09T19:54:45.783119Z","shell.execute_reply.started":"2021-12-09T19:54:45.655243Z","shell.execute_reply":"2021-12-09T19:54:45.782225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['input_text_sent'] = train['input_text'].apply(sent_tokenize).tolist()","metadata":{"id":"Yh-Bl5KVN_R6","execution":{"iopub.status.busy":"2021-12-09T19:54:45.784703Z","iopub.execute_input":"2021-12-09T19:54:45.785188Z","iopub.status.idle":"2021-12-09T19:54:45.788638Z","shell.execute_reply.started":"2021-12-09T19:54:45.785151Z","shell.execute_reply":"2021-12-09T19:54:45.787954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['input_text_sent_ar'] = train['target_text'].apply(sent_tokenize).tolist()","metadata":{"id":"HpB0pxUA7ZbE","execution":{"iopub.status.busy":"2021-12-09T19:54:45.790125Z","iopub.execute_input":"2021-12-09T19:54:45.790629Z","iopub.status.idle":"2021-12-09T19:54:45.79822Z","shell.execute_reply.started":"2021-12-09T19:54:45.790595Z","shell.execute_reply":"2021-12-09T19:54:45.797441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['input_text_sent_ar'][0]","metadata":{"id":"eVzc1k9l7SFp","execution":{"iopub.status.busy":"2021-12-09T19:54:45.799315Z","iopub.execute_input":"2021-12-09T19:54:45.800705Z","iopub.status.idle":"2021-12-09T19:54:45.811078Z","shell.execute_reply.started":"2021-12-09T19:54:45.800666Z","shell.execute_reply":"2021-12-09T19:54:45.810177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['input_text_sent'][0]","metadata":{"id":"RK6x_Q7GOUBE","execution":{"iopub.status.busy":"2021-12-09T19:54:45.812981Z","iopub.execute_input":"2021-12-09T19:54:45.813587Z","iopub.status.idle":"2021-12-09T19:54:45.819024Z","shell.execute_reply.started":"2021-12-09T19:54:45.813546Z","shell.execute_reply":"2021-12-09T19:54:45.81814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count the number of sentences in the entire text corpus.","metadata":{}},{"cell_type":"code","source":"import string\nnsentences = train['input_text'].str.split('.').map(len).sum()\n# nsentences = train['input_text'].count()","metadata":{"id":"3qOnUSzTOaqI","execution":{"iopub.status.busy":"2021-12-09T19:54:45.820948Z","iopub.execute_input":"2021-12-09T19:54:45.821596Z","iopub.status.idle":"2021-12-09T19:54:45.86253Z","shell.execute_reply.started":"2021-12-09T19:54:45.821558Z","shell.execute_reply":"2021-12-09T19:54:45.861734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nsentences","metadata":{"id":"GgiJDo8ZOpYn","outputId":"d838f854-4210-4d94-d815-2e8b68193ba3","execution":{"iopub.status.busy":"2021-12-09T19:54:45.863735Z","iopub.execute_input":"2021-12-09T19:54:45.864605Z","iopub.status.idle":"2021-12-09T19:54:45.869901Z","shell.execute_reply.started":"2021-12-09T19:54:45.864567Z","shell.execute_reply":"2021-12-09T19:54:45.869218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nnsentences = train['target_text'].str.split('.').map(len).sum()\n# nsentences = train['input_text'].count()","metadata":{"id":"41-453PGPSaT","execution":{"iopub.status.busy":"2021-12-09T19:54:45.871065Z","iopub.execute_input":"2021-12-09T19:54:45.871841Z","iopub.status.idle":"2021-12-09T19:54:45.906733Z","shell.execute_reply.started":"2021-12-09T19:54:45.871787Z","shell.execute_reply":"2021-12-09T19:54:45.905752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nsentences","metadata":{"id":"DR3GgItpPX3h","outputId":"381d7a08-2b26-4090-85bf-b40d770c0b8a","execution":{"iopub.status.busy":"2021-12-09T19:54:45.908237Z","iopub.execute_input":"2021-12-09T19:54:45.908472Z","iopub.status.idle":"2021-12-09T19:54:45.915402Z","shell.execute_reply.started":"2021-12-09T19:54:45.908438Z","shell.execute_reply":"2021-12-09T19:54:45.914553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['input_text'].head(1)","metadata":{"id":"sj8ddLhKNWVI","outputId":"584d1190-8cb0-4f40-f615-790790242724","execution":{"iopub.status.busy":"2021-12-09T19:54:45.917396Z","iopub.execute_input":"2021-12-09T19:54:45.917688Z","iopub.status.idle":"2021-12-09T19:54:45.92804Z","shell.execute_reply.started":"2021-12-09T19:54:45.917626Z","shell.execute_reply":"2021-12-09T19:54:45.92721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## count the number of words in each example","metadata":{}},{"cell_type":"code","source":"train['count_words_ar'] = train['target_text'].apply(lambda row: len(word_tokenize(row)))\n# train['article_len'] = train['target_text'].apply(lambda row: len(word_tokenize(row)))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:54:45.93062Z","iopub.execute_input":"2021-12-09T19:54:45.930963Z","iopub.status.idle":"2021-12-09T19:55:03.266457Z","shell.execute_reply.started":"2021-12-09T19:54:45.930927Z","shell.execute_reply":"2021-12-09T19:55:03.265707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['count_words_en'] = train['input_text'].apply(lambda row: len(word_tokenize(row)))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:55:03.268143Z","iopub.execute_input":"2021-12-09T19:55:03.269109Z","iopub.status.idle":"2021-12-09T19:55:22.854019Z","shell.execute_reply.started":"2021-12-09T19:55:03.269036Z","shell.execute_reply":"2021-12-09T19:55:22.853134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['count_words_en'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:55:22.856465Z","iopub.execute_input":"2021-12-09T19:55:22.857021Z","iopub.status.idle":"2021-12-09T19:55:22.869796Z","shell.execute_reply.started":"2021-12-09T19:55:22.856965Z","shell.execute_reply":"2021-12-09T19:55:22.868979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['count_words_en'].shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:55:22.871384Z","iopub.execute_input":"2021-12-09T19:55:22.871874Z","iopub.status.idle":"2021-12-09T19:55:22.877408Z","shell.execute_reply.started":"2021-12-09T19:55:22.871837Z","shell.execute_reply":"2021-12-09T19:55:22.876573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"14955 - 14325","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:55:22.883894Z","iopub.execute_input":"2021-12-09T19:55:22.884364Z","iopub.status.idle":"2021-12-09T19:55:22.89007Z","shell.execute_reply.started":"2021-12-09T19:55:22.884313Z","shell.execute_reply":"2021-12-09T19:55:22.889159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['count_words_en'] <=128]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:55:22.891652Z","iopub.execute_input":"2021-12-09T19:55:22.892207Z","iopub.status.idle":"2021-12-09T19:55:22.909357Z","shell.execute_reply.started":"2021-12-09T19:55:22.892167Z","shell.execute_reply":"2021-12-09T19:55:22.908718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['count_words_ar'] <=128]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:55:22.9106Z","iopub.execute_input":"2021-12-09T19:55:22.911018Z","iopub.status.idle":"2021-12-09T19:55:22.92604Z","shell.execute_reply.started":"2021-12-09T19:55:22.910983Z","shell.execute_reply":"2021-12-09T19:55:22.925392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_128 = train[(train['count_words_ar'] <=128) | (train['count_words_en'] <=128)]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:55:22.927468Z","iopub.execute_input":"2021-12-09T19:55:22.927929Z","iopub.status.idle":"2021-12-09T19:55:22.934877Z","shell.execute_reply.started":"2021-12-09T19:55:22.927895Z","shell.execute_reply":"2021-12-09T19:55:22.934122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_128.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T19:55:22.935913Z","iopub.execute_input":"2021-12-09T19:55:22.936541Z","iopub.status.idle":"2021-12-09T19:55:22.943359Z","shell.execute_reply.started":"2021-12-09T19:55:22.936478Z","shell.execute_reply":"2021-12-09T19:55:22.942595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I can first try to train on lengths < 128","metadata":{}},{"cell_type":"markdown","source":"### create val. data","metadata":{"id":"zM5dGtatCvzm"}},{"cell_type":"code","source":"val = train_128.sample(frac = 0.05)\ntrain_128 = train_128.drop(index = val.index).astype(str)","metadata":{"id":"O1ZgzRkH8Qx3","execution":{"iopub.status.busy":"2021-12-09T19:55:22.944691Z","iopub.execute_input":"2021-12-09T19:55:22.945583Z","iopub.status.idle":"2021-12-09T19:55:22.989334Z","shell.execute_reply.started":"2021-12-09T19:55:22.94554Z","shell.execute_reply":"2021-12-09T19:55:22.988661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val.info()","metadata":{"id":"4j9PF6Wl8e6c","outputId":"a73de973-9a82-496a-f1e8-7f8ac86ea5cf","execution":{"iopub.status.busy":"2021-12-09T19:55:22.990604Z","iopub.execute_input":"2021-12-09T19:55:22.991047Z","iopub.status.idle":"2021-12-09T19:55:23.016942Z","shell.execute_reply.started":"2021-12-09T19:55:22.991013Z","shell.execute_reply":"2021-12-09T19:55:23.012353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.drop(columns = ['input_text_sent', 'input_text_sent_ar'], inplace=True)","metadata":{"id":"En4QvquY5E5s","execution":{"iopub.status.busy":"2021-12-09T19:55:23.018764Z","iopub.execute_input":"2021-12-09T19:55:23.01933Z","iopub.status.idle":"2021-12-09T19:55:23.026081Z","shell.execute_reply.started":"2021-12-09T19:55:23.019291Z","shell.execute_reply":"2021-12-09T19:55:23.025318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['INPUT_ len'] = train['target_text'].apply(lambda row: len(word_tokenize(row)))","metadata":{"id":"qorcpC8t-jyQ","execution":{"iopub.status.busy":"2021-12-09T19:55:23.027233Z","iopub.execute_input":"2021-12-09T19:55:23.028998Z","iopub.status.idle":"2021-12-09T19:55:23.037746Z","shell.execute_reply.started":"2021-12-09T19:55:23.028962Z","shell.execute_reply":"2021-12-09T19:55:23.034814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['INPUT_ len'].describe()","metadata":{"id":"NLH_4r1i-z3H","execution":{"iopub.status.busy":"2021-12-09T19:55:23.042963Z","iopub.execute_input":"2021-12-09T19:55:23.043191Z","iopub.status.idle":"2021-12-09T19:55:23.049641Z","shell.execute_reply.started":"2021-12-09T19:55:23.043161Z","shell.execute_reply":"2021-12-09T19:55:23.048768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### split into sentences according to the max length= 100","metadata":{"id":"7EIBOqoo_dyh"}},{"cell_type":"code","source":"","metadata":{"id":"Qhi1synj_kPv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{"id":"FtINhOca80h1"}},{"cell_type":"markdown","source":"## 1. mt5-small - 300M parameters (small xD)","metadata":{"id":"J0kWvou8-CJg"}},{"cell_type":"markdown","source":"51M parameters","metadata":{"id":"sTe9haaeUuPQ"}},{"cell_type":"markdown","source":"###Sidenote on GPU memory usage\nThe amount of GPU memory required to train a Transformer model depends on many different factors (maximum sequence length, number of layers, number of attention heads, size of the hidden dimensions, size of the vocabulary, etc.). Out of these, the maximum sequence length of the model is one of the most significant.\nAlso, mT5 has a much larger vocabulary than T5 (~250,000 tokens to ~32,000 tokens), contributing to mT5 being quite punishing in terms of GPU memory required.","metadata":{"id":"uxFes4ACEoFr"}},{"cell_type":"markdown","source":"","metadata":{"id":"h-Kum6iUPoi2"}},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"id":"TjHGGSch5zwZ","execution":{"iopub.status.busy":"2021-12-09T19:55:23.055398Z","iopub.execute_input":"2021-12-09T19:55:23.055835Z","iopub.status.idle":"2021-12-09T19:55:23.06406Z","shell.execute_reply.started":"2021-12-09T19:55:23.055794Z","shell.execute_reply":"2021-12-09T19:55:23.063365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n# del variables\ngc.collect()","metadata":{"id":"bHis6-Kx54pF","outputId":"1e417f34-327a-4b5f-b816-5a3f66086cd3","execution":{"iopub.status.busy":"2021-12-09T19:55:23.064978Z","iopub.execute_input":"2021-12-09T19:55:23.065211Z","iopub.status.idle":"2021-12-09T19:55:23.378455Z","shell.execute_reply.started":"2021-12-09T19:55:23.065178Z","shell.execute_reply":"2021-12-09T19:55:23.377664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ok let's train on a max length of 1024, then split text into sentences.\nBut a big seq_length could cause out of memory issues.\n\nAnother approach is to split each article into sentences. However, doing this wouldn't be straightforward. We need to automate this in an automated fashion","metadata":{"id":"ifXNQHo9Bd9n"}},{"cell_type":"code","source":"logging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\ntrain_df = train_128\neval_df = val\n\ntrain_df[\"prefix\"] = \"\"\neval_df[\"prefix\"] = \"\"\n\nmodel_args = T5Args()\n\n#The maximum sequence length of 100\n#  allows the model to work with reasonably long text (typically a few sentences) while also keeping the training time practical.\n# model_args.max_seq_length = 1024\nmodel_args.max_seq_length = 128\n#Generally, larger batch sizes mean better GPU utilization, and therefore, shorter training times\nmodel_args.train_batch_size = 8\nmodel_args.eval_batch_size = 8\nmodel_args.num_train_epochs = 5\nmodel_args.scheduler = \"cosine_schedule_with_warmup\"\nmodel_args.evaluate_during_training = True\nmodel_args.evaluate_during_training_steps = 10000\nmodel_args.learning_rate = 0.0001\nmodel_args.optimizer = 'Adafactor'\nmodel_args.use_multiprocessing = False\nmodel_args.fp16 = False\nmodel_args.save_steps = -1\nmodel_args.save_eval_checkpoints = False\nmodel_args.no_cache = True\nmodel_args.reprocess_input_data = True\nmodel_args.overwrite_output_dir = True\nmodel_args.save_model_every_epoch = False\nmodel_args.preprocess_inputs = False\nmodel_args.use_early_stopping = True\nmodel_args.num_return_sequences = 1\nmodel_args.do_lower_case = True\nmodel_args.output_dir = \"/kaggle/output/kaggle/working/mt5/\"\nmodel_args.best_model_dir = \"/kaggle/output/kaggle/working/mt5/best_model\"\n\n#If you are using wandb add: wandb.login(key=\"API KEY\")\nmodel_args.wandb_project = \"Yoruba mT5\"\n\nmodel = T5Model(\"mt5\", \"google/mt5-small\", args=model_args)\n\n# Train the model\nmodel.train_model(train_df, eval_data=eval_df)","metadata":{"id":"il66LGzU816Y","outputId":"a73546f5-428c-4ddd-95a3-21a0dc4a053c","execution":{"iopub.status.busy":"2021-12-09T19:55:23.382612Z","iopub.execute_input":"2021-12-09T19:55:23.384516Z","iopub.status.idle":"2021-12-09T20:49:09.596818Z","shell.execute_reply.started":"2021-12-09T19:55:23.384459Z","shell.execute_reply":"2021-12-09T20:49:09.596022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trial: seq_length=128 >> started 4:50 \n\nEpochs 0/5. Running Loss: 4.3811\n\nEpochs 1/5. Running Loss: 3.5221\n\nEpochs 2/5. Running Loss: 3.6496\n\nEpochs 3/5. Running Loss: 3.0159: \n{'global_step': [1776, 3552, 5328, 7104, 8880],\n  'eval_loss': [3.452780764153663,\n   2.907095891364077,\n   2.6020818492199513,\n   2.4616720942740744,\n   2.4358180659882565],\n  'train_loss': [4.381147861480713,\n   3.5221028327941895,\n   3.6496052742004395,\n   3.0159411430358887,\n   3.2644360065460205]})","metadata":{}},{"cell_type":"markdown","source":"# Inference\nFor inference, we first need to load the fine-tuned model from the output directory specified earlier( in model.best_model_dir)","metadata":{}},{"cell_type":"code","source":"model_args = T5Args()\nmodel_args.max_length = 128 #should match the max_seq_length that the model was trained on\nmodel_args.length_penalty = 2.5 #Exponential penalty to the length. Default to 2\nmodel_args.repetition_penalty = 1.5 #The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\nmodel_args.num_beams = 10\n\nmodel1 = T5Model(\"mt5\",\"/kaggle/output/kaggle/working/mt5/best_model\" , args = model_args)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:13:27.013391Z","iopub.execute_input":"2021-12-09T21:13:27.014208Z","iopub.status.idle":"2021-12-09T21:13:31.516954Z","shell.execute_reply.started":"2021-12-09T21:13:27.014169Z","shell.execute_reply":"2021-12-09T21:13:31.516133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1","metadata":{"id":"f3os_ArwKeHw","execution":{"iopub.status.busy":"2021-12-09T21:13:36.871269Z","iopub.execute_input":"2021-12-09T21:13:36.872093Z","iopub.status.idle":"2021-12-09T21:13:36.878975Z","shell.execute_reply.started":"2021-12-09T21:13:36.872055Z","shell.execute_reply":"2021-12-09T21:13:36.878211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1 epoch in around half an hour","metadata":{"id":"rtmkfwQVYsVQ"}},{"cell_type":"markdown","source":"1.20.. 1.50","metadata":{"id":"cGEKx2RCdiJq"}},{"cell_type":"markdown","source":"train loss is larger than eval_loss, wierd","metadata":{"id":"JIG8lDhDJWOi"}},{"cell_type":"markdown","source":"**Trial 1** : coursera DL >> garbage , took like 10 mins, 6k sentences.\n\n**Trial 2** :  2 coursera courses (DL and Michigan), springer file, DS_codata_org, yt_stanford), split into sentences.","metadata":{"id":"vnL8FeWOSxVj"}},{"cell_type":"markdown","source":"#evaluation","metadata":{"id":"S_G7EqTHIGEa"}},{"cell_type":"code","source":"trans = model1.predict(\"In the last few years the Recurrent Neural Network-based architectures have shown the best performance in machine translation problems, but still they have some problems that had to be solved. First, they have a difficulty to cope with long-range dependencies (also LSTM when it has to deal with really long sentences). Secondly, each hidden state depends on the previous one\")","metadata":{"id":"U_HHvjhPIHMI","execution":{"iopub.status.busy":"2021-12-09T21:13:43.176586Z","iopub.execute_input":"2021-12-09T21:13:43.176846Z","iopub.status.idle":"2021-12-09T21:14:03.625334Z","shell.execute_reply.started":"2021-12-09T21:13:43.176817Z","shell.execute_reply":"2021-12-09T21:14:03.62451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trans)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:14:03.637483Z","iopub.execute_input":"2021-12-09T21:14:03.63823Z","iopub.status.idle":"2021-12-09T21:14:03.647385Z","shell.execute_reply.started":"2021-12-09T21:14:03.638194Z","shell.execute_reply":"2021-12-09T21:14:03.646739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:14:14.679261Z","iopub.execute_input":"2021-12-09T21:14:14.679543Z","iopub.status.idle":"2021-12-09T21:14:14.690988Z","shell.execute_reply.started":"2021-12-09T21:14:14.679491Z","shell.execute_reply":"2021-12-09T21:14:14.689748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:05:14.595595Z","iopub.execute_input":"2021-12-09T21:05:14.596132Z","iopub.status.idle":"2021-12-09T21:05:14.610057Z","shell.execute_reply.started":"2021-12-09T21:05:14.596094Z","shell.execute_reply":"2021-12-09T21:05:14.609366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_list = val.input_text.values.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:11:38.564568Z","iopub.execute_input":"2021-12-09T21:11:38.565099Z","iopub.status.idle":"2021-12-09T21:11:38.57129Z","shell.execute_reply.started":"2021-12-09T21:11:38.565062Z","shell.execute_reply":"2021-12-09T21:11:38.570559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_list[4]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:13:10.581783Z","iopub.execute_input":"2021-12-09T21:13:10.582304Z","iopub.status.idle":"2021-12-09T21:13:10.588209Z","shell.execute_reply.started":"2021-12-09T21:13:10.582269Z","shell.execute_reply":"2021-12-09T21:13:10.587566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.predict(val.input_text.values.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:14:23.554183Z","iopub.execute_input":"2021-12-09T21:14:23.554452Z","iopub.status.idle":"2021-12-09T21:16:40.176343Z","shell.execute_reply.started":"2021-12-09T21:14:23.55442Z","shell.execute_reply":"2021-12-09T21:16:40.175562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ، هناك بعض النقاط التي يمكن أن ت'","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:49:54.564571Z","iopub.execute_input":"2021-12-09T20:49:54.565068Z","iopub.status.idle":"2021-12-09T20:49:54.571863Z","shell.execute_reply.started":"2021-12-09T20:49:54.565024Z","shell.execute_reply":"2021-12-09T20:49:54.570395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val['input_text'].head(1)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:49:54.572925Z","iopub.status.idle":"2021-12-09T20:49:54.573856Z","shell.execute_reply.started":"2021-12-09T20:49:54.573593Z","shell.execute_reply":"2021-12-09T20:49:54.573619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom simpletransformers.t5 import T5Model, T5Args\nfrom rouge import Rouge \n\n#Load validation set\nvalidation = pd.read_csv(os.path.join(PATH_TO_DATA, \"validation.csv\"))\n\nmodel_args = T5Args()\nmodel_args.max_length = 100\nmodel_args.length_penalty = 2.5\nmodel_args.repetition_penalty = 1.5\nmodel_args.num_beams = 5\n\n#Load model\nmodel = T5Model(\"mt5\", \"mT5/best_model\", args=model_args)\n\n\n#Perform the inference\nvalidation[\"preds\"] = model.predict(validation.input_text.values.tolist())\n\n#Compute rouge score\nrouge = Rouge()\nscores = rouge.get_scores(preds, validation[\"target_text\"].values.tolist(), avg=True)","metadata":{"id":"5ITHhuE5Imnv","execution":{"iopub.status.busy":"2021-12-09T20:49:54.575229Z","iopub.status.idle":"2021-12-09T20:49:54.57603Z","shell.execute_reply.started":"2021-12-09T20:49:54.575736Z","shell.execute_reply":"2021-12-09T20:49:54.575761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#inference","metadata":{"id":"Nk2i_EkbIEd-"}},{"cell_type":"code","source":"model.predict(\"In the last few years the Recurrent Neural Network-based architectures have shown the best performance in machine translation problems, but still they have some problems that had to be solved. \")","metadata":{"id":"Y4NwECV1IANO","execution":{"iopub.status.busy":"2021-12-09T20:49:54.579594Z","iopub.status.idle":"2021-12-09T20:49:54.580338Z","shell.execute_reply.started":"2021-12-09T20:49:54.580077Z","shell.execute_reply":"2021-12-09T20:49:54.580103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. 6k sentences; started 1:50 --8 mins","metadata":{"id":"SLLyi1cvQrlz"}},{"cell_type":"markdown","source":"# Resources \n- https://huggingface.co/transformers/notebooks.html\n- https://github.com/huggingface/transformers/issues/8704\n- https://towardsdatascience.com/how-to-train-an-mt5-model-for-translation-with-simple-transformers-30ba5fa66c5f\n- https://simpletransformers.ai/docs/usage/\n- https://simpletransformers.ai/docs/t5-model/\n- https://github.com/maroxtn/mt5-M2M-comparison/blob/main/mt5_test.ipynb","metadata":{"id":"nBeRFtjQQcrQ"}}]}