{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUG41_Hw2210",
        "outputId": "b4ca3c67-7ef3-4e2f-841c-2ea414c2001c"
      },
      "source": [
        "!pip install deep_translator"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deep_translator in /usr/local/lib/python3.7/dist-packages (1.5.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from deep_translator) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.7/dist-packages (from deep_translator) (4.10.0)\n",
            "Requirement already satisfied: click<9.0.0,>=8.0.1 in /usr/local/lib/python3.7/dist-packages (from deep_translator) (8.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.3.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click<9.0.0,>=8.0.1->deep_translator) (4.8.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click<9.0.0,>=8.0.1->deep_translator) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click<9.0.0,>=8.0.1->deep_translator) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsTEwiaa2T54"
      },
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "import pandas as pd\n",
        "import time\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9EqPYeg2nzy"
      },
      "source": [
        "springer = pd.read_excel(r'/content/springer_articles.xlsx')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "6haenCaN-MyL",
        "outputId": "0311ba4d-dd36-4119-95c2-425d88e224e2"
      },
      "source": [
        "springer.Content[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Abstract\\r\\nThe application of various sensors in hospitals has enabled the widespread utilization of multivariate time series signals for chronic disease diagnosis in the data-driven world. The key challenge is how to model the complex temporal (linear and nonlinear) correlations among multiple longitudinal variables. Due to scarcity of labels in practice, unsupervised learning methods have already become indispensable. However, state-of-the-art approaches mainly focus on the extraction of linear correlation-induced feature connectivity network, e.g., Pearson correlation, partial correlation, etc. To this end, for chronic disease (e.g., Parkinson disease) diagnosis, an unsupervised representation learning method is first designed to obtain informative correlation-aware signals from multivariate time series data. At the core is a contrastive learning framework with a graph neural network (GNN) encoder to capture the inter-correlation and intra-correlation of multiple longitudinal variables. Then, the previously learned representations are sent to a simple fully connected neural network, which can be trained using fewer labels compared with end-to-end complex supervised learning models. Further, to assist the decision-making process in the high-stake chronic disease detection task, model uncertainty quantification is enabled according to evidential theory. The experimental results on two public Parkinson’s disease data sets show the expressiveness of the learned embeddings, and the final lightweight classifier achieves the best performance.\\r\\nIntroduction\\r\\nWith the development of science and technology, people’s living standards have been greatly improved. However, chronic diseases such as heart disease, cancer, and diabetes are still the leading causes of death and disability in many places around the world. For instance, many old people are suffering from cardiovascular disease, Alzheimer’s disease (AD) [1], and Parkinson’s disease (PD) [2]. What’s more, some chronic diseases such as diabetes and chronic kidney are becoming more and more prevalent among younger age groups [3]. Indeed, people of all ages might suffer from chronic diseases. Therefore, timely diagnosis becomes very important.At present, the widespread deployment of sensors in hospitals has helped accumulate multivariate time series data for patients with chronic diseases or healthy people. Hence, recent academic world has been able to conduct various multivariate time series analysis for chronic diseases diagnosis. In particular, the most efficient approaches adopt the similar pipeline, i.e., extraction of expressive signals from multivariate time series via an unsupervised learning method; then, these signals are sent to a lightweight classifier for supervised learning. Various works have devoted attention to the first step. For instance, it has been shown that feature connectivity network [4,5,6] can highly summarize the structural composition of a complex dynamic system (e.g., the multivariate time series data of a patient with chronic diseases), and then the network serves as a high degree of information for various machine learning tasks. Therefore, state-of-the-art chronic disease detection algorithms mainly focus on the construction of effective feature connectivity matrices. However, they mainly explore to utilize the linear relationships. For example, both Qu et al. [7] and Cheng et al. [8] propose to learn the partial correlation-based connectivity matrix via either shared template regularization [7] or the sparse variational regularization [8]. Even though graph neural networks (GNNs) have been widely adopted to automatically extract the high-level complex feature correlations from multivariate time series data [9,10,11,12], present frameworks are not ideal for multivariate longitudinal data-induced chronic disease diagnosis tasks. On the one hand, most of the frameworks are usually proposed under an ideal fully supervised learning scenario, which is often impossible in a clinical disease diagnosis task due to the difficulties of obtaining the labels; on the other hand, they tend to model the features inter-correlations without careful consideration of the intra-correlations along the temporal series.Worse still, current chronic diseases (e.g., PD) classification algorithms [7, 8, 13,14,15] fail to quantify the model uncertainty [16, 17], which is extremely important in such high-stake healthcare analysis task. For example, if some abnormal noises are added to the multivariate time series data, the model will usually give a wrong prediction with high confidence since the final predictions are obtained only based on the output probability score; thus, it inherently is an overconfident and unreliable model [10]. In contrast, it is much more reasonable to enable the model to filter out the potential uncertain samples that need human experts to judge, i.e., chronic disease diagnosis task needs a confidence-calibrated and uncertainty-aware model.Considering the above limitations, this paper proposes an effective chronic disease diagnosis framework consisting of an unsupervised learning module and an uncertainty-aware supervised learning module. More specifically, in the unsupervised representation learning module, a contrastive learning mechanism is utilized to capture the longitudinal feature intra-correlations, while high-order multiple variables’ inter-correlations are modeled using a GNN, which is chosen as a time span encoder and coupled with the contrastive learning mechanism. Then, the learned representations are sent to a fully connected neural network for supervised learning. Note that since the unsupervised learning module aims at capturing as much as informative signals as possible, any lightweight classifier can be utilized. Further, the evidential theory is leveraged to form the confidence-calibrated and uncertainty-aware classifier. Finally, the effectiveness of the proposed algorithm model is verified based on two public Parkinson's disease data sets.Our contributions are summarized as follows:\\r\\n\\r\\nWe propose an uncertainty-aware chronic disease diagnosis framework, which first extracts informative signals from multivariate time series data via the graph neural newwork and an unsupervised learning paradigm.\\r\\n\\r\\n\\r\\nWe are the first to utilize the time-contrastive learning framework coupled with a graph neural network encoder to model the complex multilevel correlations hidden in the clinical longitudinal data.\\r\\n\\r\\n\\r\\nExperimental results on two public chronic disease classification data sets show the superior performance of the patient representations obtained via the unsupervised learning module. And, the uncertainty quantification indeed helps improve the robustness and reliability of the model.\\r\\nRelated works\\r\\nResearch on chronic disease diagnosis algorithmsChronic disease diagnosis tasks have always attracted widespread attention from both the academic and industrial world. Most of the existing approaches belong to the fully supervised learning setting, which cannot make use of unlabeled data and depends on complex feature processing process [13,14,15]. Recent progresses have been made based on the concept of functional connectivity network [4,5,6], which is constructed via an unsupervised learning model firstly and then is applied to downstream tasks (e.g., classification). For instance, PCC [18] constructed the feature connectivity network based on Pearson correlation. PCC Fisher [19] built the feature connected network based on Pearson correlation and Fisher transform. SR-C [20] captured the sparse feature connected network by Pearson correlation identification with L1 regularization, while SR-PC [21] discovered the feature connectivity network based on partial correlation with lasso regression. Reweighted LASSO [22] constructed feature connected network based on weighted penalty matrix and LASSO regression. SAMCN LASSO [8] utilized gradient meta-learning method. SAMCN Structural LASSO [8] constructed the features connectivity network based on gradient meta-learning, sparse weight penalty matrix and LASSO regression. However, these methods depend on complex feature processing and calculation and can only capture linear relationships, e.g., Pearson correlation, partial correlation, etc. Therefore, more comprehensive algorithms should be designed to capture complex temporal (linear and nonlinear) correlation information.Multivariate time series modelingCurrently, multivariate time series modeling can be categorized into supervised and unsupervised learning. Supervised learning methods [10,11,12] mainly follow an end-to-end learning pipeline, i.e., leveraging different basic neural network components (e.g., CNN, LSTM, etc.) to extract time span features, which are then sent to some well-known machine learning classifiers. On the one hand, fully supervised learning paradigm requires intensive labels, which are often not available for some safety–critical tasks like chronic disease diagnosis. On the other hand, direct usage of labeled data for error backpropagation may introduce some noise leading to unsatisfactory optimization effect [41]. Therefore, unsupervised learning approaches have gradually become the priority choice. Lei [23] proposed an unsupervised learning method based on the distance between the learning representations and the original time series; Malhotra et al. [24] designed the encoder based on recurrent neural network, reconstructed the input time series through the decoder, and combined the encoder and decoder for unsupervised learning. Franceschi et al. [25] proposed the time-contrastive learning framework, which combines triplet loss, causal convolution, and hole convolution network through positive and negative sampling of time series data. Though this method is one of the most simple and efficient unsupervised learning methods for time series data, it fails to model the inter-variable interactions, inspiring us to design more sophisticated unsupervised learning modules.Along this line, graph-structured data mining provides insights that graph can describe the relationships between different nodes, such as social network, transportation network, and human skeleton graph structure. Graph neural network (GNN) can aggregate the feature information of adjacent nodes in graph data. This way of transferring information through graph structure can effectively process graph data and obtain entity representation with stronger expression ability [26, 27]. Various multivariate modeling methods [28, 29] regard each feature as a node in the graph and then utilize graph neural networks to model the complex multilevel correlations. However, most GNN encoder learning methods mainly consider the inter-correlations without considering the intra-correlations. On the contrary, TAGCN [30] uses K convolution kernels of different sizes for feature extraction in each layer. The receptive fields of K convolution kernels are 1\\u2009~\\u2009k, which has strong information aggregation ability, good adaptability, low computational complexity, and can be used in undirected graph or directed graph. Therefore, in this paper, the features of Parkinson's patients are regarded as nodes in the graph structure and TAGCN is used as the encoder of the model.Uncertainty evaluationMost of the existing AI models are known to be prone to overconfidence and are difficult to produce effective patient-level uncertainty scores [31], thus affecting the model reliability, especially in the clinical context (e.g., chronic disease diagnosis). What’s more, even the model has excellent performance for a certain patient population, there are still some out-of-distribution (OOD) patients whose predictions are quite uncertain and often difficult to be detected. Therefore, estimating the uncertainty in an AI system is important to improve the safety and informativeness of the black-box model. For uncertainty quantification, Bayesian neural network (BNN) [16, 32] has achieved good results in providing model uncertainty estimation. At the core of the model is regarding each parameter as distributions rather than fixed parameters, which thereby relies on complex calculation process. Murat et al. [17] proposed a data uncertainty modeling method based on evidential theory, which does not need complex calculation, and can be easily combined with a variety of neural networks. It has the characteristics of simple, efficient, and good adaptability. Therefore, it motivates us to inject it into the supervised learning module of our framework.\\r\\nResearch methods\\r\\nGraph construction methodFirstly, two graph structures are constructed based on the preprocessed PPMI and PS data (see the Sect. 4.1 for details). Note that, in this paper, the features of Parkinson’s patients are regarded as nodes in the graph structure, and the graph structure is constructed by measuring the Euclidean distance between time series features. Given the Parkinson’s disease data set \\\\(X\\\\in {\\\\mathbb{R}}^{N\\\\times T\\\\times D}\\\\) and labels \\\\(Y\\\\in {\\\\mathbb{R}}^{N\\\\times 1}\\\\), where \\\\(N\\\\) denotes the dimension of samples, \\\\(T\\\\) denotes the dimension of time, and \\\\(D\\\\) denotes the dimension of features. Flattening the sample dimension and time dimension, and only keeping the feature dimension, we can obtain the feature vectors: \\\\(\\\\tilde{D}_{N \\\\times T}^{i} = \\\\left\\\\{ {x_{1}^{i} ,x_{2}^{i} , \\\\ldots ,x_{N \\\\times T}^{i} } \\\\right\\\\}\\\\). The Euclidean distance matrix can be calculated between two feature vectors:$$ \\\\tilde{d}_{{\\\\left( {i,j} \\\\right)}} = \\\\sqrt {\\\\mathop \\\\sum \\\\limits_{1}^{{N \\\\times {\\\\text{T}}}} \\\\left( {\\\\tilde{D}_{{N \\\\times {\\\\text{T}}}}^{i} - \\\\tilde{D}_{{N \\\\times {\\\\text{T}}}}^{j} } \\\\right)^{2} } $$\\r\\n                    (1)\\r\\n                Furthermore, the symmetric adjacency matrix is obtained:$$ A_{{\\\\left( {i,j} \\\\right)}} = \\\\frac{1}{{1 + \\\\tilde{d}_{{\\\\left( {i,j} \\\\right)}} }}. $$\\r\\n                    (2)\\r\\n                Note that, in this paper, i and j can be same in the calculation process of Eq.\\xa0(1) and it means the correlation coefficient between features i and j is one after the computation of Eq.\\xa0(2). At the same time, in order to reduce the influence of noise and improve the model performance, robustness and training efficiency, sparsity techniques are adopted. Specifically, the edge connection between two nodes whose weight value is lower than a certain threshold will be deleted, so as to achieve the sparse adjacent matrix. Consider the pruning process of the first row of the matrix:$$ A_{1j}^{*} = \\\\left\\\\{ {\\\\begin{array}{*{20}l} {A_{1,j} ,} \\\\hfill & {A_{1,j} > \\\\eta \\\\cdot \\\\max \\\\left( {A_{1,j} } \\\\right)} \\\\hfill \\\\\\\\ {0,} \\\\hfill & {A_{1,j} \\\\le \\\\eta \\\\cdot \\\\max \\\\left( {A_{1,j} } \\\\right)} \\\\hfill \\\\\\\\ \\\\end{array} } \\\\right. $$\\r\\n                    (3)\\r\\n                where \\\\(\\\\eta \\\\) denotes the sparsity control factor, and \\\\(\\\\mathrm{max}({A}_{1,j})\\\\) denotes the largest weight value when the first feature is connected with all other features. When the weight value of the first feature connected with another feature is lower than threshold \\\\(\\\\eta \\\\cdot \\\\mathrm{max}({A}_{1,j})\\\\), the edge connection between the feature and the first feature will be deleted (the weight value will be set as zero). In the same way, we can get the final matrix \\\\({A}_{ij}^{*}\\\\) after filtering all rows of adjacency matrix.TAGCN feature encodingFor the \\\\(n\\\\)th hidden layer of TAGCN, the input is the graph structure and the corresponding node feature data or the output of the \\\\(n\\\\)th−1 hidden layer. When convolution operation is performed, each node in the graph structure has \\\\(t\\\\in T\\\\) features. For the graph convolution operation in the \\\\(n\\\\)th hidden layer, the \\\\(k\\\\)th feature data on all nodes as the input can be defined as \\\\(x_{k}^{n} \\\\in {\\\\mathbb{R}}^{D} , \\\\quad k = 1,2,3, \\\\ldots ,t\\\\). The specific composition of the feature vector is determined by the node index in the graph structure, and we define \\\\({{G}_{k,f}^{n}\\\\in {\\\\mathbb{R}}}^{D\\\\times D}\\\\) as the \\\\(f\\\\)th convolution kernel. In essence, graph convolution is the product operation between the matrix vectors. For example, \\\\({G}_{k,f}^{n}{x}_{k}^{n}\\\\) denotes an operation of graph convolution.In the process of graph convolution operation, the adjacency matrix is first normalized to ensure that its eigenvalues are in the unit circle and so as to further ensure the stability of graph convolution operation. The normalized adjacency matrix \\\\({\\\\overline{A} }^{*}\\\\) can be described as:$$ \\\\begin{aligned} & \\\\overline{A}^{*} = \\\\overline{D}^{ - 1/2} A^{*} \\\\overline{D}^{ - 1/2} , \\\\\\\\ & \\\\overline{D} = {\\\\text{diag}}\\\\left[ {\\\\overline{d}\\\\left( i \\\\right)} \\\\right], \\\\\\\\ & \\\\overline{d}\\\\left( i \\\\right) = \\\\mathop \\\\sum \\\\limits_{j} A_{ij}^{*} . \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (4)\\r\\n                Further, we can get the convolution kernel by translation invariance principle [30]:$$ G_{k,f}^{n} = \\\\mathop \\\\sum \\\\limits_{c = 0}^{C} g_{k,f,c}^{n} \\\\overline{A}^{*} $$\\r\\n                    (5)\\r\\n                \\r\\nwhere \\\\({g}_{k,f,c}^{n}\\\\) are the graph filter polynomial coefficients. \\\\(C\\\\) is hyperparameter and denotes convolution kernels with different sizes used in each convolutional layer, which is similar to GoogLeNet [39].Then, graph convolution kernels with different sizes slide on the graph-structured data and extract different scales of features. Then, they are combined linearly to form the feature mapping of the output of the \\\\(n\\\\) th hidden layer:$$ y_{f}^{n} = \\\\mathop \\\\sum \\\\limits_{k}^{t} G_{k,f}^{n} x_{k}^{n} + b_{f}^{n} 1_{D} $$\\r\\n                    (6)\\r\\n                \\r\\nwhere \\\\({b}_{f}^{n}\\\\) is trainable parameters, and \\\\({1}_{D}\\\\) is a vector whose element values of dimension D are all one. \\\\({G}_{k,f{x}_{k}^{n}}^{n}\\\\) represents an efficient graph convolution operation on a graph with arbitrary topology.Then, the nonlinear operation is performed on the obtained feature map to get the final output:$$ x_{f}^{n + 1} = \\\\sigma \\\\left( {y_{f}^{n} } \\\\right), $$\\r\\n                    (7)\\r\\n                \\r\\nwhere \\\\(\\\\sigma \\\\) denotes activation function, e.g., RELU.Finally, the output of the graph convolution is transformed into latent space again, and the useful information is further extracted and integrated through the linear function:$$ V^{n + 1} = W_{\\\\eta } x_{f}^{n + 1} + b_{\\\\eta } $$\\r\\n                    (8)\\r\\n                Unsupervised learning based on time-contrastive learningThe most traditional GNN encoders only concentrate on learning node embeddings by aggregating the neighbor nodes information (focus on node level) and then obtain the information of the whole graph, conducting the graph classification task. However, the inter-correlations of the longitudinal nodes between the different graphs and the intracorrelations of the longitudinal nodes among the same graph should also be considered (graph level). In multivariate time series data, each multivariate feature can be seen a node in the graph, few researchers further pay attention to the inter-correlations and intra-correlations in the obtained graph, which limits the performance of the encoders. Hence, in our work, we adopt the time-contrastive learning framework to consider both the intra-correlations and inter-correlations. More specifically, given a time series data \\\\({y}_{i}\\\\) of a sample. Firstly, a random subseries (i.e., a subsequence of a time series composed by consecutive time steps of this time series) \\\\({x}^{\\\\mathrm{rep}}\\\\) of \\\\({y}_{i}\\\\) is sampled, and then subseries  \\\\({x}^{\\\\mathrm{pos}}\\\\) of \\\\({x}^{\\\\mathrm{rep}}\\\\) is obtained as positive pairs. On the one hand, they come from a same time series of a same sample; on the other hand, the \\\\({x}^{\\\\mathrm{rep}}\\\\) can be seen as a word context and the \\\\({x}^{\\\\mathrm{pos}}\\\\) can be seen as the word in the context. Hence, they should have similar representations through the encoder (intra-correlations). Further, the subseries  \\\\({x}^{\\\\mathrm{neg}}\\\\) of another time series data is randomly selected as negative sample. Similarly, on the one hand, \\\\({x}^{\\\\mathrm{rep}}\\\\) and \\\\({x}^{\\\\mathrm{neg}}\\\\) come from a different time series of a different sample; on the other hand, the \\\\({x}^{\\\\mathrm{rep}}\\\\) can be seen as a word context and the \\\\({x}^{\\\\mathrm{neg}}\\\\) can be seen as a random word form the another context. Hence, the representation of the \\\\({x}^{\\\\mathrm{neg}}\\\\) obtained by the encoder should be far away from that of \\\\({x}^{\\\\mathrm{rep}}\\\\) (inter-correlations).Besides, to improve the stability and convergence of the training procedure as well as the experimental results of the learned representations, several negative samples \\\\({{(x}_{k}^{\\\\mathrm{neg}})}_{k\\\\in [1,K]}\\\\) (K denotes the number of random selected negative samples in all time series data) are chosen independently at random in training process. Specially, define the length of time series data \\\\({y}_{i}\\\\) as \\\\({s}_{i}\\\\), where \\\\(i\\\\in [1,N]\\\\) and N denotes the number of time series data. \\\\({s}^{\\\\mathrm{pos}}=\\\\mathrm{size}\\\\left({x}^{\\\\mathrm{pos}}\\\\right),\\\\in [1,{s}_{i}]\\\\), \\\\({s}^{\\\\mathrm{rep}}=\\\\mathrm{size}\\\\left({x}^{\\\\mathrm{rep}}\\\\right),\\\\in [{s}^{\\\\mathrm{pos}},{s}_{i}]\\\\) and \\\\({s}_{k}^{\\\\mathrm{neg}}=\\\\mathrm{size}\\\\left({x}_{k}^{\\\\mathrm{neg}}\\\\right),\\\\in [1,{s}_{k}]\\\\) are picked uniformly at random, where \\\\(size(\\\\cdot )\\\\) means the length of time series data \\\\(x\\\\) and \\\\(k=\\\\mathrm{1,2},\\\\dots ,K\\\\). Then, \\\\({x}^{\\\\mathrm{rep}}\\\\), \\\\({x}^{\\\\mathrm{pos}}\\\\) and \\\\({x}_{k}^{\\\\mathrm{neg}}\\\\) are picked uniformly at random among subseries of \\\\({y}_{i}\\\\) of length \\\\({s}^{\\\\mathrm{rep}}\\\\), \\\\({s}^{\\\\mathrm{pos}}\\\\) and \\\\({s}_{k}^{\\\\mathrm{neg}}\\\\).Finally, the triplet loss function can induce the encoder to have similar representation for similar time series and then learn meaningful encoding representations. The loss function of the unsupervised learning part is:$$ \\\\begin{aligned} & L^{{{\\\\text{tri}}}} = \\\\left[ { - \\\\log (\\\\sigma \\\\left( {f\\\\left( {x^{{{\\\\text{rep}}}} ,\\\\theta } \\\\right)^{{\\\\text{T}}} f\\\\left( {x^{{{\\\\text{pos}}}} ,\\\\theta } \\\\right)} \\\\right)} \\\\right. \\\\\\\\ & \\\\quad - \\\\mathop \\\\sum \\\\limits_{k = 1}^{K} \\\\log \\\\left( {\\\\sigma \\\\left( { - f\\\\left( {x^{{{\\\\text{rep}}}} ,\\\\theta } \\\\right)^{{\\\\text{T}}} } \\\\right.} \\\\right. \\\\\\\\ & \\\\quad \\\\left. {\\\\left. {\\\\left. {f\\\\left( {x_{h}^{{{\\\\text{neg}}}} ,\\\\theta } \\\\right)} \\\\right)} \\\\right)} \\\\right] \\\\cdot \\\\kappa \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (9)\\r\\n                \\r\\nwhere \\\\(\\\\sigma \\\\) denotes sigmoid activation function here and \\\\(f(\\\\cdot ,\\\\theta )\\\\) denotes neural networks (e.g., TAGCN encoder used in this paper) with trainable parameters. \\\\(\\\\kappa \\\\) is used to control the strength of the loss. The loss function will force the encoder to better distinguish \\\\({x}^{\\\\mathrm{rep}}\\\\) and \\\\({x}^{\\\\mathrm{neg}}\\\\) after optimization, and make \\\\({x}^{\\\\mathrm{rep}}\\\\) and \\\\({x}^{\\\\mathrm{pos}}\\\\) have similar representations. Therefore, the triplet loss function leverages TAGCN to capture the high-order inter-correlation and intra-correlation of the multivariate time series.Dirichlet distribution and uncertaintyEvidential uncertainty measurement is derived from the Dempster Shafer Theory of Evidence (DST), which is a generalization of the Bayesian theory to subjective probabilities [40]. In particular, subjective logic (SL) formalizes DST’s notion of belief mass assignments over a frame of discernment based on a Dirichlet distribution with Z parameters \\\\(\\\\alpha = \\\\left[ {\\\\alpha_{1} ,\\\\alpha_{2} , \\\\ldots ,\\\\alpha_{Z} } \\\\right]\\\\)  (subjective opinion\\\\(\\\\alpha \\\\)). Normally, the output of the standard neural network is the probability of each possible category of the sample, and the Dirichlet distribution based on the evidence theory represents the density function of the probability, so it can simulate the second-order probability and uncertainty [34, 35]. The Dirichlet distribution is a probability density function (pdf) for possible values of the probability mass function (pmf) \\\\(p\\\\) and can be expressed by Z parameters\\\\(\\\\alpha \\\\):$$ {\\\\text{Dir}}\\\\left( {p|\\\\alpha } \\\\right) = \\\\left\\\\{ {\\\\begin{array}{*{20}l} {\\\\frac{1}{B\\\\left( \\\\alpha \\\\right)}\\\\mathop \\\\prod \\\\limits_{i = 1}^{Z} p_{i}^{{\\\\alpha_{i - 1} }} ,} \\\\hfill & { p \\\\in S_{Z} } \\\\hfill \\\\\\\\ 0 \\\\hfill & {{\\\\text{otherwise}}} \\\\hfill \\\\\\\\ \\\\end{array} } \\\\right. $$\\r\\n                    (10)\\r\\n                \\r\\nwhere \\\\(p\\\\) is the probability mass function and \\\\(\\\\alpha =[{\\\\alpha }_{1},\\\\dots ,{\\\\alpha }_{Z}]\\\\) are the parameters of Dirichlet distribution. \\\\(Z\\\\) denotes the label category. \\\\(B\\\\left(\\\\alpha \\\\right)\\\\) is a polynomial beta function in \\\\(Z\\\\) dimension [36]. \\\\({S}_{Z}\\\\) is the Z-dimensional unit simplex:$$ S_{Z} = \\\\left\\\\{ {p\\\\left| {\\\\mathop \\\\sum \\\\limits_{i = 1}^{Z} p_{i} = 1\\\\,} \\\\right.{\\\\text{and}}\\\\, 0 \\\\le p_{1} , \\\\ldots ,p_{Z} \\\\le 1} \\\\right\\\\} $$\\r\\n                    (11)\\r\\n                Based on SL, the relationship between belief mass for each singleton \\\\({b}_{z}\\\\) and uncertainty \\\\(u\\\\) is computed as:$$ u + \\\\mathop \\\\sum \\\\limits_{z = 1}^{Z} b_{z} = 1. $$\\r\\n                    (12)\\r\\n                \\r\\nwhere \\\\(u\\\\ge 0\\\\) and \\\\({b}_{z}\\\\ge 0\\\\) for \\\\(z=\\\\mathrm{1,2},\\\\dots ,Z\\\\). A belief mass \\\\({b}_{z}\\\\) for a class label \\\\(z\\\\) is computed using evidence \\\\({e}_{z}\\\\) (\\\\(e=[{e}_{1},{e}_{2},\\\\dots ,{e}_{Z}]\\\\)). More specifically,$$ b_{z} = \\\\frac{{e_{z} }}{S}, u = \\\\frac{Z}{S}, $$\\r\\n                    (13)\\r\\n                $$ \\\\alpha_{z} = e_{z} + 1, $$\\r\\n                    (14)\\r\\n                $$ e_{z} = \\\\zeta \\\\left( {\\\\hat{y}^{s} } \\\\right), $$\\r\\n                    (15)\\r\\n                $$ S = \\\\mathop \\\\sum \\\\limits_{z = 1}^{Z} \\\\alpha_{z} . $$\\r\\n                    (16)\\r\\n                \\r\\nwhere \\\\(Z\\\\) denotes the number of labels, \\\\({\\\\alpha }_{z}\\\\) is Dirichlet parameters,  \\\\({\\\\widehat{y}}^{s}\\\\) is the output vector before being sent to softmax layer, and  \\\\(\\\\zeta \\\\left(\\\\cdot \\\\right)\\\\) denotes an activation layer, e.g., ReLU. \\\\({e}_{z}\\\\) is the amount of evidence and \\\\(S\\\\) is the Dirichlet strength.Further, the expectation of Dirichlet distribution based on neural network evidence theory can be computed as:$$ \\\\hat{p}^{z} = \\\\alpha_{z} /S. $$\\r\\n                    (17)\\r\\n                At present, the well-known Type \\\\(\\\\mathrm{\\\\rm I}\\\\mathrm{\\\\rm I}\\\\) maximum likelihood is always used to optimize the Dirichlet distribution \\\\(D({p}_{i}|{\\\\alpha }_{i})\\\\):$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = - {\\\\text{log}}\\\\left( {\\\\int \\\\mathop \\\\prod \\\\limits_{j = 1}^{Z} p_{ij}^{{y_{ij} }} \\\\frac{1}{{B\\\\left( {\\\\alpha_{i} } \\\\right)}}\\\\mathop \\\\prod \\\\limits_{j = 1}^{Z} p_{ij}^{{\\\\alpha_{ij} - 1}} {\\\\text{d}}P_{i} } \\\\right) \\\\\\\\ & \\\\quad = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} y_{ij} \\\\left( {\\\\log \\\\left( {S_{i} } \\\\right) - {\\\\text{log}}\\\\left( {\\\\alpha_{ij} } \\\\right)} \\\\right). \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (18)\\r\\n                \\r\\nwhere \\\\({y}_{i}\\\\) is a one-hot vector encoding of the ground-truth class with \\\\({y}_{ij}=1\\\\) and \\\\({y}_{iz}=0\\\\) for all \\\\(z\\\\ne j\\\\) and \\\\({\\\\alpha }_{i}\\\\) is the parameters of the Dirichlet density on the predictors.We treat \\\\(D({p}_{i}|{\\\\alpha }_{i})\\\\) as a prior on the likelihood Multi \\\\(({y}_{i}|{p}_{i})\\\\) and obtain the negated logarithm of the marginal likelihood by integrating out the class probabilities. Finally, we minimize the parameters of the Dirichlet density \\\\({\\\\alpha }_{i}\\\\).More specially, following Eq.\\xa0(18), for better quantifying the uncertainties \\\\(u\\\\) of different samples, a cross-entropy loss and Bayes risk-based loss function is defined:$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = \\\\int \\\\left[ {\\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} - y_{ij} \\\\left( {{\\\\text{log}}\\\\left( {p_{ij} } \\\\right)} \\\\right)} \\\\right] \\\\\\\\ & \\\\frac{1}{{B\\\\left( {\\\\alpha_{i} } \\\\right)}}\\\\mathop \\\\prod \\\\limits_{j = 1}^{Z} p_{ij}^{{\\\\alpha_{ij} - 1}} {\\\\text{d}}P_{i} \\\\\\\\ & = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} y_{ij} \\\\left( {\\\\psi \\\\left( {S_{i} } \\\\right) - \\\\psi \\\\left( {\\\\alpha_{ij} } \\\\right)} \\\\right), \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (19)\\r\\n                \\r\\nwhere \\\\(\\\\psi \\\\left(\\\\cdot \\\\right)\\\\) is the digamma function. Following Eq.\\xa0(18), we also define a squares (\\\\(\\\\left\\\\| {y_{i} - p_{i} } \\\\right\\\\|_{2}^{2}\\\\))-based loss function:$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = \\\\int \\\\left\\\\| {y_{i} - p_{i} } \\\\right\\\\|_{2}^{2} \\\\frac{1}{{B\\\\left( {\\\\alpha_{i} } \\\\right)}} \\\\\\\\ & \\\\mathop \\\\prod \\\\limits_{i = 1}^{Z} p_{ij}^{{\\\\alpha_{ij} - 1}} {\\\\text{d}}P_{i} \\\\\\\\ & = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} {\\\\mathbb{E}}\\\\left[ {y_{ij}^{2} - 2y_{ij} p_{ij} + p_{ij}^{2} } \\\\right] \\\\\\\\ & = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left[ {y_{ij}^{2} - 2y_{ij} {\\\\mathbb{E}}[p_{ij} ] + {\\\\mathbb{E}}[p_{ij}^{2} ]} \\\\right]. \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (20)\\r\\n                The above two loss functions both can be used to optimize the Dirichlet distribution, and we will choose the better one through the experiments and theoretical analysis in the following sections.In Dirichlet distribution, if a sample cannot be correctly classified and its total evidence will be close to zero, e.g., \\\\(S\\\\approx K\\\\), corresponding to the uniform distribution and indicating the total uncertainty, e.g., \\\\(u\\\\approx 1\\\\). This is not a good condition and means the classifier fails to differentiate these samples. If the Dirichlet distribution converges to uniform distribution early, the model would not be good at quantifying the uncertainties of most samples. Hence, a Kullback–Leibler (KL) divergence term is incorporated into the defined loss function:$$ \\\\begin{aligned} & {\\\\mathcal{L}}\\\\left( {\\\\Theta } \\\\right) = \\\\mathop \\\\sum \\\\limits_{i = 1}^{N} {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) + \\\\varrho_{t} \\\\\\\\ & \\\\quad \\\\mathop \\\\sum \\\\limits_{i = 1}^{N} KL\\\\left[ {\\\\left. {\\\\hat{D}(p_{i} |\\\\tilde{\\\\alpha }_{i} )} \\\\right\\\\|} \\\\right. \\\\\\\\ & \\\\quad \\\\hat{D}\\\\left. {\\\\left( {p_{i} \\\\left| { < 1, \\\\ldots ,1 > } \\\\right.} \\\\right)} \\\\right], \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (21)\\r\\n                where \\\\({\\\\varrho }_{t}=\\\\mathrm{min}\\\\left(1.0,\\\\frac{t}{10}\\\\right)\\\\in [\\\\mathrm{0,1}]\\\\) is the annealing coefficient, \\\\(t\\\\) is the index of the current training epoch, and \\\\({\\\\stackrel{\\\\sim }{\\\\alpha }}_{i}={y}_{i}+(1-{y}_{i})\\\\odot {\\\\alpha }_{i}\\\\) is the Dirichlet parameters after removal of the nonmisleading evidence from predicted parameters \\\\({\\\\alpha }_{i}\\\\) for sample \\\\(i\\\\). The KL divergence term in the loss function can be calculated as:$$ \\\\begin{aligned} & KL[\\\\hat{D}(p_{i} |\\\\tilde{\\\\alpha }_{i} )||\\\\hat{D}(p_{i} |1)] \\\\\\\\ & = \\\\log \\\\left( {\\\\frac{{\\\\Gamma \\\\left( {\\\\mathop \\\\sum \\\\nolimits_{z = 1}^{Z} \\\\tilde{\\\\alpha }_{iz} } \\\\right)}}{{\\\\Gamma \\\\left( Z \\\\right)\\\\mathop \\\\prod \\\\nolimits_{z = 1}^{Z} \\\\Gamma \\\\left( {\\\\tilde{\\\\alpha }_{iz} } \\\\right)}}} \\\\right) + \\\\mathop \\\\sum \\\\limits_{z = 1}^{Z} \\\\left( {\\\\tilde{\\\\alpha }_{iz} - 1} \\\\right) \\\\\\\\ & \\\\quad \\\\left[ {\\\\psi \\\\left( {\\\\tilde{\\\\alpha }_{iz} } \\\\right) - \\\\psi \\\\left( {\\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\tilde{\\\\alpha }_{ij} } \\\\right)} \\\\right], \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (22)\\r\\n                where \\\\(\\\\Gamma \\\\left(\\\\cdot \\\\right)\\\\) is the gamma function. The annealing coefficient \\\\({\\\\varrho }_{t}\\\\) can control the effect of the KL divergence, and so the Dirichlet distribution will be induced to explore the Dirichlet parameter space as far as possible. The model can be induced to avoid premature convergence to the uniform distribution for the misclassified samples and try to classify them correctly in the training epochs as far as possible.In the process of experiment, we found that the cross-entropy loss and Bayes risk-based loss function tend to generate excessively high belief masses for classes and exhibit relatively less stable performance than the squares loss-based loss function. What’s more, the relationship between the expectation and variance can be described as:$$ {\\\\mathbb{E}}\\\\left[ {p_{ij}^{2} } \\\\right] = {\\\\mathbb{E}}\\\\left[ {p_{ij} } \\\\right]^{2} + {\\\\text{Var}}\\\\left( {p_{ij} } \\\\right). $$\\r\\n                    (23)\\r\\n                Hence, based on the relationship between the expectation and variance, the squares loss-based loss function can be transformed into more interpretable form:$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left( {y_{ij} - {\\\\mathbb{E}}\\\\left[ {p_{ij} } \\\\right]} \\\\right)^{2} + {\\\\text{Var}}\\\\left( {p_{ij} } \\\\right) \\\\\\\\ & \\\\quad = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left( {y_{ij} - \\\\alpha_{ij} /S_{i} } \\\\right)^{2} \\\\\\\\ & \\\\quad \\\\quad + \\\\frac{{\\\\alpha_{ij} \\\\left( {S_{i} - \\\\alpha_{ij} } \\\\right)}}{{S_{i}^{2} \\\\left( {S_{i} + 1} \\\\right)}} \\\\\\\\ & \\\\quad = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left( {y_{ij} - { }\\\\hat{p}_{ij} } \\\\right)^{2} + \\\\frac{{\\\\hat{p}_{ij} \\\\left( {1 - \\\\hat{p}_{ij} } \\\\right)}}{{S_{i} + 1}}. \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (24)\\r\\n                By minimizing the expectation error and variance of the Dirichlet distribution, it can assign belief mass for different samples reasonably and bette\""
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AruIPaK65veS"
      },
      "source": [
        "for i in range(len(springer.Content)):\n",
        "  springer.Content[i] = str(springer.Content[i]).replace('Abstract\\r\\n', '')\n",
        "  springer.Content[i] = re.sub(r'[^a-zA-Z.]', ' ', str(springer.Content[i]))\n",
        "  springer.Content[i] = springer.Content[i].replace('\\n', '')\n",
        "  springer.Content[i] = springer.Content[i].replace('  ', '')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "-8eavtA3A0ea",
        "outputId": "8b0cb1fe-8964-43ab-ec6e-0eb70ac7295f"
      },
      "source": [
        "springer.Content[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The application of various sensors in hospitals has enabled the widespread utilization of multivariate time series signals for chronic disease diagnosis in the data driven world. The key challenge is how to model the complex temporallinear and nonlinearcorrelations among multiple longitudinal variables. Due to scarcity of labels in practiceunsupervised learning methods have already become indispensable. Howeverstate of the art approaches mainly focus on the extraction of linear correlation induced feature connectivity networke.g.Pearson correlationpartial correlationetc. To this endfor chronic diseasee.g.Parkinson diseasediagnosisan unsupervised representation learning method is first designed to obtain informative correlation aware signals from multivariate time series data. At the core is a contrastive learning framework with a graph neural networkGNNencoder to capture the inter correlation and intra correlation of multiple longitudinal variables. Thenthe previously learned representations are sent to a simple fully connected neural networkwhich can be trained using fewer labels compared with end to end complex supervised learning models. Furtherto assist the decision making process in the high stake chronic disease detection taskmodel uncertainty quantification is enabled according to evidential theory. The experimental results on two public Parkinson s disease data sets show the expressiveness of the learned embeddingsand the final lightweight classifier achieves the best performance.IntroductionWith the development of science and technologypeople s living standards have been greatly improved. Howeverchronic diseases such as heart diseasecancerand diabetes are still the leading causes of death and disability in many places around the world. For instancemany old people are suffering from cardiovascular diseaseAlzheimer s diseaseAD and Parkinson s diseasePD . What s moresome chronic diseases such as diabetes and chronic kidney are becoming more and more prevalent among younger age groups. Indeedpeople of all ages might suffer from chronic diseases. Thereforetimely diagnosis becomes very important.At presentthe widespread deployment of sensors in hospitals has helped accumulate multivariate time series data for patients with chronic diseases or healthy people. Hencerecent academic world has been able to conduct various multivariate time series analysis for chronic diseases diagnosis. In particularthe most efficient approaches adopt the similar pipelinei.e.extraction of expressive signals from multivariate time series via an unsupervised learning methodthenthese signals are sent to a lightweight classifier for supervised learning. Various works have devoted attention to the first step. For instanceit has been shown that feature connectivity network can highly summarize the structural composition of a complex dynamic systeme.g.the multivariate time series data of a patient with chronic diseases and then the network serves as a high degree of information for various machine learning tasks. Thereforestate of the art chronic disease detection algorithms mainly focus on the construction of effective feature connectivity matrices. Howeverthey mainly explore to utilize the linear relationships. For exampleboth Qu et al. and Cheng et al. propose to learn the partial correlation based connectivity matrix via either shared template regularization or the sparse variational regularization. Even though graph neural networksGNNshave been widely adopted to automatically extract the high level complex feature correlations from multivariate time series data present frameworks are not ideal for multivariate longitudinal data induced chronic disease diagnosis tasks. On the one handmost of the frameworks are usually proposed under an ideal fully supervised learning scenariowhich is often impossible in a clinical disease diagnosis task due to the difficulties of obtaining the labelson the other handthey tend to model the features inter correlations without careful consideration of the intra correlations along the temporal series.Worse stillcurrent chronic diseasese.g.PDclassification algorithmsfail to quantify the model uncertainty which is extremely important in such high stake healthcare analysis task. For exampleif some abnormal noises are added to the multivariate time series datathe model will usually give a wrong prediction with high confidence since the final predictions are obtained only based on the output probability scorethusit inherently is an overconfident and unreliable model . In contrastit is much more reasonable to enable the model to filter out the potential uncertain samples that need human experts to judgei.e.chronic disease diagnosis task needs a confidence calibrated and uncertainty aware model.Considering the above limitationsthis paper proposes an effective chronic disease diagnosis framework consisting of an unsupervised learning module and an uncertainty aware supervised learning module. More specificallyin the unsupervised representation learning modulea contrastive learning mechanism is utilized to capture the longitudinal feature intra correlationswhile high order multiple variablesinter correlations are modeled using a GNNwhich is chosen as a time span encoder and coupled with the contrastive learning mechanism. Thenthe learned representations are sent to a fully connected neural network for supervised learning. Note that since the unsupervised learning module aims at capturing as much as informative signals as possibleany lightweight classifier can be utilized. Furtherthe evidential theory is leveraged to form the confidence calibrated and uncertainty aware classifier. Finallythe effectiveness of the proposed algorithm model is verified based on two public Parkinson s disease data sets.Our contributions are summarized as follows We propose an uncertainty aware chronic disease diagnosis frameworkwhich first extracts informative signals from multivariate time series data via the graph neural newwork and an unsupervised learning paradigm.We are the first to utilize the time contrastive learning framework coupled with a graph neural network encoder to model the complex multilevel correlations hidden in the clinical longitudinal data.Experimental results on two public chronic disease classification data sets show the superior performance of the patient representations obtained via the unsupervised learning module. Andthe uncertainty quantification indeed helps improve the robustness and reliability of the model.Related worksResearch on chronic disease diagnosis algorithmsChronic disease diagnosis tasks have always attracted widespread attention from both the academic and industrial world. Most of the existing approaches belong to the fully supervised learning settingwhich cannot make use of unlabeled data and depends on complex feature processing process . Recent progresses have been made based on the concept of functional connectivity networkwhich is constructed via an unsupervised learning model firstly and then is applied to downstream taskse.g.classification . For instancePCCconstructed the feature connectivity network based on Pearson correlation. PCC Fisherbuilt the feature connected network based on Pearson correlation and Fisher transform. SR Ccaptured the sparse feature connected network by Pearson correlation identification with Lregularizationwhile SR PCdiscovered the feature connectivity network based on partial correlation with lasso regression. Reweighted LASSOconstructed feature connected network based on weighted penalty matrix and LASSO regression. SAMCN LASSO utilized gradient meta learning method. SAMCN Structural LASSO constructed the features connectivity network based on gradient meta learningsparse weight penalty matrix and LASSO regression. Howeverthese methods depend on complex feature processing and calculation and can only capture linear relationshipse.g.Pearson correlationpartial correlationetc. Thereforemore comprehensive algorithms should be designed to capture complex temporallinear and nonlinearcorrelation information.Multivariate time series modelingCurrentlymultivariate time series modeling can be categorized into supervised and unsupervised learning. Supervised learning methodsmainly follow an end to end learning pipelinei.e.leveraging different basic neural network componentse.g.CNNLSTMetc.to extract time span featureswhich are then sent to some well known machine learning classifiers. On the one handfully supervised learning paradigm requires intensive labelswhich are often not available for some safety critical tasks like chronic disease diagnosis. On the other handdirect usage of labeled data for error backpropagation may introduce some noise leading to unsatisfactory optimization effect . Thereforeunsupervised learning approaches have gradually become the priority choice. Leiproposed an unsupervised learning method based on the distance between the learning representations and the original time seriesMalhotra et al.designed the encoder based on recurrent neural networkreconstructed the input time series through the decoderand combined the encoder and decoder for unsupervised learning. Franceschi et al.proposed the time contrastive learning frameworkwhich combines triplet losscausal convolutionand hole convolution network through positive and negative sampling of time series data. Though this method is one of the most simple and efficient unsupervised learning methods for time series datait fails to model the inter variable interactionsinspiring us to design more sophisticated unsupervised learning modules.Along this linegraph structured data mining provides insights that graph can describe the relationships between different nodessuch as social networktransportation networkand human skeleton graph structure. Graph neural networkGNNcan aggregate the feature information of adjacent nodes in graph data. This way of transferring information through graph structure can effectively process graph data and obtain entity representation with stronger expression ability . Various multivariate modeling methodsregard each feature as a node in the graph and then utilize graph neural networks to model the complex multilevel correlations. Howevermost GNN encoder learning methods mainly consider the inter correlations without considering the intra correlations. On the contraryTAGCNuses K convolution kernels of different sizes for feature extraction in each layer. The receptive fields of K convolution kernels are kwhich has strong information aggregation abilitygood adaptabilitylow computational complexityand can be used in undirected graph or directed graph. Thereforein this paperthe features of Parkinson s patients are regarded as nodes in the graph structure and TAGCN is used as the encoder of the model.Uncertainty evaluationMost of the existing AI models are known to be prone to overconfidence and are difficult to produce effective patient level uncertainty scores thus affecting the model reliabilityespecially in the clinical contexte.g.chronic disease diagnosis . What s moreeven the model has excellent performance for a certain patient populationthere are still some out of distributionOODpatients whose predictions are quite uncertain and often difficult to be detected. Thereforeestimating the uncertainty in an AI system is important to improve the safety and informativeness of the black box model. For uncertainty quantificationBayesian neural networkBNN has achieved good results in providing model uncertainty estimation. At the core of the model is regarding each parameter as distributions rather than fixed parameterswhich thereby relies on complex calculation process. Murat et al.proposed a data uncertainty modeling method based on evidential theorywhich does not need complex calculationand can be easily combined with a variety of neural networks. It has the characteristics of simpleefficientand good adaptability. Thereforeit motivates us to inject it into the supervised learning module of our framework.Research methodsGraph construction methodFirstlytwo graph structures are constructed based on the preprocessed PPMI and PS datasee the Sect..for details . Note thatin this paperthe features of Parkinson s patients are regarded as nodes in the graph structureand the graph structure is constructed by measuring the Euclidean distance between time series features. Given the Parkinson s disease data set X in mathbb RN times T times Dand labels Y in mathbb RN times where N denotes the dimension of samplesT denotes the dimension of timeand D denotes the dimension of features. Flattening the sample dimension and time dimensionand only keeping the feature dimensionwe can obtain the feature vectors tilde D Ntimes T i leftxi xi ldotsxNtimes T i right. The Euclidean distance matrix can be calculated between two feature vectors tilde d left i j right sqrt mathopsumlimits Ntimes text Tlefttilde DNtimes text Ti tilde DNtimes text Tj right Furthermorethe symmetric adjacency matrix is obtainedAleft i j right fractilde d left i j right.Note thatin this paperi and j can be same in the calculation process of Eq. and it means the correlation coefficient between features i and j is one after the computation of Eq.. At the same timein order to reduce the influence of noise and improve the model performancerobustness and training efficiencysparsity techniques are adopted. Specificallythe edge connection between two nodes whose weight value is lower than a certain threshold will be deletedso as to achieve the sparse adjacent matrix. Consider the pruning process of the first row of the matrixA j left begin array l AjhfillAj etacdotmaxleft Aj righthfillhfillAj leetacdotmaxleft Aj righthfill end array right.whereetadenotes the sparsity control factorandmathrm max A j denotes the largest weight value when the first feature is connected with all other features. When the weight value of the first feature connected with another feature is lower than thresholdetacdotmathrm max A jthe edge connection between the feature and the first feature will be deletedthe weight value will be set as zero . In the same waywe can get the final matrixA ijafter filtering all rows of adjacency matrix.TAGCN feature encodingFor the nth hidden layer of TAGCNthe input is the graph structure and the corresponding node feature data or the output of the nth hidden layer. When convolution operation is performedeach node in the graph structure has t in T features. For the graph convolution operation in the nth hidden layerthe kth feature data on all nodes as the input can be defined as xk n in mathbb RD quad k ldotst. The specific composition of the feature vector is determined by the node index in the graph structureand we define G k f nin mathbb R D times Das the fth convolution kernel. In essencegraph convolution is the product operation between the matrix vectors. For example G k f nx k ndenotes an operation of graph convolution.In the process of graph convolution operationthe adjacency matrix is first normalized to ensure that its eigenvalues are in the unit circle and so as to further ensure the stability of graph convolution operation. The normalized adjacency matrix overline Acan be described as begin aligned overline A overline D Aoverline D overline Dtext diag leftoverline dleftirightright overline dleftiright mathopsumlimitsjAij. end aligned Furtherwe can get the convolution kernel by translation invariance principle Gk f n mathopsumlimitsc Cgk f c n overline A whereg k f c nare the graph filter polynomial coefficients. C is hyperparameter and denotes convolution kernels with different sizes used in each convolutional layerwhich is similar to GoogLeNet .Thengraph convolution kernels with different sizes slide on the graph structured data and extract different scales of features. Thenthey are combined linearly to form the feature mapping of the output of the n th hidden layeryf n mathopsumlimitsk tGk f nxk nbf n D whereb f nis trainable parametersandDis a vector whose element values of dimension D are all one.G k f x k nnrepresents an efficient graph convolution operation on a graph with arbitrary topology.Thenthe nonlinear operation is performed on the obtained feature map to get the final outputxf n sigmaleft yf n rightwheresigmadenotes activation functione.g.RELU.Finallythe output of the graph convolution is transformed into latent space againand the useful information is further extracted and integrated through the linear functionVnW eta xf nb etaUnsupervised learning based on time contrastive learningThe most traditional GNN encoders only concentrate on learning node embeddings by aggregating the neighbor nodes informationfocus on node leveland then obtain the information of the whole graphconducting the graph classification task. Howeverthe inter correlations of the longitudinal nodes between the different graphs and the intracorrelations of the longitudinal nodes among the same graph should also be consideredgraph level . In multivariate time series dataeach multivariate feature can be seen a node in the graphfew researchers further pay attention to the inter correlations and intra correlations in the obtained graphwhich limits the performance of the encoders. Hencein our workwe adopt the time contrastive learning framework to consider both the intra correlations and inter correlations. More specificallygiven a time series datay iof a sample. Firstlya random subseriesi.e.a subsequence of a time series composed by consecutive time steps of this time series xmathrm rep ofy iis sampledand then subseries xmathrm pos ofxmathrm rep is obtained as positive pairs. On the one handthey come from a same time series of a same sampleon the other handthexmathrm rep can be seen as a word context and thexmathrm pos can be seen as the word in the context. Hencethey should have similar representations through the encoderintra correlations . Furtherthe subseries xmathrm neg of another time series data is randomly selected as negative sample. Similarlyon the one hand xmathrm rep andxmathrm neg come from a different time series of a different sampleon the other handthexmathrm rep can be seen as a word context and thexmathrm neg can be seen as a random word form the another context. Hencethe representation of thexmathrm neg obtained by the encoder should be far away from that ofxmathrm repinter correlations .Besidesto improve the stability and convergence of the training procedure as well as the experimental results of the learned representationsseveral negative samplesx kmathrm negk inKK denotes the number of random selected negative samples in all time series dataare chosen independently at random in training process. Speciallydefine the length of time series datay iass i where i inNand N denotes the number of time series data.smathrm posmathrm sizeleftxmathrm pos right in s i smathrm repmathrm sizeleftxmathrm rep right in smathrm poss i ands kmathrm negmathrm sizeleftx kmathrm neg right in s k are picked uniformly at randomwhere sizecdot means the length of time series data x and kmathrm dotsK. Then xmathrm rep xmathrm pos andx kmathrm neg are picked uniformly at random among subseries ofy iof lengthsmathrm rep smathrm pos ands kmathrm neg.Finallythe triplet loss function can induce the encoder to have similar representation for similar time series and then learn meaningful encoding representations. The loss function of the unsupervised learning part is begin alignedL text trileft log sigmaleft f left x text rep thetaright text Tf left x text pos thetarightrightright. quadmathopsumlimitsk K logleftsigmaleftf left x text rep thetaright text T right. right. quadleft. left. left.f left xhtext neg thetarightrightrightright cdotkappa end aligned wheresigmadenotes sigmoid activation function here and fcdot theta denotes neural networkse.g.TAGCN encoder used in this paperwith trainable parameters.kappais used to control the strength of the loss. The loss function will force the encoder to better distinguishxmathrm rep andxmathrm neg after optimizationand makexmathrm rep andxmathrm pos have similar representations. Thereforethe triplet loss function leverages TAGCN to capture the high order inter correlation and intra correlation of the multivariate time series.Dirichlet distribution and uncertaintyEvidential uncertainty measurement is derived from the Dempster Shafer Theory of EvidenceDST which is a generalization of the Bayesian theory to subjective probabilities . In particularsubjective logicSLformalizes DST s notion of belief mass assignments over a frame of discernment based on a Dirichlet distribution with Z parametersalphaleftalpha alphaldots alphaZ rightsubjective opinion alpha. Normallythe output of the standard neural network is the probability of each possible category of the sampleand the Dirichlet distribution based on the evidence theory represents the density function of the probabilityso it can simulate the second order probability and uncertainty . The Dirichlet distribution is a probability density functionpdffor possible values of the probability mass functionpmfp and can be expressed by Z parameters alpha text Dir left palpharight left begin array lfracB left alpharight mathopprodlimitsi Zpi alphai hfill pin SZ hfill hfilltext otherwise hfill end array right. where p is the probability mass function andalpha alphadotsalphaZ are the parameters of Dirichlet distribution. Z denotes the label category. B leftalpharightis a polynomial beta function in Z dimension .S Zis the Z dimensional unit simplexSZ leftp leftmathopsumlimitsi Zpiright.text andle pldotspZ le right Based on SLthe relationship between belief mass for each singletonb zand uncertainty u is computed asumathopsumlimitsz Zbz . where u ge andb zge for zmathrm dotsZ. A belief massb zfor a class label z is computed using evidencee z e e e dots e Z . More specificallybz fracez S ufrac ZS alphazezez zetalefthat y s rightSmathopsumlimitsz Z alphaz. where Z denotes the number of labelsalphazis Dirichlet parameters widehat ysis the output vector before being sent to softmax layerand zetaleftcdotrightdenotes an activation layere.g.ReLU.e zis the amount of evidence and S is the Dirichlet strength.Furtherthe expectation of Dirichlet distribution based on neural network evidence theory can be computed as hat p z alphaz S. At presentthe well known Typemathrmrm Imathrmrm Imaximum likelihood is always used to optimize the Dirichlet distribution Dp ialphai begin alignedmathcal Li leftThetarighttext log leftintmathopprodlimitsj Zpijyijfrac B leftalphai rightmathopprodlimitsj Zpij alphaijtext dPi rightquadmathopsumlimitsj Zyij leftlogleft Si righttext log leftalphaij rightright . end alignedwherey iis a one hot vector encoding of the ground truth class withy ijandy izfor all z ne j and alphaiis the parameters of the Dirichlet density on the predictors.We treat Dp ialphai as a prior on the likelihood Multi y i p i and obtain the negated logarithm of the marginal likelihood by integrating out the class probabilities. Finallywe minimize the parameters of the Dirichlet density alphai .More speciallyfollowing Eq. for better quantifying the uncertainties u of different samplesa cross entropy loss and Bayes risk based loss function is defined begin alignedmathcal Li leftThetaright intleftmathopsumlimitsj Zyij left text log left pij rightrightrightfrac B leftalphai rightmathopprodlimitsj Zpij alphaijtext dPimathopsumlimitsj Zyij leftpsileft Si right psileftalphaij rightright end alignedwherepsileftcdotrightis the digamma function. Following Eq. we also define a squares leftyipi rightbased loss function begin alignedmathcal Li leftThetaright intleftyipi rightfrac B leftalphai rightmathopprodlimitsi Zpij alphaijtext dPimathopsumlimitsj Zmathbb E left yij yijpijpij rightmathopsumlimitsj Z left yij yijmathbb E pijmathbb E pijright . end alignedThe above two loss functions both can be used to optimize the Dirichlet distributionand we will choose the better one through the experiments and theoretical analysis in the following sections.In Dirichlet distributionif a sample cannot be correctly classified and its total evidence will be close to zeroe.g.S approx Kcorresponding to the uniform distribution and indicating the total uncertaintye.g.u approx. This is not a good condition and means the classifier fails to differentiate these samples. If the Dirichlet distribution converges to uniform distribution earlythe model would not be good at quantifying the uncertainties of most samples. Hencea Kullback LeiblerKLdivergence term is incorporated into the defined loss function begin alignedmathcal L leftThetaright mathopsumlimitsi Nmathcal Li leftThetaright varrhotquadmathopsumlimitsi NKL leftleft. hat Dpitildealphairight right. quadhat Dleft. left pi leftldots right. rightright end alignedwhere varrhot mathrm minleft. frac trightin mathrm is the annealing coefficientt is the index of the current training epochand stackrelsimalpha i y iy i odot alphaiis the Dirichlet parameters after removal of the nonmisleading evidence from predicted parameters alphaifor sample i. The KL divergence term in the loss function can be calculated as begin alignedKLhat Dpitildealphaihat Dpi logleftfrac Gammaleftmathopsumnolimitsz Z tildealphaiz rightGammaleftZrightmathopprodnolimitsz Z Gammalefttildealphaiz rightright mathopsumlimitsz Z lefttildealphaizrightquadleftpsilefttildealphaiz right psileftmathopsumlimitsj Z tildealphaij rightright end alignedwhereGammaleftcdotrightis the gamma function. The annealing coefficient varrhotcan control the effect of the KL divergenceand so the Dirichlet distribution will be induced to explore the Dirichlet parameter space as far as possible. The model can be induced to avoid premature convergence to the uniform distribution for the misclassified samples and try to classify them correctly in the training epochs as far as possible.In the process of experimentwe found that the cross entropy loss and Bayes risk based loss function tend to generate excessively high belief masses for classes and exhibit relatively less stable performance than the squares loss based loss function. What s morethe relationship between the expectation and variance can be described asmathbb E left pij rightmathbb E left pij righttext Var left pij right . Hencebased on the relationship between the expectation and variancethe squares loss based loss function can be transformed into more interpretable form begin alignedmathcal Li leftThetaright mathopsumlimitsj Z left yijmathbb E left pij rightrighttext Var left pij rightquadmathopsumlimitsj Z left yij alphaij Si rightquadquadfrac alphaij left Si alphaij right Si left Sirightquadmathopsumlimitsj Z left yijhat p ij right frac hat p ij lefthat p ij right Si . end alignedBy minimizing the expectation error and variance of the Dirichlet distributionit can assign belief mass for different samples reasonably and bette'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9ExFWNWffpz"
      },
      "source": [
        "final_list = []\n",
        "\n",
        "def dataset_divider(data, column):\n",
        "  for par in data[column]: \n",
        "    start = 0\n",
        "    d = []\n",
        "    chunk = 4950\n",
        "    n = [chunk, chunk, chunk, chunk, chunk, chunk, chunk]\n",
        "    for i in n:\n",
        "        d.append(par[start:start+i])\n",
        "        start += i\n",
        "    d.append(par[start:])\n",
        "    p = [x for x in d if len(x) > 1] \n",
        "    final_list.append(p)\n",
        "\n",
        "  return final_list"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnmS3G2H27lN"
      },
      "source": [
        "final_list = dataset_divider(springer, 'Content')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT2vqWNJgu2u",
        "outputId": "8a02ff17-63b8-4674-9692-528a53a78ae5"
      },
      "source": [
        "len(final_list)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzzt8-nNeCOt"
      },
      "source": [
        "# yfrom googletrans import Translator\n",
        "translator  = GoogleTranslator()\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRUQYP9p6-15"
      },
      "source": [
        "def Translator(list_to_translate, language):\n",
        "  counter = 0\n",
        "  for i in list_to_translate:\n",
        "      final_translated_to_arabic = []\n",
        "      row_translated_to_arabic = []\n",
        "      # [128:]\n",
        "      print(counter)\n",
        "      for j in range(len(i)):\n",
        "\n",
        "        row = GoogleTranslator(source='auto', target=language).translate(text=str(i[j]))\n",
        "        row_translated_to_arabic.append(row)\n",
        "\n",
        "      final_translated_to_arabic.append('.'.join(map(str, row_translated_to_arabic)))\n",
        "              \n",
        "      row_translated_to_arabic = []\n",
        "      \n",
        "      counter = counter + 1\n",
        "\n",
        "  return final_translated_to_arabic"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPVxVVFi2ZjY",
        "outputId": "ad9fcd88-f07d-454f-cace-c285ef631469"
      },
      "source": [
        "Translator(final_list, 'ar')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-ksayik3L7t",
        "outputId": "ce5a1517-4552-438c-ca7b-7936e45c3747"
      },
      "source": [
        "len(final_translated_to_arabic)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoemNT9Misrs"
      },
      "source": [
        "# final_translated_to_arabic"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iPw9T_v19jj"
      },
      "source": [
        "springer['arabic_trnaslated'] = final_translated_to_arabic"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "AcWtvVCP2JnX",
        "outputId": "8f61b70f-e5ef-4931-f484-827b45187e32"
      },
      "source": [
        "springer.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Content</th>\n",
              "      <th>URL</th>\n",
              "      <th>arabic_trnaslated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Exploring unsupervised multivariate time serie...</td>\n",
              "      <td>The application of various sensors in hospital...</td>\n",
              "      <td>https://link.springer.com//article/10.1007/s41...</td>\n",
              "      <td>أتاح تطبيق أجهزة الاستشعار المختلفة في المستشف...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Big social data provenance framework for Zero-...</td>\n",
              "      <td>Social media has been playing a vital importan...</td>\n",
              "      <td>https://link.springer.com//article/10.1007/s41...</td>\n",
              "      <td>تلعب وسائل التواصل الاجتماعي أهمية حيوية في مش...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Graph sparsification with graph convolutional ...</td>\n",
              "      <td>Graphs are ubiquitous across the globe and wit...</td>\n",
              "      <td>https://link.springer.com//article/10.1007/s41...</td>\n",
              "      <td>الرسوم البيانية منتشرة في كل مكان حول العالم و...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Anonymization of German financial documents us...</td>\n",
              "      <td>The automatization and digitalization of busin...</td>\n",
              "      <td>https://link.springer.com//article/10.1007/s41...</td>\n",
              "      <td>أدت أتمتة العمليات التجارية ورقمنتها إلى زيادة...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Correction to: Conventional displays of struct...</td>\n",
              "      <td>Correction toInternational Journal of Data Sci...</td>\n",
              "      <td>https://link.springer.com//article/10.1007/s41...</td>\n",
              "      <td>تصحيح في المجلة الدولية لعلوم وتحليلات البيانا...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        ArticleTitle  ...                                  arabic_trnaslated\n",
              "0  Exploring unsupervised multivariate time serie...  ...  أتاح تطبيق أجهزة الاستشعار المختلفة في المستشف...\n",
              "1  Big social data provenance framework for Zero-...  ...  تلعب وسائل التواصل الاجتماعي أهمية حيوية في مش...\n",
              "2  Graph sparsification with graph convolutional ...  ...  الرسوم البيانية منتشرة في كل مكان حول العالم و...\n",
              "3  Anonymization of German financial documents us...  ...  أدت أتمتة العمليات التجارية ورقمنتها إلى زيادة...\n",
              "4  Correction to: Conventional displays of struct...  ...  تصحيح في المجلة الدولية لعلوم وتحليلات البيانا...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv0yu77g7DrM"
      },
      "source": [
        "springer.to_csv('/content/translated_to_arabic.csv') "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvOpc_HYj7d6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}