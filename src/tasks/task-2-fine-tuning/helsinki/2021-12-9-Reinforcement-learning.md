---
layout: post
title: "Reinforcement Learning"
description: "Reinforcement Learning"
date: 2021-12-9T07:00:00-07:00
tags: Arabic
#image: /img/foo/bar.png
---

<h1 dir='rtl' align='right'>التعلم المعزز </h1>

<p dir='rtl' align='right'>
التعلم المعزز العميق هو أحد أكثر فروع الذكاء الاصطناعي إثارة للاهتمام ، وهو متخلف عن بعض أهم إنجازات الذكاء الاصطناعي ، بما في ذلك ضرب اللاعبين البشريين على السبورة وألعاب الفيديو ، والسيارات ذاتية القيادة ، والروبوتات ، وتصميم أجهزة الذكاء الاصطناعي.
</p>

<p dir='rtl' align='right'>
يعتمد التعلم المعزز العميق على قدرة الشبكات العصبية العميقة على معالجة المشاكل المعقدة للغاية لتقنيات RL الكلاسيكية يعد التعلم المعزز العميق أكثر تعقيدًا من الفروع الأخرى للتعلم الآلي ، ولكن في هذا المقال ، سأحاول إزالة الغموض دون الدخول إلى التفاصيل التقنية
</p>

----

<h3 dir='rtl' align='right'>حالات ومكافآت وأفعال </h3>

<p dir='rtl' align='right'>
في قلب كل مشكلة التعلم المعزز هي عامل وبيئة ، توفر البيئة معلومات عن حالة النظام ويلاحظ الوكيل هذه الحالات ويتفاعل مع البيئة من خلال اتخاذ الإجراءات ويمكن أن تكون الإجراءات منفصلة ، على سبيل المثال ، مقلبًا لمفتاح التشغيل أو مستمرة ، على سبيل المثال ، تحول knob ، وهذه الإجراءات تؤدي إلى انتقال البيئة إلى حالة جديدة بناءً على ما إذا كانت الحالة الجديدة ذات صلة بهدف النظام ، يمكن أن تكون المكافأة هي أيضًا صفرًا أو سلبيًا إذا
</p>



<p dir='rtl' align='right'>
كل دورة من دورة الإجراءات إلى الأمام تسمى الخطوة ، يستمر نظام التعلم المعزز بالتكرار حتى يصل إلى الحالة المرغوب فيها أو عدد أقصى من الخطوات ينتهي به الأمر ، تسمى سلسلة الخطوات هذه حلقة ، وفي بداية كل حلقة ، تضبط البيئة على حالة أولية ويتم إعادة المكافأة إلى الصفر
</p>

<p dir='rtl' align='right'>
والهدف من التعلم المعزز هو تدريب الوكيل على اتخاذ إجراءات تزيد من مكافآتها ، وظيفة اتخاذ إجراءات الوكيل تسمى السياسة ، وعادة ما يحتاج الوكيل إلى العديد من الحلقات لتعلم سياسة جيدة ، وبالنسبة لمشاكل أبسط ، قد تكون مائة حلقة كافية للوكيل لمعرفة سياسة مناسبة ، وبالنسبة لمشاكل أكثر تعقيدًا ، قد يحتاج الوكيل إلى ملايين الحلقات التدريبية
</p>

<p dir='rtl' align='right'>
هناك المزيد من الفروق الدقيقة على أنظمة التعلم المعزز، على سبيل المثال ، يمكن أن تكون بيئة RL حتمية أو غير حتمية ، وفي البيئات الحتمية ، يؤدي تشغيل سلسلة من أزواج إجراءات الدولة عدة مرات إلى نفس النتيجة، على عكس ذلك ، في مشاكل RL غير حتمية ، يمكن أن تتغير حالة البيئة من أشياء أخرى غير أفعال الوكيل مثل مرور الوقت والطقس والوكلاء الآخرين في البيئة
</p>

----

<h3 dir='rtl' align='right'>تطبيقات التعلم المعزز </h3>


<p dir='rtl' align='right'>
من أجل فهم مكونات التعلم المعزز ، دعونا نفكر في بعض الأمثلة
</p>

<p dir='rtl' align='right'>
الشطرنج: هنا ، البيئة هي رقعة الشطرنج وحالة البيئة هي موضع قطع الشطرنج على السبورة، يمكن أن يكون وكيل RL أحد اللاعبين يمكن أن يكون كلا اللاعبين وكلاء RL تدريبيًا منفصلًا في نفس البيئة. كل لعبة من لعبة الشطرن حلقة تبدأ الحلقة في حالة أولية على طول حواف السبورة. في كل خطوة ، يراقب الوكيل اللوحة (الحالة) وينقل إحدى قطعها (يتخذ إجراءً ينقل البيئة إلى حالة جديدة. يكافأ الوكيل على الوصول إلى حالة التحقق من المكافآت ولا أي مكافآت خلاف ذلك. أحد تحديات الشطرن الرئيسية هو أن الوكيل لا يحصل على أي مكافآت قبل التحقق من هوية الخصم مما يجعل من الصعب التعلم.
</p>

<p dir='rtl' align='right'>
أتاري: الخروج عبارة عن لعبة يتحكم فيها اللاعب في المضرب ، وهناك كرة تتحرك عبر الشاشة. في كل مرة تصطدم فيها إلى المضرب ، ينتقل إلى أعلى الشاشة حيث تصطف الصفوف من اللبنات. في كل مرة يصطدم فيها المصب بصلبًا ، يتلف اللبن ويرجع الكرة مرة أخرى ،في الانهيار ، البيئة هي شاشات اللعبة الدولة هي موقع المصف والطوب وموقع الكرة وسرعتها. أن الإجراءات التي يمكن للوكيلها تتحرك إلى اليسار ، أو تتحرك يمينًا ، أو لا تتحرك على الإطلاق. يتلقى الوكيل مكافأة إيجابية في كل مرة تصطدم فيها الكرة بصخرة ومكافأة سلبية إذا انتقلت الكرة إلى أسفل الشاشة
</p>

<p dir='rtl' align='right'>
السيارات ذاتية القيادة: في القيادة الذاتية ، الوكيل هو السيارة والبيئة هي العالم الذي تتنقل فيه السيارة، ويلاحظ وكيل RL حالة البيئة من خلال آلات التصوير والأصفادات وغيرها من أجهزة الاستشعار. يمكن للوكيل اتخاذ إجراءات الملاحة مثل التسارع أو الانعكاس على المكابح أو الانعكاس على اليسار أو اليمين أو عدم القيام بأي شيء. يتم مكافأة وكيل RL على البقاء على الطريق وتجنب الاصطدام والامتثال لأنظمة القيادة والبقاء على الطريق. 
</p>

----
 
<h3 dir='rtl' align='right'>دالات التعلم المعزز </h3>

<p dir='rtl' align='right'>
في الأساس ، فإن الهدف من التعلم المعزز هو تعيين الحالات إلى الإجراءات بطريقة تزيد من المكافآت ولكن ماذا يتعلم وكيل RL بالضبط؟
</p>

<p dir='rtl' align='right'>
هناك ثلاث فئات من خوارزميات التعلم لأنظمة RL:
</p>

<p dir='rtl' align='right'>
الخوارزميات القائمة على السياسة: هذا هو أكثر أنواع التحسين عمومية. تقوم سياسة بتعيين الحالات إلى الإجراءات. يمكن لعامل RL الذي يتعلم سياسة أن يخلق مسارًا للإجراءات يؤدي من الحالة الحالية إلى الهدف.
</p>
<p dir='rtl' align='right'>
على سبيل المثال ، فكر في عامل يعمل على تحسين سياسة الانتقال عبر المتاه والوصول إلى المخرج. أولاً ، يبدأ بإجراء خطوات عشوائية لا يكافأ عليها، في إحدى الحلقات ، تصل أخيرًا إلى المخرج وتحصل على مكافأة الخروج. إنها تعيد ضبط مسارها وتقرأ مكافأة كل من أزواج الإجراءات الخاصة بكل حالة بناءً على مدى تقاربها من الهدف النهائي، في الحلقة التالية ، يكون لدى وكيل RL فهم أفضل للإجراءات التي يجب اتخاذها في كل حالة، إنه يعدل السياسة تدريجياً حتى تتقارب إلى حل مثالي.
</p>


<p dir='rtl' align='right'>
تعزيز خوارزمية قائمة على السياسة العامة ، وميزة الوظائف القائمة على السياسة هي أنه يمكن تطبيقها على جميع أنواع مشاكل التعلم المعزز ، ومفاضلة الخوارزميات القائمة على السياسة هي أنها غير فعالة ، وتتطلب الكثير من التدريب قبل التقارب حول الحلول المثلى.
</p>

<p dir='rtl' align='right'>
خوارزميات قائمة على القيمة: الوظائف القائمة على القيمة تتعلم تقييم قيمة الحالات والإجراءات، الوظائف القائمة على القيمة ، وتساعد وكيل RL على تقييم العائد المستقبلي المحتمل على الحالة والإجراءات الحالية. 
هناك متغيران للدوال القائمة على القيمة هما قيم Q وقيم V ،وظائف Q تقدر العائد المتوقع على أزواج الإجراءات الخاصة بكل حالة، وظائف V تقدر فقط قيمة الحالات، وظائف Q الأكثر شيوعًا لأنه من الأسهل تغيير أزواج إجراءات الدولة إلى سياسة. 

</p>

<p dir='rtl' align='right'>
خوارزميتين قائمتين على القيمة شائعتان هما SARSA و DQN ، خوارزميات قائمة على القيمة أكثر كفاءة من RL قائم على السياسة ، فإن محدوديتها هي أنها قابلة للتطبيق فقط على مساحات الإجراءات المنفصلة ، ما لم تقم بإجراء بعض التغييرات عليها.
</p>

<p dir='rtl' align='right'>
الخوارزميات القائمة على النماذج: تتخذ الخوارزميات القائمة على النماذج نهجًا مختلفًا للتعلم المعزز بدلاً من تقييم قيمة الحالات والإجراءات ، تحاول توقع حالة البيئة بالنظر إلى الحالة الحالية والعمل. يتيح التعلم المعزز القائم على النموذج للعامل محاكاة مسارات مختلفة قبل اتخاذ أي إجراء.
</p>

<p dir='rtl' align='right'>
النهج القائمة على النماذج توفر للعامل البصير وتقليل الحاجة إلى جمع البيانات يدويًا، يمكن أن يكون هذا مفيدًا جدًا في التطبيقات التي يكون جمع بيانات وخبرات التدريب باهظ التكلفة وبطيئًا ، على سبيل المثال ، الروبوتات والسيارات ذاتية القيادة.

ولكن التحدي الرئيسي للتعلم المعزز القائم على النموذج هو أن خلق نموذج منطقي للبيئة قد يكون صعبًا للغاية، من الصعب جدًا نمذجة البيئات غير الحتمية ، مثل العالم الحقيقي ، وفي بعض الحالات ، يعمل المطورون على إنشاء عمليات محاكاة تقارب البيئة الحقيقية، ولكن حتى نماذج التعلم الخاصة بهذه البيئات المحاكاة ينتهي بها الأمر بأن تكون صعبة جدًا.

</p>

<p dir='rtl' align='right'>
ومع ذلك ، أصبحت الخوارزميات القائمة على النموذج شائعة في مشاكل حتمية مثل الشطرنط، البحث عن شجرة مونت كارلو هو طريقة تستند إلى نموذج شائع يمكن تطبيقها على البيئات الحتمية. 
</p>

<p dir='rtl' align='right'>
الأساليب المجمعة: للتغلب على أوجه قصور كل فئة من خوارزميات التعلم المعزز ، طور العلماء خوارزميات تدمج عناصر أنواع مختلفة من وظائف التعلم، على سبيل المثال ، تدمج خوارزميات الداما نقاط القوة القائمة على السياسة وعلى القيمة، وتستخدم هذه الخوارزميات تعليقات من دالة القيمة لتوجيه متعلم السياسة في الاتجاه الصحيح مما يؤدي إلى نظام أكثر كفاءة من العينات.
</p>

----

<h3 dir='rtl' align='right'>لماذا التعلم المعزز العميق؟ </h3>

<p dir='rtl' align='right'>
  حتى الآن ، لم نقول شيئًا عن الشبكات العصبية العميقة ، وفي الواقع ، يمكنك تنفيذ جميع الخوارزميات المذكورة أعلاه بأي طريقة تريدها على سبيل المثال ، فإن التعلم Q ، وهو نوع كلاسيكي من خوارزميات التعلم المعزز ، يخلق جدولًا لقيم الحالة ، والعمل ، والمكافآت ، مع تفاعل الوكيل مع البيئة ، وهذه الأساليب جيدة جدًا عندما تتعامل مع بيئة بسيطة جدًا يكون عدد الحالات والإجراءات صغيرًا جدًا

</p>

<p dir='rtl' align='right'>
ولكن عندما تتعامل مع بيئة معقدة ، حيث يمكن أن يصل العدد المشترك للإجراءات والحالات إلى أرقام ضخمة ، أو حيث تكون البيئة غير حتمية ويمكن أن تكون لها حالات غير محدودة بالفعل ، يصبح تقييم كل زوج ممكن من أزواج الإجراءات.
</p>


<p dir='rtl' align='right'>
في هذه الحالات ، ستحتاج إلى دالة تقريب يمكن أن تتعلم سياسات مثلى تستند إلى بيانات محدودة وهذا ما تفعله الشبكات العصبية الاصطناعية نظرًا لوظيفة الهندسة المعمارية الصحيحة وتحسينها ، يمكن أن تتعلم الشبكة العصبية العميقة سياسة مثلى دون الاطلاع على جميع الحالات الممكنة للنظام، ولا يزال وكلاء التعلم المعزز العميق بحاجة إلى كميات هائلة من البيانات (كآلاف ساعات اللعب في Dta و StarCraft ، ولكن يمكنهم معالجة المشاكل التي كان من المستحيل حلها باستخدام أنظمة التعلم المعزز الكلاسيكية.
</p>

<p dir='rtl' align='right'>
على سبيل المثال ، يمكن لنموذج RL العميق استخدام الشبكات العصبية التلافيفية لاستخراج معلومات الحالة من البيانات المرئية مثل مقاطع الفيديو ومصفوفات الفيديو والشبكات العصبية المتكررة يمكن أن تستخرج معلومات مفيدة من تسلسل الإطارات ، مثل مكان اتجاه الكرة أو إذا كانت السيارة تقف أو تتحرك، قدرة التعلم المعقدة هذه يمكن أن تساعد وكلاء RL على فهم بيئات أكثر تعقيدًا وتعيين ولاياتهم على الإجراءات.
</p>

<p dir='rtl' align='right'>
التعلم المعزز العميق مشابه للتعلم الآلي الخاضع للإشراف. ومع ذلك ، فإن التعلم المعزز العميق أيضًا له بعض التحديات الفريدة التي تجعله مختلفًا عن التعلم التقليدي تحت الإشراف.
</p>

<p dir='rtl' align='right'>
على غرار مشاكل التعلم تحت الإشراف ، حيث يكون لدى النموذج مجموعة من البيانات المصنفة ، يمكن لوكيل RL فقط الوصول إلى نتائج تجاربه الخاصة وقد يكون قادرًا على تعلم سياسة مثلى تستند إلى التجارب التي يجمعها عبر حلقات التدريب المختلفة، ولكن قد يفشل أيضًا العديد من المسارات المثلى التي يمكن أن تؤدي إلى سياسات أفضل يحتاج التعلم المعزز أيضًا إلى تقييم مسارات أزواج العمل القائمة على الدولة ، والتي من الصعب للغاية التعلم من مشاكل التعلم تحت الإشراف حيث يرتبط كل مثال تدريبي بنتائجها المتوقعة.
  
</p>

<p dir='rtl' align='right'>
يؤدي هذا التعقيد الإضافي إلى زيادة متطلبات البيانات لنماذج التعلم المعزز العميق ولكن على عكس التعلم تحت الإشراف ، حيث يمكن دمج بيانات التدريب وإعدادها مسبقًا ، تجمع نماذج التعلم المعزز العميق بياناتها أثناء التدريب، في بعض أنواع خوارزميات RL ، يجب تجاهل البيانات التي يتم جمعها في حلقة ما بعد ذلك ولا يمكن استخدامها لزيادة تسريع عملية ضبط النموذج في الحلقات المستقبلية.
  
</p>

----

<h3 dir='rtl' align='right'>التعلم المعزز العميق والذكاء الاصطناعي العام  </h3>

<p dir='rtl' align='right'>
مجتمع الذكاء الاصطناعي مقسومًا على المدى الذي يمكنك من خلالها المضي قدمًا في التعلم المعزز العميق يعتقد بعض العلماء أنه مع هندسة RL الصحيح ، يمكنك معالجة أي نوع من المشاكل بما في ذلك الذكاء العام الاصطناعي. التعلم المعزز هو نفس الخوارزمية التي أدت إلى الذكاء الطبيعي ، يعتقد هؤلاء العلماء ، وفي ضوء الوقت الكافي والطاقة والمكافآت الصحيحة ، يمكننا إعادة إنشاء الذكاء على مستوى الإنسان.
  
</p>

<p dir='rtl' align='right'>
يعتقد البعض أن التعلم المعزز لا يعالج بعض أهم مشاكل الذكاء الاصطناعي، على الرغم من جميع فوائدها ، يحتاج وكلاء التعلم المعزز العميق إلى مشاكل محددة جيدًا ولا يستطيعون اكتشاف مشاكل وحلول جديدة بأنفسهم ، تعتقد هذه المجموعة الثانية.
  
</p>

<p dir='rtl' align='right'>
  
على أي حال ، ما لا يمكن إنكاره هو أن التعلم المعزز العميق ساعد على حل بعض التحديات المعقدة للغاية وسيظل مجالًا مهمًا للاهتمام والبحث لمجتمع الذكاء الاصطناعي في الوقت الحالي.  
</p>

----

<h3 dir='rtl' align='right'> مصدر:</h3> 

* <a>https://bdtechtalks.com/2021/09/02/deep-reinforcement-learning-explainer/</a>





  
