{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUG41_Hw2210",
        "outputId": "b4ca3c67-7ef3-4e2f-841c-2ea414c2001c"
      },
      "source": [
        "!pip install deep_translator"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deep_translator in /usr/local/lib/python3.7/dist-packages (1.5.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from deep_translator) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.7/dist-packages (from deep_translator) (4.10.0)\n",
            "Requirement already satisfied: click<9.0.0,>=8.0.1 in /usr/local/lib/python3.7/dist-packages (from deep_translator) (8.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.3.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click<9.0.0,>=8.0.1->deep_translator) (4.8.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click<9.0.0,>=8.0.1->deep_translator) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click<9.0.0,>=8.0.1->deep_translator) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsTEwiaa2T54"
      },
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "import pandas as pd\n",
        "import time\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9EqPYeg2nzy"
      },
      "source": [
        "springer = pd.read_excel(r'/content/springer_articles.xlsx')"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "6haenCaN-MyL",
        "outputId": "53f97f56-d2f7-4d9c-cc65-dfb52ce01f00"
      },
      "source": [
        "springer.Content[0]"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Abstract\\r\\nThe application of various sensors in hospitals has enabled the widespread utilization of multivariate time series signals for chronic disease diagnosis in the data-driven world. The key challenge is how to model the complex temporal (linear and nonlinear) correlations among multiple longitudinal variables. Due to scarcity of labels in practice, unsupervised learning methods have already become indispensable. However, state-of-the-art approaches mainly focus on the extraction of linear correlation-induced feature connectivity network, e.g., Pearson correlation, partial correlation, etc. To this end, for chronic disease (e.g., Parkinson disease) diagnosis, an unsupervised representation learning method is first designed to obtain informative correlation-aware signals from multivariate time series data. At the core is a contrastive learning framework with a graph neural network (GNN) encoder to capture the inter-correlation and intra-correlation of multiple longitudinal variables. Then, the previously learned representations are sent to a simple fully connected neural network, which can be trained using fewer labels compared with end-to-end complex supervised learning models. Further, to assist the decision-making process in the high-stake chronic disease detection task, model uncertainty quantification is enabled according to evidential theory. The experimental results on two public Parkinson’s disease data sets show the expressiveness of the learned embeddings, and the final lightweight classifier achieves the best performance.\\r\\nIntroduction\\r\\nWith the development of science and technology, people’s living standards have been greatly improved. However, chronic diseases such as heart disease, cancer, and diabetes are still the leading causes of death and disability in many places around the world. For instance, many old people are suffering from cardiovascular disease, Alzheimer’s disease (AD) [1], and Parkinson’s disease (PD) [2]. What’s more, some chronic diseases such as diabetes and chronic kidney are becoming more and more prevalent among younger age groups [3]. Indeed, people of all ages might suffer from chronic diseases. Therefore, timely diagnosis becomes very important.At present, the widespread deployment of sensors in hospitals has helped accumulate multivariate time series data for patients with chronic diseases or healthy people. Hence, recent academic world has been able to conduct various multivariate time series analysis for chronic diseases diagnosis. In particular, the most efficient approaches adopt the similar pipeline, i.e., extraction of expressive signals from multivariate time series via an unsupervised learning method; then, these signals are sent to a lightweight classifier for supervised learning. Various works have devoted attention to the first step. For instance, it has been shown that feature connectivity network [4,5,6] can highly summarize the structural composition of a complex dynamic system (e.g., the multivariate time series data of a patient with chronic diseases), and then the network serves as a high degree of information for various machine learning tasks. Therefore, state-of-the-art chronic disease detection algorithms mainly focus on the construction of effective feature connectivity matrices. However, they mainly explore to utilize the linear relationships. For example, both Qu et al. [7] and Cheng et al. [8] propose to learn the partial correlation-based connectivity matrix via either shared template regularization [7] or the sparse variational regularization [8]. Even though graph neural networks (GNNs) have been widely adopted to automatically extract the high-level complex feature correlations from multivariate time series data [9,10,11,12], present frameworks are not ideal for multivariate longitudinal data-induced chronic disease diagnosis tasks. On the one hand, most of the frameworks are usually proposed under an ideal fully supervised learning scenario, which is often impossible in a clinical disease diagnosis task due to the difficulties of obtaining the labels; on the other hand, they tend to model the features inter-correlations without careful consideration of the intra-correlations along the temporal series.Worse still, current chronic diseases (e.g., PD) classification algorithms [7, 8, 13,14,15] fail to quantify the model uncertainty [16, 17], which is extremely important in such high-stake healthcare analysis task. For example, if some abnormal noises are added to the multivariate time series data, the model will usually give a wrong prediction with high confidence since the final predictions are obtained only based on the output probability score; thus, it inherently is an overconfident and unreliable model [10]. In contrast, it is much more reasonable to enable the model to filter out the potential uncertain samples that need human experts to judge, i.e., chronic disease diagnosis task needs a confidence-calibrated and uncertainty-aware model.Considering the above limitations, this paper proposes an effective chronic disease diagnosis framework consisting of an unsupervised learning module and an uncertainty-aware supervised learning module. More specifically, in the unsupervised representation learning module, a contrastive learning mechanism is utilized to capture the longitudinal feature intra-correlations, while high-order multiple variables’ inter-correlations are modeled using a GNN, which is chosen as a time span encoder and coupled with the contrastive learning mechanism. Then, the learned representations are sent to a fully connected neural network for supervised learning. Note that since the unsupervised learning module aims at capturing as much as informative signals as possible, any lightweight classifier can be utilized. Further, the evidential theory is leveraged to form the confidence-calibrated and uncertainty-aware classifier. Finally, the effectiveness of the proposed algorithm model is verified based on two public Parkinson's disease data sets.Our contributions are summarized as follows:\\r\\n\\r\\nWe propose an uncertainty-aware chronic disease diagnosis framework, which first extracts informative signals from multivariate time series data via the graph neural newwork and an unsupervised learning paradigm.\\r\\n\\r\\n\\r\\nWe are the first to utilize the time-contrastive learning framework coupled with a graph neural network encoder to model the complex multilevel correlations hidden in the clinical longitudinal data.\\r\\n\\r\\n\\r\\nExperimental results on two public chronic disease classification data sets show the superior performance of the patient representations obtained via the unsupervised learning module. And, the uncertainty quantification indeed helps improve the robustness and reliability of the model.\\r\\nRelated works\\r\\nResearch on chronic disease diagnosis algorithmsChronic disease diagnosis tasks have always attracted widespread attention from both the academic and industrial world. Most of the existing approaches belong to the fully supervised learning setting, which cannot make use of unlabeled data and depends on complex feature processing process [13,14,15]. Recent progresses have been made based on the concept of functional connectivity network [4,5,6], which is constructed via an unsupervised learning model firstly and then is applied to downstream tasks (e.g., classification). For instance, PCC [18] constructed the feature connectivity network based on Pearson correlation. PCC Fisher [19] built the feature connected network based on Pearson correlation and Fisher transform. SR-C [20] captured the sparse feature connected network by Pearson correlation identification with L1 regularization, while SR-PC [21] discovered the feature connectivity network based on partial correlation with lasso regression. Reweighted LASSO [22] constructed feature connected network based on weighted penalty matrix and LASSO regression. SAMCN LASSO [8] utilized gradient meta-learning method. SAMCN Structural LASSO [8] constructed the features connectivity network based on gradient meta-learning, sparse weight penalty matrix and LASSO regression. However, these methods depend on complex feature processing and calculation and can only capture linear relationships, e.g., Pearson correlation, partial correlation, etc. Therefore, more comprehensive algorithms should be designed to capture complex temporal (linear and nonlinear) correlation information.Multivariate time series modelingCurrently, multivariate time series modeling can be categorized into supervised and unsupervised learning. Supervised learning methods [10,11,12] mainly follow an end-to-end learning pipeline, i.e., leveraging different basic neural network components (e.g., CNN, LSTM, etc.) to extract time span features, which are then sent to some well-known machine learning classifiers. On the one hand, fully supervised learning paradigm requires intensive labels, which are often not available for some safety–critical tasks like chronic disease diagnosis. On the other hand, direct usage of labeled data for error backpropagation may introduce some noise leading to unsatisfactory optimization effect [41]. Therefore, unsupervised learning approaches have gradually become the priority choice. Lei [23] proposed an unsupervised learning method based on the distance between the learning representations and the original time series; Malhotra et al. [24] designed the encoder based on recurrent neural network, reconstructed the input time series through the decoder, and combined the encoder and decoder for unsupervised learning. Franceschi et al. [25] proposed the time-contrastive learning framework, which combines triplet loss, causal convolution, and hole convolution network through positive and negative sampling of time series data. Though this method is one of the most simple and efficient unsupervised learning methods for time series data, it fails to model the inter-variable interactions, inspiring us to design more sophisticated unsupervised learning modules.Along this line, graph-structured data mining provides insights that graph can describe the relationships between different nodes, such as social network, transportation network, and human skeleton graph structure. Graph neural network (GNN) can aggregate the feature information of adjacent nodes in graph data. This way of transferring information through graph structure can effectively process graph data and obtain entity representation with stronger expression ability [26, 27]. Various multivariate modeling methods [28, 29] regard each feature as a node in the graph and then utilize graph neural networks to model the complex multilevel correlations. However, most GNN encoder learning methods mainly consider the inter-correlations without considering the intra-correlations. On the contrary, TAGCN [30] uses K convolution kernels of different sizes for feature extraction in each layer. The receptive fields of K convolution kernels are 1\\u2009~\\u2009k, which has strong information aggregation ability, good adaptability, low computational complexity, and can be used in undirected graph or directed graph. Therefore, in this paper, the features of Parkinson's patients are regarded as nodes in the graph structure and TAGCN is used as the encoder of the model.Uncertainty evaluationMost of the existing AI models are known to be prone to overconfidence and are difficult to produce effective patient-level uncertainty scores [31], thus affecting the model reliability, especially in the clinical context (e.g., chronic disease diagnosis). What’s more, even the model has excellent performance for a certain patient population, there are still some out-of-distribution (OOD) patients whose predictions are quite uncertain and often difficult to be detected. Therefore, estimating the uncertainty in an AI system is important to improve the safety and informativeness of the black-box model. For uncertainty quantification, Bayesian neural network (BNN) [16, 32] has achieved good results in providing model uncertainty estimation. At the core of the model is regarding each parameter as distributions rather than fixed parameters, which thereby relies on complex calculation process. Murat et al. [17] proposed a data uncertainty modeling method based on evidential theory, which does not need complex calculation, and can be easily combined with a variety of neural networks. It has the characteristics of simple, efficient, and good adaptability. Therefore, it motivates us to inject it into the supervised learning module of our framework.\\r\\nResearch methods\\r\\nGraph construction methodFirstly, two graph structures are constructed based on the preprocessed PPMI and PS data (see the Sect. 4.1 for details). Note that, in this paper, the features of Parkinson’s patients are regarded as nodes in the graph structure, and the graph structure is constructed by measuring the Euclidean distance between time series features. Given the Parkinson’s disease data set \\\\(X\\\\in {\\\\mathbb{R}}^{N\\\\times T\\\\times D}\\\\) and labels \\\\(Y\\\\in {\\\\mathbb{R}}^{N\\\\times 1}\\\\), where \\\\(N\\\\) denotes the dimension of samples, \\\\(T\\\\) denotes the dimension of time, and \\\\(D\\\\) denotes the dimension of features. Flattening the sample dimension and time dimension, and only keeping the feature dimension, we can obtain the feature vectors: \\\\(\\\\tilde{D}_{N \\\\times T}^{i} = \\\\left\\\\{ {x_{1}^{i} ,x_{2}^{i} , \\\\ldots ,x_{N \\\\times T}^{i} } \\\\right\\\\}\\\\). The Euclidean distance matrix can be calculated between two feature vectors:$$ \\\\tilde{d}_{{\\\\left( {i,j} \\\\right)}} = \\\\sqrt {\\\\mathop \\\\sum \\\\limits_{1}^{{N \\\\times {\\\\text{T}}}} \\\\left( {\\\\tilde{D}_{{N \\\\times {\\\\text{T}}}}^{i} - \\\\tilde{D}_{{N \\\\times {\\\\text{T}}}}^{j} } \\\\right)^{2} } $$\\r\\n                    (1)\\r\\n                Furthermore, the symmetric adjacency matrix is obtained:$$ A_{{\\\\left( {i,j} \\\\right)}} = \\\\frac{1}{{1 + \\\\tilde{d}_{{\\\\left( {i,j} \\\\right)}} }}. $$\\r\\n                    (2)\\r\\n                Note that, in this paper, i and j can be same in the calculation process of Eq.\\xa0(1) and it means the correlation coefficient between features i and j is one after the computation of Eq.\\xa0(2). At the same time, in order to reduce the influence of noise and improve the model performance, robustness and training efficiency, sparsity techniques are adopted. Specifically, the edge connection between two nodes whose weight value is lower than a certain threshold will be deleted, so as to achieve the sparse adjacent matrix. Consider the pruning process of the first row of the matrix:$$ A_{1j}^{*} = \\\\left\\\\{ {\\\\begin{array}{*{20}l} {A_{1,j} ,} \\\\hfill & {A_{1,j} > \\\\eta \\\\cdot \\\\max \\\\left( {A_{1,j} } \\\\right)} \\\\hfill \\\\\\\\ {0,} \\\\hfill & {A_{1,j} \\\\le \\\\eta \\\\cdot \\\\max \\\\left( {A_{1,j} } \\\\right)} \\\\hfill \\\\\\\\ \\\\end{array} } \\\\right. $$\\r\\n                    (3)\\r\\n                where \\\\(\\\\eta \\\\) denotes the sparsity control factor, and \\\\(\\\\mathrm{max}({A}_{1,j})\\\\) denotes the largest weight value when the first feature is connected with all other features. When the weight value of the first feature connected with another feature is lower than threshold \\\\(\\\\eta \\\\cdot \\\\mathrm{max}({A}_{1,j})\\\\), the edge connection between the feature and the first feature will be deleted (the weight value will be set as zero). In the same way, we can get the final matrix \\\\({A}_{ij}^{*}\\\\) after filtering all rows of adjacency matrix.TAGCN feature encodingFor the \\\\(n\\\\)th hidden layer of TAGCN, the input is the graph structure and the corresponding node feature data or the output of the \\\\(n\\\\)th−1 hidden layer. When convolution operation is performed, each node in the graph structure has \\\\(t\\\\in T\\\\) features. For the graph convolution operation in the \\\\(n\\\\)th hidden layer, the \\\\(k\\\\)th feature data on all nodes as the input can be defined as \\\\(x_{k}^{n} \\\\in {\\\\mathbb{R}}^{D} , \\\\quad k = 1,2,3, \\\\ldots ,t\\\\). The specific composition of the feature vector is determined by the node index in the graph structure, and we define \\\\({{G}_{k,f}^{n}\\\\in {\\\\mathbb{R}}}^{D\\\\times D}\\\\) as the \\\\(f\\\\)th convolution kernel. In essence, graph convolution is the product operation between the matrix vectors. For example, \\\\({G}_{k,f}^{n}{x}_{k}^{n}\\\\) denotes an operation of graph convolution.In the process of graph convolution operation, the adjacency matrix is first normalized to ensure that its eigenvalues are in the unit circle and so as to further ensure the stability of graph convolution operation. The normalized adjacency matrix \\\\({\\\\overline{A} }^{*}\\\\) can be described as:$$ \\\\begin{aligned} & \\\\overline{A}^{*} = \\\\overline{D}^{ - 1/2} A^{*} \\\\overline{D}^{ - 1/2} , \\\\\\\\ & \\\\overline{D} = {\\\\text{diag}}\\\\left[ {\\\\overline{d}\\\\left( i \\\\right)} \\\\right], \\\\\\\\ & \\\\overline{d}\\\\left( i \\\\right) = \\\\mathop \\\\sum \\\\limits_{j} A_{ij}^{*} . \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (4)\\r\\n                Further, we can get the convolution kernel by translation invariance principle [30]:$$ G_{k,f}^{n} = \\\\mathop \\\\sum \\\\limits_{c = 0}^{C} g_{k,f,c}^{n} \\\\overline{A}^{*} $$\\r\\n                    (5)\\r\\n                \\r\\nwhere \\\\({g}_{k,f,c}^{n}\\\\) are the graph filter polynomial coefficients. \\\\(C\\\\) is hyperparameter and denotes convolution kernels with different sizes used in each convolutional layer, which is similar to GoogLeNet [39].Then, graph convolution kernels with different sizes slide on the graph-structured data and extract different scales of features. Then, they are combined linearly to form the feature mapping of the output of the \\\\(n\\\\) th hidden layer:$$ y_{f}^{n} = \\\\mathop \\\\sum \\\\limits_{k}^{t} G_{k,f}^{n} x_{k}^{n} + b_{f}^{n} 1_{D} $$\\r\\n                    (6)\\r\\n                \\r\\nwhere \\\\({b}_{f}^{n}\\\\) is trainable parameters, and \\\\({1}_{D}\\\\) is a vector whose element values of dimension D are all one. \\\\({G}_{k,f{x}_{k}^{n}}^{n}\\\\) represents an efficient graph convolution operation on a graph with arbitrary topology.Then, the nonlinear operation is performed on the obtained feature map to get the final output:$$ x_{f}^{n + 1} = \\\\sigma \\\\left( {y_{f}^{n} } \\\\right), $$\\r\\n                    (7)\\r\\n                \\r\\nwhere \\\\(\\\\sigma \\\\) denotes activation function, e.g., RELU.Finally, the output of the graph convolution is transformed into latent space again, and the useful information is further extracted and integrated through the linear function:$$ V^{n + 1} = W_{\\\\eta } x_{f}^{n + 1} + b_{\\\\eta } $$\\r\\n                    (8)\\r\\n                Unsupervised learning based on time-contrastive learningThe most traditional GNN encoders only concentrate on learning node embeddings by aggregating the neighbor nodes information (focus on node level) and then obtain the information of the whole graph, conducting the graph classification task. However, the inter-correlations of the longitudinal nodes between the different graphs and the intracorrelations of the longitudinal nodes among the same graph should also be considered (graph level). In multivariate time series data, each multivariate feature can be seen a node in the graph, few researchers further pay attention to the inter-correlations and intra-correlations in the obtained graph, which limits the performance of the encoders. Hence, in our work, we adopt the time-contrastive learning framework to consider both the intra-correlations and inter-correlations. More specifically, given a time series data \\\\({y}_{i}\\\\) of a sample. Firstly, a random subseries (i.e., a subsequence of a time series composed by consecutive time steps of this time series) \\\\({x}^{\\\\mathrm{rep}}\\\\) of \\\\({y}_{i}\\\\) is sampled, and then subseries  \\\\({x}^{\\\\mathrm{pos}}\\\\) of \\\\({x}^{\\\\mathrm{rep}}\\\\) is obtained as positive pairs. On the one hand, they come from a same time series of a same sample; on the other hand, the \\\\({x}^{\\\\mathrm{rep}}\\\\) can be seen as a word context and the \\\\({x}^{\\\\mathrm{pos}}\\\\) can be seen as the word in the context. Hence, they should have similar representations through the encoder (intra-correlations). Further, the subseries  \\\\({x}^{\\\\mathrm{neg}}\\\\) of another time series data is randomly selected as negative sample. Similarly, on the one hand, \\\\({x}^{\\\\mathrm{rep}}\\\\) and \\\\({x}^{\\\\mathrm{neg}}\\\\) come from a different time series of a different sample; on the other hand, the \\\\({x}^{\\\\mathrm{rep}}\\\\) can be seen as a word context and the \\\\({x}^{\\\\mathrm{neg}}\\\\) can be seen as a random word form the another context. Hence, the representation of the \\\\({x}^{\\\\mathrm{neg}}\\\\) obtained by the encoder should be far away from that of \\\\({x}^{\\\\mathrm{rep}}\\\\) (inter-correlations).Besides, to improve the stability and convergence of the training procedure as well as the experimental results of the learned representations, several negative samples \\\\({{(x}_{k}^{\\\\mathrm{neg}})}_{k\\\\in [1,K]}\\\\) (K denotes the number of random selected negative samples in all time series data) are chosen independently at random in training process. Specially, define the length of time series data \\\\({y}_{i}\\\\) as \\\\({s}_{i}\\\\), where \\\\(i\\\\in [1,N]\\\\) and N denotes the number of time series data. \\\\({s}^{\\\\mathrm{pos}}=\\\\mathrm{size}\\\\left({x}^{\\\\mathrm{pos}}\\\\right),\\\\in [1,{s}_{i}]\\\\), \\\\({s}^{\\\\mathrm{rep}}=\\\\mathrm{size}\\\\left({x}^{\\\\mathrm{rep}}\\\\right),\\\\in [{s}^{\\\\mathrm{pos}},{s}_{i}]\\\\) and \\\\({s}_{k}^{\\\\mathrm{neg}}=\\\\mathrm{size}\\\\left({x}_{k}^{\\\\mathrm{neg}}\\\\right),\\\\in [1,{s}_{k}]\\\\) are picked uniformly at random, where \\\\(size(\\\\cdot )\\\\) means the length of time series data \\\\(x\\\\) and \\\\(k=\\\\mathrm{1,2},\\\\dots ,K\\\\). Then, \\\\({x}^{\\\\mathrm{rep}}\\\\), \\\\({x}^{\\\\mathrm{pos}}\\\\) and \\\\({x}_{k}^{\\\\mathrm{neg}}\\\\) are picked uniformly at random among subseries of \\\\({y}_{i}\\\\) of length \\\\({s}^{\\\\mathrm{rep}}\\\\), \\\\({s}^{\\\\mathrm{pos}}\\\\) and \\\\({s}_{k}^{\\\\mathrm{neg}}\\\\).Finally, the triplet loss function can induce the encoder to have similar representation for similar time series and then learn meaningful encoding representations. The loss function of the unsupervised learning part is:$$ \\\\begin{aligned} & L^{{{\\\\text{tri}}}} = \\\\left[ { - \\\\log (\\\\sigma \\\\left( {f\\\\left( {x^{{{\\\\text{rep}}}} ,\\\\theta } \\\\right)^{{\\\\text{T}}} f\\\\left( {x^{{{\\\\text{pos}}}} ,\\\\theta } \\\\right)} \\\\right)} \\\\right. \\\\\\\\ & \\\\quad - \\\\mathop \\\\sum \\\\limits_{k = 1}^{K} \\\\log \\\\left( {\\\\sigma \\\\left( { - f\\\\left( {x^{{{\\\\text{rep}}}} ,\\\\theta } \\\\right)^{{\\\\text{T}}} } \\\\right.} \\\\right. \\\\\\\\ & \\\\quad \\\\left. {\\\\left. {\\\\left. {f\\\\left( {x_{h}^{{{\\\\text{neg}}}} ,\\\\theta } \\\\right)} \\\\right)} \\\\right)} \\\\right] \\\\cdot \\\\kappa \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (9)\\r\\n                \\r\\nwhere \\\\(\\\\sigma \\\\) denotes sigmoid activation function here and \\\\(f(\\\\cdot ,\\\\theta )\\\\) denotes neural networks (e.g., TAGCN encoder used in this paper) with trainable parameters. \\\\(\\\\kappa \\\\) is used to control the strength of the loss. The loss function will force the encoder to better distinguish \\\\({x}^{\\\\mathrm{rep}}\\\\) and \\\\({x}^{\\\\mathrm{neg}}\\\\) after optimization, and make \\\\({x}^{\\\\mathrm{rep}}\\\\) and \\\\({x}^{\\\\mathrm{pos}}\\\\) have similar representations. Therefore, the triplet loss function leverages TAGCN to capture the high-order inter-correlation and intra-correlation of the multivariate time series.Dirichlet distribution and uncertaintyEvidential uncertainty measurement is derived from the Dempster Shafer Theory of Evidence (DST), which is a generalization of the Bayesian theory to subjective probabilities [40]. In particular, subjective logic (SL) formalizes DST’s notion of belief mass assignments over a frame of discernment based on a Dirichlet distribution with Z parameters \\\\(\\\\alpha = \\\\left[ {\\\\alpha_{1} ,\\\\alpha_{2} , \\\\ldots ,\\\\alpha_{Z} } \\\\right]\\\\)  (subjective opinion\\\\(\\\\alpha \\\\)). Normally, the output of the standard neural network is the probability of each possible category of the sample, and the Dirichlet distribution based on the evidence theory represents the density function of the probability, so it can simulate the second-order probability and uncertainty [34, 35]. The Dirichlet distribution is a probability density function (pdf) for possible values of the probability mass function (pmf) \\\\(p\\\\) and can be expressed by Z parameters\\\\(\\\\alpha \\\\):$$ {\\\\text{Dir}}\\\\left( {p|\\\\alpha } \\\\right) = \\\\left\\\\{ {\\\\begin{array}{*{20}l} {\\\\frac{1}{B\\\\left( \\\\alpha \\\\right)}\\\\mathop \\\\prod \\\\limits_{i = 1}^{Z} p_{i}^{{\\\\alpha_{i - 1} }} ,} \\\\hfill & { p \\\\in S_{Z} } \\\\hfill \\\\\\\\ 0 \\\\hfill & {{\\\\text{otherwise}}} \\\\hfill \\\\\\\\ \\\\end{array} } \\\\right. $$\\r\\n                    (10)\\r\\n                \\r\\nwhere \\\\(p\\\\) is the probability mass function and \\\\(\\\\alpha =[{\\\\alpha }_{1},\\\\dots ,{\\\\alpha }_{Z}]\\\\) are the parameters of Dirichlet distribution. \\\\(Z\\\\) denotes the label category. \\\\(B\\\\left(\\\\alpha \\\\right)\\\\) is a polynomial beta function in \\\\(Z\\\\) dimension [36]. \\\\({S}_{Z}\\\\) is the Z-dimensional unit simplex:$$ S_{Z} = \\\\left\\\\{ {p\\\\left| {\\\\mathop \\\\sum \\\\limits_{i = 1}^{Z} p_{i} = 1\\\\,} \\\\right.{\\\\text{and}}\\\\, 0 \\\\le p_{1} , \\\\ldots ,p_{Z} \\\\le 1} \\\\right\\\\} $$\\r\\n                    (11)\\r\\n                Based on SL, the relationship between belief mass for each singleton \\\\({b}_{z}\\\\) and uncertainty \\\\(u\\\\) is computed as:$$ u + \\\\mathop \\\\sum \\\\limits_{z = 1}^{Z} b_{z} = 1. $$\\r\\n                    (12)\\r\\n                \\r\\nwhere \\\\(u\\\\ge 0\\\\) and \\\\({b}_{z}\\\\ge 0\\\\) for \\\\(z=\\\\mathrm{1,2},\\\\dots ,Z\\\\). A belief mass \\\\({b}_{z}\\\\) for a class label \\\\(z\\\\) is computed using evidence \\\\({e}_{z}\\\\) (\\\\(e=[{e}_{1},{e}_{2},\\\\dots ,{e}_{Z}]\\\\)). More specifically,$$ b_{z} = \\\\frac{{e_{z} }}{S}, u = \\\\frac{Z}{S}, $$\\r\\n                    (13)\\r\\n                $$ \\\\alpha_{z} = e_{z} + 1, $$\\r\\n                    (14)\\r\\n                $$ e_{z} = \\\\zeta \\\\left( {\\\\hat{y}^{s} } \\\\right), $$\\r\\n                    (15)\\r\\n                $$ S = \\\\mathop \\\\sum \\\\limits_{z = 1}^{Z} \\\\alpha_{z} . $$\\r\\n                    (16)\\r\\n                \\r\\nwhere \\\\(Z\\\\) denotes the number of labels, \\\\({\\\\alpha }_{z}\\\\) is Dirichlet parameters,  \\\\({\\\\widehat{y}}^{s}\\\\) is the output vector before being sent to softmax layer, and  \\\\(\\\\zeta \\\\left(\\\\cdot \\\\right)\\\\) denotes an activation layer, e.g., ReLU. \\\\({e}_{z}\\\\) is the amount of evidence and \\\\(S\\\\) is the Dirichlet strength.Further, the expectation of Dirichlet distribution based on neural network evidence theory can be computed as:$$ \\\\hat{p}^{z} = \\\\alpha_{z} /S. $$\\r\\n                    (17)\\r\\n                At present, the well-known Type \\\\(\\\\mathrm{\\\\rm I}\\\\mathrm{\\\\rm I}\\\\) maximum likelihood is always used to optimize the Dirichlet distribution \\\\(D({p}_{i}|{\\\\alpha }_{i})\\\\):$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = - {\\\\text{log}}\\\\left( {\\\\int \\\\mathop \\\\prod \\\\limits_{j = 1}^{Z} p_{ij}^{{y_{ij} }} \\\\frac{1}{{B\\\\left( {\\\\alpha_{i} } \\\\right)}}\\\\mathop \\\\prod \\\\limits_{j = 1}^{Z} p_{ij}^{{\\\\alpha_{ij} - 1}} {\\\\text{d}}P_{i} } \\\\right) \\\\\\\\ & \\\\quad = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} y_{ij} \\\\left( {\\\\log \\\\left( {S_{i} } \\\\right) - {\\\\text{log}}\\\\left( {\\\\alpha_{ij} } \\\\right)} \\\\right). \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (18)\\r\\n                \\r\\nwhere \\\\({y}_{i}\\\\) is a one-hot vector encoding of the ground-truth class with \\\\({y}_{ij}=1\\\\) and \\\\({y}_{iz}=0\\\\) for all \\\\(z\\\\ne j\\\\) and \\\\({\\\\alpha }_{i}\\\\) is the parameters of the Dirichlet density on the predictors.We treat \\\\(D({p}_{i}|{\\\\alpha }_{i})\\\\) as a prior on the likelihood Multi \\\\(({y}_{i}|{p}_{i})\\\\) and obtain the negated logarithm of the marginal likelihood by integrating out the class probabilities. Finally, we minimize the parameters of the Dirichlet density \\\\({\\\\alpha }_{i}\\\\).More specially, following Eq.\\xa0(18), for better quantifying the uncertainties \\\\(u\\\\) of different samples, a cross-entropy loss and Bayes risk-based loss function is defined:$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = \\\\int \\\\left[ {\\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} - y_{ij} \\\\left( {{\\\\text{log}}\\\\left( {p_{ij} } \\\\right)} \\\\right)} \\\\right] \\\\\\\\ & \\\\frac{1}{{B\\\\left( {\\\\alpha_{i} } \\\\right)}}\\\\mathop \\\\prod \\\\limits_{j = 1}^{Z} p_{ij}^{{\\\\alpha_{ij} - 1}} {\\\\text{d}}P_{i} \\\\\\\\ & = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} y_{ij} \\\\left( {\\\\psi \\\\left( {S_{i} } \\\\right) - \\\\psi \\\\left( {\\\\alpha_{ij} } \\\\right)} \\\\right), \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (19)\\r\\n                \\r\\nwhere \\\\(\\\\psi \\\\left(\\\\cdot \\\\right)\\\\) is the digamma function. Following Eq.\\xa0(18), we also define a squares (\\\\(\\\\left\\\\| {y_{i} - p_{i} } \\\\right\\\\|_{2}^{2}\\\\))-based loss function:$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = \\\\int \\\\left\\\\| {y_{i} - p_{i} } \\\\right\\\\|_{2}^{2} \\\\frac{1}{{B\\\\left( {\\\\alpha_{i} } \\\\right)}} \\\\\\\\ & \\\\mathop \\\\prod \\\\limits_{i = 1}^{Z} p_{ij}^{{\\\\alpha_{ij} - 1}} {\\\\text{d}}P_{i} \\\\\\\\ & = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} {\\\\mathbb{E}}\\\\left[ {y_{ij}^{2} - 2y_{ij} p_{ij} + p_{ij}^{2} } \\\\right] \\\\\\\\ & = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left[ {y_{ij}^{2} - 2y_{ij} {\\\\mathbb{E}}[p_{ij} ] + {\\\\mathbb{E}}[p_{ij}^{2} ]} \\\\right]. \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (20)\\r\\n                The above two loss functions both can be used to optimize the Dirichlet distribution, and we will choose the better one through the experiments and theoretical analysis in the following sections.In Dirichlet distribution, if a sample cannot be correctly classified and its total evidence will be close to zero, e.g., \\\\(S\\\\approx K\\\\), corresponding to the uniform distribution and indicating the total uncertainty, e.g., \\\\(u\\\\approx 1\\\\). This is not a good condition and means the classifier fails to differentiate these samples. If the Dirichlet distribution converges to uniform distribution early, the model would not be good at quantifying the uncertainties of most samples. Hence, a Kullback–Leibler (KL) divergence term is incorporated into the defined loss function:$$ \\\\begin{aligned} & {\\\\mathcal{L}}\\\\left( {\\\\Theta } \\\\right) = \\\\mathop \\\\sum \\\\limits_{i = 1}^{N} {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) + \\\\varrho_{t} \\\\\\\\ & \\\\quad \\\\mathop \\\\sum \\\\limits_{i = 1}^{N} KL\\\\left[ {\\\\left. {\\\\hat{D}(p_{i} |\\\\tilde{\\\\alpha }_{i} )} \\\\right\\\\|} \\\\right. \\\\\\\\ & \\\\quad \\\\hat{D}\\\\left. {\\\\left( {p_{i} \\\\left| { < 1, \\\\ldots ,1 > } \\\\right.} \\\\right)} \\\\right], \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (21)\\r\\n                where \\\\({\\\\varrho }_{t}=\\\\mathrm{min}\\\\left(1.0,\\\\frac{t}{10}\\\\right)\\\\in [\\\\mathrm{0,1}]\\\\) is the annealing coefficient, \\\\(t\\\\) is the index of the current training epoch, and \\\\({\\\\stackrel{\\\\sim }{\\\\alpha }}_{i}={y}_{i}+(1-{y}_{i})\\\\odot {\\\\alpha }_{i}\\\\) is the Dirichlet parameters after removal of the nonmisleading evidence from predicted parameters \\\\({\\\\alpha }_{i}\\\\) for sample \\\\(i\\\\). The KL divergence term in the loss function can be calculated as:$$ \\\\begin{aligned} & KL[\\\\hat{D}(p_{i} |\\\\tilde{\\\\alpha }_{i} )||\\\\hat{D}(p_{i} |1)] \\\\\\\\ & = \\\\log \\\\left( {\\\\frac{{\\\\Gamma \\\\left( {\\\\mathop \\\\sum \\\\nolimits_{z = 1}^{Z} \\\\tilde{\\\\alpha }_{iz} } \\\\right)}}{{\\\\Gamma \\\\left( Z \\\\right)\\\\mathop \\\\prod \\\\nolimits_{z = 1}^{Z} \\\\Gamma \\\\left( {\\\\tilde{\\\\alpha }_{iz} } \\\\right)}}} \\\\right) + \\\\mathop \\\\sum \\\\limits_{z = 1}^{Z} \\\\left( {\\\\tilde{\\\\alpha }_{iz} - 1} \\\\right) \\\\\\\\ & \\\\quad \\\\left[ {\\\\psi \\\\left( {\\\\tilde{\\\\alpha }_{iz} } \\\\right) - \\\\psi \\\\left( {\\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\tilde{\\\\alpha }_{ij} } \\\\right)} \\\\right], \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (22)\\r\\n                where \\\\(\\\\Gamma \\\\left(\\\\cdot \\\\right)\\\\) is the gamma function. The annealing coefficient \\\\({\\\\varrho }_{t}\\\\) can control the effect of the KL divergence, and so the Dirichlet distribution will be induced to explore the Dirichlet parameter space as far as possible. The model can be induced to avoid premature convergence to the uniform distribution for the misclassified samples and try to classify them correctly in the training epochs as far as possible.In the process of experiment, we found that the cross-entropy loss and Bayes risk-based loss function tend to generate excessively high belief masses for classes and exhibit relatively less stable performance than the squares loss-based loss function. What’s more, the relationship between the expectation and variance can be described as:$$ {\\\\mathbb{E}}\\\\left[ {p_{ij}^{2} } \\\\right] = {\\\\mathbb{E}}\\\\left[ {p_{ij} } \\\\right]^{2} + {\\\\text{Var}}\\\\left( {p_{ij} } \\\\right). $$\\r\\n                    (23)\\r\\n                Hence, based on the relationship between the expectation and variance, the squares loss-based loss function can be transformed into more interpretable form:$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left( {y_{ij} - {\\\\mathbb{E}}\\\\left[ {p_{ij} } \\\\right]} \\\\right)^{2} + {\\\\text{Var}}\\\\left( {p_{ij} } \\\\right) \\\\\\\\ & \\\\quad = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left( {y_{ij} - \\\\alpha_{ij} /S_{i} } \\\\right)^{2} \\\\\\\\ & \\\\quad \\\\quad + \\\\frac{{\\\\alpha_{ij} \\\\left( {S_{i} - \\\\alpha_{ij} } \\\\right)}}{{S_{i}^{2} \\\\left( {S_{i} + 1} \\\\right)}} \\\\\\\\ & \\\\quad = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left( {y_{ij} - { }\\\\hat{p}_{ij} } \\\\right)^{2} + \\\\frac{{\\\\hat{p}_{ij} \\\\left( {1 - \\\\hat{p}_{ij} } \\\\right)}}{{S_{i} + 1}}. \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (24)\\r\\n                By minimizing the expectation error and variance of the Dirichlet distribution, it can assign belief mass for different samples reasonably and bette\""
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AruIPaK65veS"
      },
      "source": [
        "for i in range(len(springer.Content)):\n",
        "  springer.Content[i] = str(springer.Content[i]).replace('Abstract\\r\\n', '')\n",
        "#   springer.Content[i] = re.sub(r'[^a-zA-Z.]', ' ', str(springer.Content[i]))\n",
        "#   springer.Content[i] = springer.Content[i].replace('\\n', '')\n",
        "#   springer.Content[i] = springer.Content[i].replace('  ', '')"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "-8eavtA3A0ea",
        "outputId": "0e78abe6-0a12-4447-e68d-1982182f5b85"
      },
      "source": [
        "springer.Content[0]"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The application of various sensors in hospitals has enabled the widespread utilization of multivariate time series signals for chronic disease diagnosis in the data-driven world. The key challenge is how to model the complex temporal (linear and nonlinear) correlations among multiple longitudinal variables. Due to scarcity of labels in practice, unsupervised learning methods have already become indispensable. However, state-of-the-art approaches mainly focus on the extraction of linear correlation-induced feature connectivity network, e.g., Pearson correlation, partial correlation, etc. To this end, for chronic disease (e.g., Parkinson disease) diagnosis, an unsupervised representation learning method is first designed to obtain informative correlation-aware signals from multivariate time series data. At the core is a contrastive learning framework with a graph neural network (GNN) encoder to capture the inter-correlation and intra-correlation of multiple longitudinal variables. Then, the previously learned representations are sent to a simple fully connected neural network, which can be trained using fewer labels compared with end-to-end complex supervised learning models. Further, to assist the decision-making process in the high-stake chronic disease detection task, model uncertainty quantification is enabled according to evidential theory. The experimental results on two public Parkinson’s disease data sets show the expressiveness of the learned embeddings, and the final lightweight classifier achieves the best performance.\\r\\nIntroduction\\r\\nWith the development of science and technology, people’s living standards have been greatly improved. However, chronic diseases such as heart disease, cancer, and diabetes are still the leading causes of death and disability in many places around the world. For instance, many old people are suffering from cardiovascular disease, Alzheimer’s disease (AD) [1], and Parkinson’s disease (PD) [2]. What’s more, some chronic diseases such as diabetes and chronic kidney are becoming more and more prevalent among younger age groups [3]. Indeed, people of all ages might suffer from chronic diseases. Therefore, timely diagnosis becomes very important.At present, the widespread deployment of sensors in hospitals has helped accumulate multivariate time series data for patients with chronic diseases or healthy people. Hence, recent academic world has been able to conduct various multivariate time series analysis for chronic diseases diagnosis. In particular, the most efficient approaches adopt the similar pipeline, i.e., extraction of expressive signals from multivariate time series via an unsupervised learning method; then, these signals are sent to a lightweight classifier for supervised learning. Various works have devoted attention to the first step. For instance, it has been shown that feature connectivity network [4,5,6] can highly summarize the structural composition of a complex dynamic system (e.g., the multivariate time series data of a patient with chronic diseases), and then the network serves as a high degree of information for various machine learning tasks. Therefore, state-of-the-art chronic disease detection algorithms mainly focus on the construction of effective feature connectivity matrices. However, they mainly explore to utilize the linear relationships. For example, both Qu et al. [7] and Cheng et al. [8] propose to learn the partial correlation-based connectivity matrix via either shared template regularization [7] or the sparse variational regularization [8]. Even though graph neural networks (GNNs) have been widely adopted to automatically extract the high-level complex feature correlations from multivariate time series data [9,10,11,12], present frameworks are not ideal for multivariate longitudinal data-induced chronic disease diagnosis tasks. On the one hand, most of the frameworks are usually proposed under an ideal fully supervised learning scenario, which is often impossible in a clinical disease diagnosis task due to the difficulties of obtaining the labels; on the other hand, they tend to model the features inter-correlations without careful consideration of the intra-correlations along the temporal series.Worse still, current chronic diseases (e.g., PD) classification algorithms [7, 8, 13,14,15] fail to quantify the model uncertainty [16, 17], which is extremely important in such high-stake healthcare analysis task. For example, if some abnormal noises are added to the multivariate time series data, the model will usually give a wrong prediction with high confidence since the final predictions are obtained only based on the output probability score; thus, it inherently is an overconfident and unreliable model [10]. In contrast, it is much more reasonable to enable the model to filter out the potential uncertain samples that need human experts to judge, i.e., chronic disease diagnosis task needs a confidence-calibrated and uncertainty-aware model.Considering the above limitations, this paper proposes an effective chronic disease diagnosis framework consisting of an unsupervised learning module and an uncertainty-aware supervised learning module. More specifically, in the unsupervised representation learning module, a contrastive learning mechanism is utilized to capture the longitudinal feature intra-correlations, while high-order multiple variables’ inter-correlations are modeled using a GNN, which is chosen as a time span encoder and coupled with the contrastive learning mechanism. Then, the learned representations are sent to a fully connected neural network for supervised learning. Note that since the unsupervised learning module aims at capturing as much as informative signals as possible, any lightweight classifier can be utilized. Further, the evidential theory is leveraged to form the confidence-calibrated and uncertainty-aware classifier. Finally, the effectiveness of the proposed algorithm model is verified based on two public Parkinson's disease data sets.Our contributions are summarized as follows:\\r\\n\\r\\nWe propose an uncertainty-aware chronic disease diagnosis framework, which first extracts informative signals from multivariate time series data via the graph neural newwork and an unsupervised learning paradigm.\\r\\n\\r\\n\\r\\nWe are the first to utilize the time-contrastive learning framework coupled with a graph neural network encoder to model the complex multilevel correlations hidden in the clinical longitudinal data.\\r\\n\\r\\n\\r\\nExperimental results on two public chronic disease classification data sets show the superior performance of the patient representations obtained via the unsupervised learning module. And, the uncertainty quantification indeed helps improve the robustness and reliability of the model.\\r\\nRelated works\\r\\nResearch on chronic disease diagnosis algorithmsChronic disease diagnosis tasks have always attracted widespread attention from both the academic and industrial world. Most of the existing approaches belong to the fully supervised learning setting, which cannot make use of unlabeled data and depends on complex feature processing process [13,14,15]. Recent progresses have been made based on the concept of functional connectivity network [4,5,6], which is constructed via an unsupervised learning model firstly and then is applied to downstream tasks (e.g., classification). For instance, PCC [18] constructed the feature connectivity network based on Pearson correlation. PCC Fisher [19] built the feature connected network based on Pearson correlation and Fisher transform. SR-C [20] captured the sparse feature connected network by Pearson correlation identification with L1 regularization, while SR-PC [21] discovered the feature connectivity network based on partial correlation with lasso regression. Reweighted LASSO [22] constructed feature connected network based on weighted penalty matrix and LASSO regression. SAMCN LASSO [8] utilized gradient meta-learning method. SAMCN Structural LASSO [8] constructed the features connectivity network based on gradient meta-learning, sparse weight penalty matrix and LASSO regression. However, these methods depend on complex feature processing and calculation and can only capture linear relationships, e.g., Pearson correlation, partial correlation, etc. Therefore, more comprehensive algorithms should be designed to capture complex temporal (linear and nonlinear) correlation information.Multivariate time series modelingCurrently, multivariate time series modeling can be categorized into supervised and unsupervised learning. Supervised learning methods [10,11,12] mainly follow an end-to-end learning pipeline, i.e., leveraging different basic neural network components (e.g., CNN, LSTM, etc.) to extract time span features, which are then sent to some well-known machine learning classifiers. On the one hand, fully supervised learning paradigm requires intensive labels, which are often not available for some safety–critical tasks like chronic disease diagnosis. On the other hand, direct usage of labeled data for error backpropagation may introduce some noise leading to unsatisfactory optimization effect [41]. Therefore, unsupervised learning approaches have gradually become the priority choice. Lei [23] proposed an unsupervised learning method based on the distance between the learning representations and the original time series; Malhotra et al. [24] designed the encoder based on recurrent neural network, reconstructed the input time series through the decoder, and combined the encoder and decoder for unsupervised learning. Franceschi et al. [25] proposed the time-contrastive learning framework, which combines triplet loss, causal convolution, and hole convolution network through positive and negative sampling of time series data. Though this method is one of the most simple and efficient unsupervised learning methods for time series data, it fails to model the inter-variable interactions, inspiring us to design more sophisticated unsupervised learning modules.Along this line, graph-structured data mining provides insights that graph can describe the relationships between different nodes, such as social network, transportation network, and human skeleton graph structure. Graph neural network (GNN) can aggregate the feature information of adjacent nodes in graph data. This way of transferring information through graph structure can effectively process graph data and obtain entity representation with stronger expression ability [26, 27]. Various multivariate modeling methods [28, 29] regard each feature as a node in the graph and then utilize graph neural networks to model the complex multilevel correlations. However, most GNN encoder learning methods mainly consider the inter-correlations without considering the intra-correlations. On the contrary, TAGCN [30] uses K convolution kernels of different sizes for feature extraction in each layer. The receptive fields of K convolution kernels are 1\\u2009~\\u2009k, which has strong information aggregation ability, good adaptability, low computational complexity, and can be used in undirected graph or directed graph. Therefore, in this paper, the features of Parkinson's patients are regarded as nodes in the graph structure and TAGCN is used as the encoder of the model.Uncertainty evaluationMost of the existing AI models are known to be prone to overconfidence and are difficult to produce effective patient-level uncertainty scores [31], thus affecting the model reliability, especially in the clinical context (e.g., chronic disease diagnosis). What’s more, even the model has excellent performance for a certain patient population, there are still some out-of-distribution (OOD) patients whose predictions are quite uncertain and often difficult to be detected. Therefore, estimating the uncertainty in an AI system is important to improve the safety and informativeness of the black-box model. For uncertainty quantification, Bayesian neural network (BNN) [16, 32] has achieved good results in providing model uncertainty estimation. At the core of the model is regarding each parameter as distributions rather than fixed parameters, which thereby relies on complex calculation process. Murat et al. [17] proposed a data uncertainty modeling method based on evidential theory, which does not need complex calculation, and can be easily combined with a variety of neural networks. It has the characteristics of simple, efficient, and good adaptability. Therefore, it motivates us to inject it into the supervised learning module of our framework.\\r\\nResearch methods\\r\\nGraph construction methodFirstly, two graph structures are constructed based on the preprocessed PPMI and PS data (see the Sect. 4.1 for details). Note that, in this paper, the features of Parkinson’s patients are regarded as nodes in the graph structure, and the graph structure is constructed by measuring the Euclidean distance between time series features. Given the Parkinson’s disease data set \\\\(X\\\\in {\\\\mathbb{R}}^{N\\\\times T\\\\times D}\\\\) and labels \\\\(Y\\\\in {\\\\mathbb{R}}^{N\\\\times 1}\\\\), where \\\\(N\\\\) denotes the dimension of samples, \\\\(T\\\\) denotes the dimension of time, and \\\\(D\\\\) denotes the dimension of features. Flattening the sample dimension and time dimension, and only keeping the feature dimension, we can obtain the feature vectors: \\\\(\\\\tilde{D}_{N \\\\times T}^{i} = \\\\left\\\\{ {x_{1}^{i} ,x_{2}^{i} , \\\\ldots ,x_{N \\\\times T}^{i} } \\\\right\\\\}\\\\). The Euclidean distance matrix can be calculated between two feature vectors:$$ \\\\tilde{d}_{{\\\\left( {i,j} \\\\right)}} = \\\\sqrt {\\\\mathop \\\\sum \\\\limits_{1}^{{N \\\\times {\\\\text{T}}}} \\\\left( {\\\\tilde{D}_{{N \\\\times {\\\\text{T}}}}^{i} - \\\\tilde{D}_{{N \\\\times {\\\\text{T}}}}^{j} } \\\\right)^{2} } $$\\r\\n                    (1)\\r\\n                Furthermore, the symmetric adjacency matrix is obtained:$$ A_{{\\\\left( {i,j} \\\\right)}} = \\\\frac{1}{{1 + \\\\tilde{d}_{{\\\\left( {i,j} \\\\right)}} }}. $$\\r\\n                    (2)\\r\\n                Note that, in this paper, i and j can be same in the calculation process of Eq.\\xa0(1) and it means the correlation coefficient between features i and j is one after the computation of Eq.\\xa0(2). At the same time, in order to reduce the influence of noise and improve the model performance, robustness and training efficiency, sparsity techniques are adopted. Specifically, the edge connection between two nodes whose weight value is lower than a certain threshold will be deleted, so as to achieve the sparse adjacent matrix. Consider the pruning process of the first row of the matrix:$$ A_{1j}^{*} = \\\\left\\\\{ {\\\\begin{array}{*{20}l} {A_{1,j} ,} \\\\hfill & {A_{1,j} > \\\\eta \\\\cdot \\\\max \\\\left( {A_{1,j} } \\\\right)} \\\\hfill \\\\\\\\ {0,} \\\\hfill & {A_{1,j} \\\\le \\\\eta \\\\cdot \\\\max \\\\left( {A_{1,j} } \\\\right)} \\\\hfill \\\\\\\\ \\\\end{array} } \\\\right. $$\\r\\n                    (3)\\r\\n                where \\\\(\\\\eta \\\\) denotes the sparsity control factor, and \\\\(\\\\mathrm{max}({A}_{1,j})\\\\) denotes the largest weight value when the first feature is connected with all other features. When the weight value of the first feature connected with another feature is lower than threshold \\\\(\\\\eta \\\\cdot \\\\mathrm{max}({A}_{1,j})\\\\), the edge connection between the feature and the first feature will be deleted (the weight value will be set as zero). In the same way, we can get the final matrix \\\\({A}_{ij}^{*}\\\\) after filtering all rows of adjacency matrix.TAGCN feature encodingFor the \\\\(n\\\\)th hidden layer of TAGCN, the input is the graph structure and the corresponding node feature data or the output of the \\\\(n\\\\)th−1 hidden layer. When convolution operation is performed, each node in the graph structure has \\\\(t\\\\in T\\\\) features. For the graph convolution operation in the \\\\(n\\\\)th hidden layer, the \\\\(k\\\\)th feature data on all nodes as the input can be defined as \\\\(x_{k}^{n} \\\\in {\\\\mathbb{R}}^{D} , \\\\quad k = 1,2,3, \\\\ldots ,t\\\\). The specific composition of the feature vector is determined by the node index in the graph structure, and we define \\\\({{G}_{k,f}^{n}\\\\in {\\\\mathbb{R}}}^{D\\\\times D}\\\\) as the \\\\(f\\\\)th convolution kernel. In essence, graph convolution is the product operation between the matrix vectors. For example, \\\\({G}_{k,f}^{n}{x}_{k}^{n}\\\\) denotes an operation of graph convolution.In the process of graph convolution operation, the adjacency matrix is first normalized to ensure that its eigenvalues are in the unit circle and so as to further ensure the stability of graph convolution operation. The normalized adjacency matrix \\\\({\\\\overline{A} }^{*}\\\\) can be described as:$$ \\\\begin{aligned} & \\\\overline{A}^{*} = \\\\overline{D}^{ - 1/2} A^{*} \\\\overline{D}^{ - 1/2} , \\\\\\\\ & \\\\overline{D} = {\\\\text{diag}}\\\\left[ {\\\\overline{d}\\\\left( i \\\\right)} \\\\right], \\\\\\\\ & \\\\overline{d}\\\\left( i \\\\right) = \\\\mathop \\\\sum \\\\limits_{j} A_{ij}^{*} . \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (4)\\r\\n                Further, we can get the convolution kernel by translation invariance principle [30]:$$ G_{k,f}^{n} = \\\\mathop \\\\sum \\\\limits_{c = 0}^{C} g_{k,f,c}^{n} \\\\overline{A}^{*} $$\\r\\n                    (5)\\r\\n                \\r\\nwhere \\\\({g}_{k,f,c}^{n}\\\\) are the graph filter polynomial coefficients. \\\\(C\\\\) is hyperparameter and denotes convolution kernels with different sizes used in each convolutional layer, which is similar to GoogLeNet [39].Then, graph convolution kernels with different sizes slide on the graph-structured data and extract different scales of features. Then, they are combined linearly to form the feature mapping of the output of the \\\\(n\\\\) th hidden layer:$$ y_{f}^{n} = \\\\mathop \\\\sum \\\\limits_{k}^{t} G_{k,f}^{n} x_{k}^{n} + b_{f}^{n} 1_{D} $$\\r\\n                    (6)\\r\\n                \\r\\nwhere \\\\({b}_{f}^{n}\\\\) is trainable parameters, and \\\\({1}_{D}\\\\) is a vector whose element values of dimension D are all one. \\\\({G}_{k,f{x}_{k}^{n}}^{n}\\\\) represents an efficient graph convolution operation on a graph with arbitrary topology.Then, the nonlinear operation is performed on the obtained feature map to get the final output:$$ x_{f}^{n + 1} = \\\\sigma \\\\left( {y_{f}^{n} } \\\\right), $$\\r\\n                    (7)\\r\\n                \\r\\nwhere \\\\(\\\\sigma \\\\) denotes activation function, e.g., RELU.Finally, the output of the graph convolution is transformed into latent space again, and the useful information is further extracted and integrated through the linear function:$$ V^{n + 1} = W_{\\\\eta } x_{f}^{n + 1} + b_{\\\\eta } $$\\r\\n                    (8)\\r\\n                Unsupervised learning based on time-contrastive learningThe most traditional GNN encoders only concentrate on learning node embeddings by aggregating the neighbor nodes information (focus on node level) and then obtain the information of the whole graph, conducting the graph classification task. However, the inter-correlations of the longitudinal nodes between the different graphs and the intracorrelations of the longitudinal nodes among the same graph should also be considered (graph level). In multivariate time series data, each multivariate feature can be seen a node in the graph, few researchers further pay attention to the inter-correlations and intra-correlations in the obtained graph, which limits the performance of the encoders. Hence, in our work, we adopt the time-contrastive learning framework to consider both the intra-correlations and inter-correlations. More specifically, given a time series data \\\\({y}_{i}\\\\) of a sample. Firstly, a random subseries (i.e., a subsequence of a time series composed by consecutive time steps of this time series) \\\\({x}^{\\\\mathrm{rep}}\\\\) of \\\\({y}_{i}\\\\) is sampled, and then subseries  \\\\({x}^{\\\\mathrm{pos}}\\\\) of \\\\({x}^{\\\\mathrm{rep}}\\\\) is obtained as positive pairs. On the one hand, they come from a same time series of a same sample; on the other hand, the \\\\({x}^{\\\\mathrm{rep}}\\\\) can be seen as a word context and the \\\\({x}^{\\\\mathrm{pos}}\\\\) can be seen as the word in the context. Hence, they should have similar representations through the encoder (intra-correlations). Further, the subseries  \\\\({x}^{\\\\mathrm{neg}}\\\\) of another time series data is randomly selected as negative sample. Similarly, on the one hand, \\\\({x}^{\\\\mathrm{rep}}\\\\) and \\\\({x}^{\\\\mathrm{neg}}\\\\) come from a different time series of a different sample; on the other hand, the \\\\({x}^{\\\\mathrm{rep}}\\\\) can be seen as a word context and the \\\\({x}^{\\\\mathrm{neg}}\\\\) can be seen as a random word form the another context. Hence, the representation of the \\\\({x}^{\\\\mathrm{neg}}\\\\) obtained by the encoder should be far away from that of \\\\({x}^{\\\\mathrm{rep}}\\\\) (inter-correlations).Besides, to improve the stability and convergence of the training procedure as well as the experimental results of the learned representations, several negative samples \\\\({{(x}_{k}^{\\\\mathrm{neg}})}_{k\\\\in [1,K]}\\\\) (K denotes the number of random selected negative samples in all time series data) are chosen independently at random in training process. Specially, define the length of time series data \\\\({y}_{i}\\\\) as \\\\({s}_{i}\\\\), where \\\\(i\\\\in [1,N]\\\\) and N denotes the number of time series data. \\\\({s}^{\\\\mathrm{pos}}=\\\\mathrm{size}\\\\left({x}^{\\\\mathrm{pos}}\\\\right),\\\\in [1,{s}_{i}]\\\\), \\\\({s}^{\\\\mathrm{rep}}=\\\\mathrm{size}\\\\left({x}^{\\\\mathrm{rep}}\\\\right),\\\\in [{s}^{\\\\mathrm{pos}},{s}_{i}]\\\\) and \\\\({s}_{k}^{\\\\mathrm{neg}}=\\\\mathrm{size}\\\\left({x}_{k}^{\\\\mathrm{neg}}\\\\right),\\\\in [1,{s}_{k}]\\\\) are picked uniformly at random, where \\\\(size(\\\\cdot )\\\\) means the length of time series data \\\\(x\\\\) and \\\\(k=\\\\mathrm{1,2},\\\\dots ,K\\\\). Then, \\\\({x}^{\\\\mathrm{rep}}\\\\), \\\\({x}^{\\\\mathrm{pos}}\\\\) and \\\\({x}_{k}^{\\\\mathrm{neg}}\\\\) are picked uniformly at random among subseries of \\\\({y}_{i}\\\\) of length \\\\({s}^{\\\\mathrm{rep}}\\\\), \\\\({s}^{\\\\mathrm{pos}}\\\\) and \\\\({s}_{k}^{\\\\mathrm{neg}}\\\\).Finally, the triplet loss function can induce the encoder to have similar representation for similar time series and then learn meaningful encoding representations. The loss function of the unsupervised learning part is:$$ \\\\begin{aligned} & L^{{{\\\\text{tri}}}} = \\\\left[ { - \\\\log (\\\\sigma \\\\left( {f\\\\left( {x^{{{\\\\text{rep}}}} ,\\\\theta } \\\\right)^{{\\\\text{T}}} f\\\\left( {x^{{{\\\\text{pos}}}} ,\\\\theta } \\\\right)} \\\\right)} \\\\right. \\\\\\\\ & \\\\quad - \\\\mathop \\\\sum \\\\limits_{k = 1}^{K} \\\\log \\\\left( {\\\\sigma \\\\left( { - f\\\\left( {x^{{{\\\\text{rep}}}} ,\\\\theta } \\\\right)^{{\\\\text{T}}} } \\\\right.} \\\\right. \\\\\\\\ & \\\\quad \\\\left. {\\\\left. {\\\\left. {f\\\\left( {x_{h}^{{{\\\\text{neg}}}} ,\\\\theta } \\\\right)} \\\\right)} \\\\right)} \\\\right] \\\\cdot \\\\kappa \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (9)\\r\\n                \\r\\nwhere \\\\(\\\\sigma \\\\) denotes sigmoid activation function here and \\\\(f(\\\\cdot ,\\\\theta )\\\\) denotes neural networks (e.g., TAGCN encoder used in this paper) with trainable parameters. \\\\(\\\\kappa \\\\) is used to control the strength of the loss. The loss function will force the encoder to better distinguish \\\\({x}^{\\\\mathrm{rep}}\\\\) and \\\\({x}^{\\\\mathrm{neg}}\\\\) after optimization, and make \\\\({x}^{\\\\mathrm{rep}}\\\\) and \\\\({x}^{\\\\mathrm{pos}}\\\\) have similar representations. Therefore, the triplet loss function leverages TAGCN to capture the high-order inter-correlation and intra-correlation of the multivariate time series.Dirichlet distribution and uncertaintyEvidential uncertainty measurement is derived from the Dempster Shafer Theory of Evidence (DST), which is a generalization of the Bayesian theory to subjective probabilities [40]. In particular, subjective logic (SL) formalizes DST’s notion of belief mass assignments over a frame of discernment based on a Dirichlet distribution with Z parameters \\\\(\\\\alpha = \\\\left[ {\\\\alpha_{1} ,\\\\alpha_{2} , \\\\ldots ,\\\\alpha_{Z} } \\\\right]\\\\)  (subjective opinion\\\\(\\\\alpha \\\\)). Normally, the output of the standard neural network is the probability of each possible category of the sample, and the Dirichlet distribution based on the evidence theory represents the density function of the probability, so it can simulate the second-order probability and uncertainty [34, 35]. The Dirichlet distribution is a probability density function (pdf) for possible values of the probability mass function (pmf) \\\\(p\\\\) and can be expressed by Z parameters\\\\(\\\\alpha \\\\):$$ {\\\\text{Dir}}\\\\left( {p|\\\\alpha } \\\\right) = \\\\left\\\\{ {\\\\begin{array}{*{20}l} {\\\\frac{1}{B\\\\left( \\\\alpha \\\\right)}\\\\mathop \\\\prod \\\\limits_{i = 1}^{Z} p_{i}^{{\\\\alpha_{i - 1} }} ,} \\\\hfill & { p \\\\in S_{Z} } \\\\hfill \\\\\\\\ 0 \\\\hfill & {{\\\\text{otherwise}}} \\\\hfill \\\\\\\\ \\\\end{array} } \\\\right. $$\\r\\n                    (10)\\r\\n                \\r\\nwhere \\\\(p\\\\) is the probability mass function and \\\\(\\\\alpha =[{\\\\alpha }_{1},\\\\dots ,{\\\\alpha }_{Z}]\\\\) are the parameters of Dirichlet distribution. \\\\(Z\\\\) denotes the label category. \\\\(B\\\\left(\\\\alpha \\\\right)\\\\) is a polynomial beta function in \\\\(Z\\\\) dimension [36]. \\\\({S}_{Z}\\\\) is the Z-dimensional unit simplex:$$ S_{Z} = \\\\left\\\\{ {p\\\\left| {\\\\mathop \\\\sum \\\\limits_{i = 1}^{Z} p_{i} = 1\\\\,} \\\\right.{\\\\text{and}}\\\\, 0 \\\\le p_{1} , \\\\ldots ,p_{Z} \\\\le 1} \\\\right\\\\} $$\\r\\n                    (11)\\r\\n                Based on SL, the relationship between belief mass for each singleton \\\\({b}_{z}\\\\) and uncertainty \\\\(u\\\\) is computed as:$$ u + \\\\mathop \\\\sum \\\\limits_{z = 1}^{Z} b_{z} = 1. $$\\r\\n                    (12)\\r\\n                \\r\\nwhere \\\\(u\\\\ge 0\\\\) and \\\\({b}_{z}\\\\ge 0\\\\) for \\\\(z=\\\\mathrm{1,2},\\\\dots ,Z\\\\). A belief mass \\\\({b}_{z}\\\\) for a class label \\\\(z\\\\) is computed using evidence \\\\({e}_{z}\\\\) (\\\\(e=[{e}_{1},{e}_{2},\\\\dots ,{e}_{Z}]\\\\)). More specifically,$$ b_{z} = \\\\frac{{e_{z} }}{S}, u = \\\\frac{Z}{S}, $$\\r\\n                    (13)\\r\\n                $$ \\\\alpha_{z} = e_{z} + 1, $$\\r\\n                    (14)\\r\\n                $$ e_{z} = \\\\zeta \\\\left( {\\\\hat{y}^{s} } \\\\right), $$\\r\\n                    (15)\\r\\n                $$ S = \\\\mathop \\\\sum \\\\limits_{z = 1}^{Z} \\\\alpha_{z} . $$\\r\\n                    (16)\\r\\n                \\r\\nwhere \\\\(Z\\\\) denotes the number of labels, \\\\({\\\\alpha }_{z}\\\\) is Dirichlet parameters,  \\\\({\\\\widehat{y}}^{s}\\\\) is the output vector before being sent to softmax layer, and  \\\\(\\\\zeta \\\\left(\\\\cdot \\\\right)\\\\) denotes an activation layer, e.g., ReLU. \\\\({e}_{z}\\\\) is the amount of evidence and \\\\(S\\\\) is the Dirichlet strength.Further, the expectation of Dirichlet distribution based on neural network evidence theory can be computed as:$$ \\\\hat{p}^{z} = \\\\alpha_{z} /S. $$\\r\\n                    (17)\\r\\n                At present, the well-known Type \\\\(\\\\mathrm{\\\\rm I}\\\\mathrm{\\\\rm I}\\\\) maximum likelihood is always used to optimize the Dirichlet distribution \\\\(D({p}_{i}|{\\\\alpha }_{i})\\\\):$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = - {\\\\text{log}}\\\\left( {\\\\int \\\\mathop \\\\prod \\\\limits_{j = 1}^{Z} p_{ij}^{{y_{ij} }} \\\\frac{1}{{B\\\\left( {\\\\alpha_{i} } \\\\right)}}\\\\mathop \\\\prod \\\\limits_{j = 1}^{Z} p_{ij}^{{\\\\alpha_{ij} - 1}} {\\\\text{d}}P_{i} } \\\\right) \\\\\\\\ & \\\\quad = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} y_{ij} \\\\left( {\\\\log \\\\left( {S_{i} } \\\\right) - {\\\\text{log}}\\\\left( {\\\\alpha_{ij} } \\\\right)} \\\\right). \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (18)\\r\\n                \\r\\nwhere \\\\({y}_{i}\\\\) is a one-hot vector encoding of the ground-truth class with \\\\({y}_{ij}=1\\\\) and \\\\({y}_{iz}=0\\\\) for all \\\\(z\\\\ne j\\\\) and \\\\({\\\\alpha }_{i}\\\\) is the parameters of the Dirichlet density on the predictors.We treat \\\\(D({p}_{i}|{\\\\alpha }_{i})\\\\) as a prior on the likelihood Multi \\\\(({y}_{i}|{p}_{i})\\\\) and obtain the negated logarithm of the marginal likelihood by integrating out the class probabilities. Finally, we minimize the parameters of the Dirichlet density \\\\({\\\\alpha }_{i}\\\\).More specially, following Eq.\\xa0(18), for better quantifying the uncertainties \\\\(u\\\\) of different samples, a cross-entropy loss and Bayes risk-based loss function is defined:$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = \\\\int \\\\left[ {\\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} - y_{ij} \\\\left( {{\\\\text{log}}\\\\left( {p_{ij} } \\\\right)} \\\\right)} \\\\right] \\\\\\\\ & \\\\frac{1}{{B\\\\left( {\\\\alpha_{i} } \\\\right)}}\\\\mathop \\\\prod \\\\limits_{j = 1}^{Z} p_{ij}^{{\\\\alpha_{ij} - 1}} {\\\\text{d}}P_{i} \\\\\\\\ & = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} y_{ij} \\\\left( {\\\\psi \\\\left( {S_{i} } \\\\right) - \\\\psi \\\\left( {\\\\alpha_{ij} } \\\\right)} \\\\right), \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (19)\\r\\n                \\r\\nwhere \\\\(\\\\psi \\\\left(\\\\cdot \\\\right)\\\\) is the digamma function. Following Eq.\\xa0(18), we also define a squares (\\\\(\\\\left\\\\| {y_{i} - p_{i} } \\\\right\\\\|_{2}^{2}\\\\))-based loss function:$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = \\\\int \\\\left\\\\| {y_{i} - p_{i} } \\\\right\\\\|_{2}^{2} \\\\frac{1}{{B\\\\left( {\\\\alpha_{i} } \\\\right)}} \\\\\\\\ & \\\\mathop \\\\prod \\\\limits_{i = 1}^{Z} p_{ij}^{{\\\\alpha_{ij} - 1}} {\\\\text{d}}P_{i} \\\\\\\\ & = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} {\\\\mathbb{E}}\\\\left[ {y_{ij}^{2} - 2y_{ij} p_{ij} + p_{ij}^{2} } \\\\right] \\\\\\\\ & = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left[ {y_{ij}^{2} - 2y_{ij} {\\\\mathbb{E}}[p_{ij} ] + {\\\\mathbb{E}}[p_{ij}^{2} ]} \\\\right]. \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (20)\\r\\n                The above two loss functions both can be used to optimize the Dirichlet distribution, and we will choose the better one through the experiments and theoretical analysis in the following sections.In Dirichlet distribution, if a sample cannot be correctly classified and its total evidence will be close to zero, e.g., \\\\(S\\\\approx K\\\\), corresponding to the uniform distribution and indicating the total uncertainty, e.g., \\\\(u\\\\approx 1\\\\). This is not a good condition and means the classifier fails to differentiate these samples. If the Dirichlet distribution converges to uniform distribution early, the model would not be good at quantifying the uncertainties of most samples. Hence, a Kullback–Leibler (KL) divergence term is incorporated into the defined loss function:$$ \\\\begin{aligned} & {\\\\mathcal{L}}\\\\left( {\\\\Theta } \\\\right) = \\\\mathop \\\\sum \\\\limits_{i = 1}^{N} {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) + \\\\varrho_{t} \\\\\\\\ & \\\\quad \\\\mathop \\\\sum \\\\limits_{i = 1}^{N} KL\\\\left[ {\\\\left. {\\\\hat{D}(p_{i} |\\\\tilde{\\\\alpha }_{i} )} \\\\right\\\\|} \\\\right. \\\\\\\\ & \\\\quad \\\\hat{D}\\\\left. {\\\\left( {p_{i} \\\\left| { < 1, \\\\ldots ,1 > } \\\\right.} \\\\right)} \\\\right], \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (21)\\r\\n                where \\\\({\\\\varrho }_{t}=\\\\mathrm{min}\\\\left(1.0,\\\\frac{t}{10}\\\\right)\\\\in [\\\\mathrm{0,1}]\\\\) is the annealing coefficient, \\\\(t\\\\) is the index of the current training epoch, and \\\\({\\\\stackrel{\\\\sim }{\\\\alpha }}_{i}={y}_{i}+(1-{y}_{i})\\\\odot {\\\\alpha }_{i}\\\\) is the Dirichlet parameters after removal of the nonmisleading evidence from predicted parameters \\\\({\\\\alpha }_{i}\\\\) for sample \\\\(i\\\\). The KL divergence term in the loss function can be calculated as:$$ \\\\begin{aligned} & KL[\\\\hat{D}(p_{i} |\\\\tilde{\\\\alpha }_{i} )||\\\\hat{D}(p_{i} |1)] \\\\\\\\ & = \\\\log \\\\left( {\\\\frac{{\\\\Gamma \\\\left( {\\\\mathop \\\\sum \\\\nolimits_{z = 1}^{Z} \\\\tilde{\\\\alpha }_{iz} } \\\\right)}}{{\\\\Gamma \\\\left( Z \\\\right)\\\\mathop \\\\prod \\\\nolimits_{z = 1}^{Z} \\\\Gamma \\\\left( {\\\\tilde{\\\\alpha }_{iz} } \\\\right)}}} \\\\right) + \\\\mathop \\\\sum \\\\limits_{z = 1}^{Z} \\\\left( {\\\\tilde{\\\\alpha }_{iz} - 1} \\\\right) \\\\\\\\ & \\\\quad \\\\left[ {\\\\psi \\\\left( {\\\\tilde{\\\\alpha }_{iz} } \\\\right) - \\\\psi \\\\left( {\\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\tilde{\\\\alpha }_{ij} } \\\\right)} \\\\right], \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (22)\\r\\n                where \\\\(\\\\Gamma \\\\left(\\\\cdot \\\\right)\\\\) is the gamma function. The annealing coefficient \\\\({\\\\varrho }_{t}\\\\) can control the effect of the KL divergence, and so the Dirichlet distribution will be induced to explore the Dirichlet parameter space as far as possible. The model can be induced to avoid premature convergence to the uniform distribution for the misclassified samples and try to classify them correctly in the training epochs as far as possible.In the process of experiment, we found that the cross-entropy loss and Bayes risk-based loss function tend to generate excessively high belief masses for classes and exhibit relatively less stable performance than the squares loss-based loss function. What’s more, the relationship between the expectation and variance can be described as:$$ {\\\\mathbb{E}}\\\\left[ {p_{ij}^{2} } \\\\right] = {\\\\mathbb{E}}\\\\left[ {p_{ij} } \\\\right]^{2} + {\\\\text{Var}}\\\\left( {p_{ij} } \\\\right). $$\\r\\n                    (23)\\r\\n                Hence, based on the relationship between the expectation and variance, the squares loss-based loss function can be transformed into more interpretable form:$$ \\\\begin{aligned} & {\\\\mathcal{L}}_{i} \\\\left( {\\\\Theta } \\\\right) = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left( {y_{ij} - {\\\\mathbb{E}}\\\\left[ {p_{ij} } \\\\right]} \\\\right)^{2} + {\\\\text{Var}}\\\\left( {p_{ij} } \\\\right) \\\\\\\\ & \\\\quad = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left( {y_{ij} - \\\\alpha_{ij} /S_{i} } \\\\right)^{2} \\\\\\\\ & \\\\quad \\\\quad + \\\\frac{{\\\\alpha_{ij} \\\\left( {S_{i} - \\\\alpha_{ij} } \\\\right)}}{{S_{i}^{2} \\\\left( {S_{i} + 1} \\\\right)}} \\\\\\\\ & \\\\quad = \\\\mathop \\\\sum \\\\limits_{j = 1}^{Z} \\\\left( {y_{ij} - { }\\\\hat{p}_{ij} } \\\\right)^{2} + \\\\frac{{\\\\hat{p}_{ij} \\\\left( {1 - \\\\hat{p}_{ij} } \\\\right)}}{{S_{i} + 1}}. \\\\\\\\ \\\\end{aligned} $$\\r\\n                    (24)\\r\\n                By minimizing the expectation error and variance of the Dirichlet distribution, it can assign belief mass for different samples reasonably and bette\""
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9ExFWNWffpz"
      },
      "source": [
        "def dataset_divider(data, column, no_of_chunks):\n",
        "  final_list = []\n",
        "  for par in data[column]: \n",
        "    start = 0\n",
        "    d = []\n",
        "    # chunk = 4950\n",
        "    n = [4950 for x in range(no_of_chunks)]\n",
        "    for i in n:\n",
        "        d.append(par[start:start+i])\n",
        "        start += i\n",
        "    d.append(par[start:])\n",
        "    p = [x for x in d if len(x) > 1] \n",
        "    final_list.append(p)\n",
        "\n",
        "  return final_list"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnmS3G2H27lN"
      },
      "source": [
        "final_list = dataset_divider(springer, 'Content', 7)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT2vqWNJgu2u",
        "outputId": "37666793-085b-427e-dfe8-a04dab6f7452"
      },
      "source": [
        "len(final_list)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe3p7fOaqw4G"
      },
      "source": [
        ""
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzzt8-nNeCOt"
      },
      "source": [
        "# yfrom googletrans import Translator\n",
        "translator  = GoogleTranslator()\n"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRUQYP9p6-15"
      },
      "source": [
        "def Translator(list_to_translate, language):\n",
        "  counter = 0\n",
        "  final_translated_to_arabic = []\n",
        "  row_translated_to_arabic = []\n",
        "  for i in list_to_translate:\n",
        "\n",
        "      # [128:]\n",
        "      print(counter)\n",
        "      for j in range(len(i)):\n",
        "\n",
        "        row = GoogleTranslator(source='auto', target=language).translate(text=str(i[j]))\n",
        "        row_translated_to_arabic.append(row)\n",
        "\n",
        "      final_translated_to_arabic.append('.'.join(map(str, row_translated_to_arabic)))\n",
        "              \n",
        "      row_translated_to_arabic = []\n",
        "      \n",
        "      counter = counter + 1\n",
        "\n",
        "  return final_translated_to_arabic\n",
        "  "
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPVxVVFi2ZjY",
        "outputId": "bf24c091-7c42-43fe-c693-c39e335e5b8e"
      },
      "source": [
        "final_translated_to_arabic = Translator(final_list, 'ar')"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-ksayik3L7t",
        "outputId": "529c4030-fa7c-40b0-87b9-64f2d2c8351b"
      },
      "source": [
        "len(final_translated_to_arabic)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoemNT9Misrs"
      },
      "source": [
        "# final_translated_to_arabic"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iPw9T_v19jj"
      },
      "source": [
        "springer['arabic_translated'] = final_translated_to_arabic"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "AcWtvVCP2JnX",
        "outputId": "49579597-6ff7-4533-c26a-17924dfc22b1"
      },
      "source": [
        "springer.head()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Content</th>\n",
              "      <th>URL</th>\n",
              "      <th>arabic_translated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Exploring unsupervised multivariate time serie...</td>\n",
              "      <td>The application of various sensors in hospital...</td>\n",
              "      <td>https://link.springer.com//article/10.1007/s41...</td>\n",
              "      <td>أتاح تطبيق أجهزة الاستشعار المختلفة في المستشف...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Big social data provenance framework for Zero-...</td>\n",
              "      <td>Social media has been playing a vital importan...</td>\n",
              "      <td>https://link.springer.com//article/10.1007/s41...</td>\n",
              "      <td>تلعب وسائل التواصل الاجتماعي أهمية حيوية في مش...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Graph sparsification with graph convolutional ...</td>\n",
              "      <td>Graphs are ubiquitous across the globe and wit...</td>\n",
              "      <td>https://link.springer.com//article/10.1007/s41...</td>\n",
              "      <td>الرسوم البيانية منتشرة في كل مكان حول العالم و...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Anonymization of German financial documents us...</td>\n",
              "      <td>The automatization and digitalization of busin...</td>\n",
              "      <td>https://link.springer.com//article/10.1007/s41...</td>\n",
              "      <td>أدت أتمتة العمليات التجارية ورقمنتها إلى زيادة...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Correction to: Conventional displays of struct...</td>\n",
              "      <td>Correction to: International Journal of Data S...</td>\n",
              "      <td>https://link.springer.com//article/10.1007/s41...</td>\n",
              "      <td>تصحيح إلى: المجلة الدولية لعلوم وتحليلات البيا...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        ArticleTitle  ...                                  arabic_translated\n",
              "0  Exploring unsupervised multivariate time serie...  ...  أتاح تطبيق أجهزة الاستشعار المختلفة في المستشف...\n",
              "1  Big social data provenance framework for Zero-...  ...  تلعب وسائل التواصل الاجتماعي أهمية حيوية في مش...\n",
              "2  Graph sparsification with graph convolutional ...  ...  الرسوم البيانية منتشرة في كل مكان حول العالم و...\n",
              "3  Anonymization of German financial documents us...  ...  أدت أتمتة العمليات التجارية ورقمنتها إلى زيادة...\n",
              "4  Correction to: Conventional displays of struct...  ...  تصحيح إلى: المجلة الدولية لعلوم وتحليلات البيا...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv0yu77g7DrM"
      },
      "source": [
        "springer.to_csv('/content/Springer_Content_Translated_into_arabic.csv') "
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB7BbXLw_Hq_"
      },
      "source": [
        "## codata articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvOpc_HYj7d6"
      },
      "source": [
        "ds_codata= pd.read_excel(r'/content/datascience.codata.org_articles.xlsx')"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gLn5V_7t_UDV",
        "outputId": "d2e63e58-5b93-4514-a915-49703fa441e7"
      },
      "source": [
        "ds_codata.head()"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Content</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A Framework for Data-Driven Solutions with COV...</td>\n",
              "      <td>1 Introduction\\nDrawn to address global challe...</td>\n",
              "      <td>https://datascience.codata.org/articles/10.533...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Interconnecting Systems Using Machine-Actionab...</td>\n",
              "      <td>1 Introduction\\nThe Data Management Plan (DMP)...</td>\n",
              "      <td>https://datascience.codata.org/articles/10.533...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Keeping Track of Samples in Multidisciplinary ...</td>\n",
              "      <td>Introduction\\nThe EU directive on Public Secto...</td>\n",
              "      <td>https://datascience.codata.org/articles/10.533...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Towards Globally Unique Identification of Phys...</td>\n",
              "      <td>Introduction: Persistent Identifiers for Sampl...</td>\n",
              "      <td>https://datascience.codata.org/articles/10.533...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Application Profile for Machine-Actionable Dat...</td>\n",
              "      <td>1 Introduction\\nData Management Plans (DMPs) a...</td>\n",
              "      <td>https://datascience.codata.org/articles/10.533...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        ArticleTitle  ...                                                URL\n",
              "0  A Framework for Data-Driven Solutions with COV...  ...  https://datascience.codata.org/articles/10.533...\n",
              "1  Interconnecting Systems Using Machine-Actionab...  ...  https://datascience.codata.org/articles/10.533...\n",
              "2  Keeping Track of Samples in Multidisciplinary ...  ...  https://datascience.codata.org/articles/10.533...\n",
              "3  Towards Globally Unique Identification of Phys...  ...  https://datascience.codata.org/articles/10.533...\n",
              "4  Application Profile for Machine-Actionable Dat...  ...  https://datascience.codata.org/articles/10.533...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "zGpNuKhC_ft5",
        "outputId": "038a8f16-d5a7-4220-c823-c499f6cdeee7"
      },
      "source": [
        "ds_codata.Content[0]"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1 Introduction\\nDrawn to address global challenges–poverty, health, inequality, climate change, innovation, environmental degradation, peace and justice, the 17 United Nations Sustainable Development Goals (SDG) have, since their inception in 2015, remained at the centre of development strategies for central and local governments, businesses and institutions across the world (United-Nations 2015). The impact of COVID-19 is felt across the sectors and areas that describe the SDG (Rothan & Byrareddy 2020). While tackling global challenges of this nature naturally entails efforts from across disciplines, sectors, borders and legislations; silo working continues to dominate research initiatives in many fields, constantly creating knowledge gaps. The COVID–19 pandemic–a typical example of a global challenge, has reminded us of such gaps in our knowledge, requiring even stronger collaborative and interdisciplinary data-driven initiatives to fill. Despite its devastating impact on our ways of life, it has been argued that COVID–19 has presented us with an excellent opportunity for accelerating attainment of the SDG through data–driven technologies (Pan & Zhang 2020), particularly because COVID–19 is happening amidst data deluge and growing capabilities in handling Big Data (Wang et al. 2020). Different countries have been dealing with the pandemic using different strategies, and the need for sharing data across geographical borders has never been greater.\\nOne of the main issues researchers face and will continue to face in the future is spatio–temporal variations and their impact on the conclusions we reach on data-driven solutions. Despite the devastating effects, the spatio-temporal variations of COVID–19 present an excellent opportunity for the research community to bridge knowledge gaps in addressing societal challenges through interdisciplinary data modelling. As data-driven solutions are dependent on the stability of the underlying data modelling assumptions, knowledge gaps inevitably arise when the assumptions are violated. We present a generic framework for filling such gaps, based on two data-driven algorithms that combine data, machine learning and interdisciplinarity to bridge societal knowledge gaps by highlighting timing and conditions for interventions. Using structured COVID–19 data obtained from the European Centre for Disease Prevention and Control (ECDC); data on its impact, obtained from the UK Office of National Statistics, and unstructured imagery COVID–19 X–obtained from GitHub and Kaggle, we present two algorithms–one for animation and visualisation and the other for enhanced classification based on an adaptive Convolutional Neural Network (CNN) model.\\nNovelty of the paper is embedded in the two algorithms–both adapted from the Sampling-Measuring-Assessing (SMA) algorithm for addressing data randomness, originally developed by Mwitondi et al. (2018a, b, 2020) for modelling structured data based on statistical model fitting and evaluation. The adaptation in Section 2, resonates with cross-disciplinary research discussions in tackling global challenges. The paper is organised as follows. Section 1 provides an introduction, motivation, research question and objectives. Section 2 presents the methods–framework, data sources and modelling techniques. Section 3 presents the analyses and Section 4 concludes the work and highlights new directional paths for research.\\n\\n1.1 Related Work\\nAs noted above, this work was motivated by Big Data Modelling of SDG (BDMSDG) (Mwitondi et al. 2020, 2018a, b) and, particularly, by the way COVID–19 has impacted our ways of life (Zambrano-Monserrate et al. 2020, Bartik et al. 2020). The complex interactions of the SDG, the magnitude and dynamics of their data attributes as well as the deep and wide socio–economic and cultural variations across the globe present both a challenge and an opportunity to the SDG project. These attributes impinge on data–driven solutions as they contribute to not only data randomness but also to variations in underlying data relationships and definitions over time, commonly known as concept drift (Zenisek et al. 2019). It is, therefore, reasonable to align the spatio–temporal variations of the impact of COVID–19 with the potential to bridge societal knowledge gaps and gain a better understanding of the challenges we face through data–driven solutions. Data variations have been extensively studied and this work draws from existing modelling techniques such as the standard variants of cross-validation (Bo et al. 2006, Xu & Goodacre 2018) and permutation feature importance (Galkin et al. 2018). The work derives from statistical models like bagging and bootstrapping, which either rely on aggregation of classifiers or sample representativeness (Mwitondi et al. 2019). The SMA algorithm’s superiority lies in its built–in mechanics for efficiently handling data randomness (Mwitondi et al. 2019, 2020).\\nSince the onset of the COVID–19 pandemic, data visualisation tools have become increasingly common across the world. Many pre-existing dashboards like Our World in Data (Roser et al. 2018), the World Bank Group (WBGroup 2018), Johns Hopkins University Coronavirus Resource Center (CRC 2021) and the Millennium Institute (MI 2021) have developed tools for mapping the pandemic across the globe, some in near real-time. Figure 1, captured from the Johns Hopkins University COVID–19 dashboard on 7th July 2021 at 17:21 hrs, displays cases, deaths and vaccine doses administered by country as well as other data attributes via the menu items. While this kind of pattern visualisation is informative of the direction the pandemic is taking, just like the aforementioned tools, it is essentially an enhanced descriptive statistics generator. Its reliance on country–specific data accuracy leaves many unanswered questions. For instance, does it truly reflect data collection and reporting in all countries displayed? Does it provide a better understanding of the challenges we face? For answers to these and many other general questions, Zhang et al. (2014) recommend a bottom–up approach. Accuracy, completeness, consistency and other aspects of data quality have been widely studied and they remain a focal point in many fields (Cai & Zhu 2015, Zhang et al. 2017).\\n\\n\\nFigure 1\\xa0 A Johns Hopkins COVID–19 visualisation dashboard.\\n(Source: https://coronavirus.jhu.edu/map.html)\\n\\nThe advent of dashboards has prompted further thinking on how they can be used for enhancing decision making processes by increasing transparency, accountability, stakeholders’ engagement, governance and institutional arrangements Matheus et al. (2020). Our work focuses on how to complement accessible descriptive data, through dashboards or otherwise, by modelling techniques to support decision making processes. It is guided by spatio-temporal variations in gaining insights into how different societies have been impacted by the pandemic.\\nThis paper proposes an interdisciplinary approach for addressing the foregoing general questions based on structured and unstructured data modelling methods. The former is an interactive data animation and visualisation tool with a built-in ability to fire warning alerts, while the latter provides a predictive power using imagery data.\\n\\n\\n1.2 Research Question and Objectives\\nSpatio–temporal variations and data randomness are some of the main factors known to impinge on the conclusions we draw from data–driven solutions (Mwitondi & Said 2013). This work combines the power of Big Data, machine learning and interdisciplinarity to address those issues. Using real–life examples based on COVID–19 pandemic data, we examine how country–specific approaches to global challenges fit in the global prism of data–driven solutions. We seek to answer the question: How do spatio–temporal variations resonate with interdisciplinary tackling of global challenges? To answer the foregoing question, we set the following objectives.\\n\\nTo illustrate the efficacy of national level multi–dimensional visualisation of COVID–19 impact on societies.\\nTo demonstrate the efficacy of combining data, techniques and skills in an interdisciplinary context.\\nTo use multi-dimensional data visualisation in a two-dimensional space for timely decisions on the impact of the pandemic and other societal challenges.\\nTo provide practical implementations of a robust machine learning algorithm, with built-in capabilities for accommodating interdisciplinary skills.\\nTo highlight a roadmap for aligning national strategies to the global prism of data–driven solutions.\\n\\n\\n\\n1.3 Contribution to Knowledge\\nThe paper’s novelty derives from applied mechanics of Algorithms 1 and 2, within the context of the data-driven framework in Figure 2 and the implementation flow in Figure 3. Its main idea hinges on data randomness that characterises all learning models as a major cause of spatio-temporal variations, as described in Mwitondi & Said (2013). Using COVID-19 illustrations, the application highlights paths for combining domain knowledge, data, tools and skills in addressing global challenges across the SDG spectrum. Based on evidence from literature, we highlight the following aspects of contribution to knowledge.\\n\\n\\nFigure 2\\xa0 A diagrammatical illustration of the interaction of challenges, data and skills.\\n\\n\\n\\nFigure 3\\xa0 Graphical illustration of the CNN classification and assessment process.\\n\\n\\n\\nAddressing Data Randomness: Leading to enhanced modelling techniques for decision support systems.\\n\\nAnimation: Multi-dimensional visualisation of data attributes in 2-D space, allowing for manual or automated intervention via Algorithm 1, is an enhanced data-driven decision support system that is not provided by any of the tools discussed under related work in Section 1.1. Algorithm 1 is adaptable to a wide range of applications and COVID-19 is used as a special case to illustrate its mechanics.\\n\\nModel Optimisation: Rather than just averaging an ensemble of models to reduce variance (as in the bagging case) or evaluating surrogate models, Algorithm 2 combines cross-validation, bagging and step-wise assessment based on updatable parameters (model weights, in this case), exhibiting a robust performance of the algorithm. Application of CNN on COVID-19 data to illustrate robust data-driven solutions for global challenges, Algorithm 2 is adaptive to a wide range of techniques–unsupervised and supervised.\\n\\n\\n\\nApplications: A novel approach towards application in the context of SDG initiatives.\\nIt complements dashboard descriptive data, in an interdisciplinary context as shown in Figure 2.\\nSpatio-temporal variations provide insights into how different societies have been impacted by the pandemic. Researchers focusing on other SDG-related challenges can easily adapt the mechanics of the two algorithms and the data-driven generic framework to their specific needs in search of robust performance.\\n\\n\\n\\n\\n\\nAlgorithm 1\\xa0 Adaptation of the SMA Algorithm (Mwitondi et al. 2020) for Animation & Visualisation.\\n\\nThe study methodology is based on structured and unstructured data. Its basic ideas are in the Sample-Measure-Assess (SMA) algorithm originally developed for structured data (Mwitondi et al. 2020, Said & Mwitondi 2021).1.1 Related Work\\nAs noted above, this work was motivated by Big Data Modelling of SDG (BDMSDG) (Mwitondi et al. 2020, 2018a, b) and, particularly, by the way COVID–19 has impacted our ways of life (Zambrano-Monserrate et al. 2020, Bartik et al. 2020). The complex interactions of the SDG, the magnitude and dynamics of their data attributes as well as the deep and wide socio–economic and cultural variations across the globe present both a challenge and an opportunity to the SDG project. These attributes impinge on data–driven solutions as they contribute to not only data randomness but also to variations in underlying data relationships and definitions over time, commonly known as concept drift (Zenisek et al. 2019). It is, therefore, reasonable to align the spatio–temporal variations of the impact of COVID–19 with the potential to bridge societal knowledge gaps and gain a better understanding of the challenges we face through data–driven solutions. Data variations have been extensively studied and this work draws from existing modelling techniques such as the standard variants of cross-validation (Bo et al. 2006, Xu & Goodacre 2018) and permutation feature importance (Galkin et al. 2018). The work derives from statistical models like bagging and bootstrapping, which either rely on aggregation of classifiers or sample representativeness (Mwitondi et al. 2019). The SMA algorithm’s superiority lies in its built–in mechanics for efficiently handling data randomness (Mwitondi et al. 2019, 2020).\\nSince the onset of the COVID–19 pandemic, data visualisation tools have become increasingly common across the world. Many pre-existing dashboards like Our World in Data (Roser et al. 2018), the World Bank Group (WBGroup 2018), Johns Hopkins University Coronavirus Resource Center (CRC 2021) and the Millennium Institute (MI 2021) have developed tools for mapping the pandemic across the globe, some in near real-time. Figure 1, captured from the Johns Hopkins University COVID–19 dashboard on 7th July 2021 at 17:21 hrs, displays cases, deaths and vaccine doses administered by country as well as other data attributes via the menu items. While this kind of pattern visualisation is informative of the direction the pandemic is taking, just like the aforementioned tools, it is essentially an enhanced descriptive statistics generator. Its reliance on country–specific data accuracy leaves many unanswered questions. For instance, does it truly reflect data collection and reporting in all countries displayed? Does it provide a better understanding of the challenges we face? For answers to these and many other general questions, Zhang et al. (2014) recommend a bottom–up approach. Accuracy, completeness, consistency and other aspects of data quality have been widely studied and they remain a focal point in many fields (Cai & Zhu 2015, Zhang et al. 2017).\\n\\n\\nFigure 1\\xa0 A Johns Hopkins COVID–19 visualisation dashboard.\\n(Source: https://coronavirus.jhu.edu/map.html)\\n\\nThe advent of dashboards has prompted further thinking on how they can be used for enhancing decision making processes by increasing transparency, accountability, stakeholders’ engagement, governance and institutional arrangements Matheus et al. (2020). Our work focuses on how to complement accessible descriptive data, through dashboards or otherwise, by modelling techniques to support decision making processes. It is guided by spatio-temporal variations in gaining insights into how different societies have been impacted by the pandemic.\\nThis paper proposes an interdisciplinary approach for addressing the foregoing general questions based on structured and unstructured data modelling methods. The former is an interactive data animation and visualisation tool with a built-in ability to fire warning alerts, while the latter provides a predictive power using imagery data.Figure 1\\xa0 A Johns Hopkins COVID–19 visualisation dashboard.\\n(Source: https://coronavirus.jhu.edu/map.html)1.2 Research Question and Objectives\\nSpatio–temporal variations and data randomness are some of the main factors known to impinge on the conclusions we draw from data–driven solutions (Mwitondi & Said 2013). This work combines the power of Big Data, machine learning and interdisciplinarity to address those issues. Using real–life examples based on COVID–19 pandemic data, we examine how country–specific approaches to global challenges fit in the global prism of data–driven solutions. We seek to answer the question: How do spatio–temporal variations resonate with interdisciplinary tackling of global challenges? To answer the foregoing question, we set the following objectives.\\n\\nTo illustrate the efficacy of national level multi–dimensional visualisation of COVID–19 impact on societies.\\nTo demonstrate the efficacy of combining data, techniques and skills in an interdisciplinary context.\\nTo use multi-dimensional data visualisation in a two-dimensional space for timely decisions on the impact of the pandemic and other societal challenges.\\nTo provide practical implementations of a robust machine learning algorithm, with built-in capabilities for accommodating interdisciplinary skills.\\nTo highlight a roadmap for aligning national strategies to the global prism of data–driven solutions.1.3 Contribution to Knowledge\\nThe paper’s novelty derives from applied mechanics of Algorithms 1 and 2, within the context of the data-driven framework in Figure 2 and the implementation flow in Figure 3. Its main idea hinges on data randomness that characterises all learning models as a major cause of spatio-temporal variations, as described in Mwitondi & Said (2013). Using COVID-19 illustrations, the application highlights paths for combining domain knowledge, data, tools and skills in addressing global challenges across the SDG spectrum. Based on evidence from literature, we highlight the following aspects of contribution to knowledge.\\n\\n\\nFigure 2\\xa0 A diagrammatical illustration of the interaction of challenges, data and skills.\\n\\n\\n\\nFigure 3\\xa0 Graphical illustration of the CNN classification and assessment process.\\n\\n\\n\\nAddressing Data Randomness: Leading to enhanced modelling techniques for decision support systems.\\n\\nAnimation: Multi-dimensional visualisation of data attributes in 2-D space, allowing for manual or automated intervention via Algorithm 1, is an enhanced data-driven decision support system that is not provided by any of the tools discussed under related work in Section 1.1. Algorithm 1 is adaptable to a wide range of applications and COVID-19 is used as a special case to illustrate its mechanics.\\n\\nModel Optimisation: Rather than just averaging an ensemble of models to reduce variance (as in the bagging case) or evaluating surrogate models, Algorithm 2 combines cross-validation, bagging and step-wise assessment based on updatable parameters (model weights, in this case), exhibiting a robust performance of the algorithm. Application of CNN on COVID-19 data to illustrate robust data-driven solutions for global challenges, Algorithm 2 is adaptive to a wide range of techniques–unsupervised and supervised.\\n\\n\\n\\nApplications: A novel approach towards application in the context of SDG initiatives.\\nIt complements dashboard descriptive data, in an interdisciplinary context as shown in Figure 2.\\nSpatio-temporal variations provide insights into how different societies have been impacted by the pandemic. Researchers focusing on other SDG-related challenges can easily adapt the mechanics of the two algorithms and the data-driven generic framework to their specific needs in search of robust performance.\\n\\n\\n\\n\\n\\nAlgorithm 1\\xa0 Adaptation of the SMA Algorithm (Mwitondi et al. 2020) for Animation & Visualisation.\\n\\nThe study methodology is based on structured and unstructured data. Its basic ideas are in the Sample-Measure-Assess (SMA) algorithm originally developed for structured data (Mwitondi et al. 2020, Said & Mwitondi 2021).Figure 2\\xa0 A diagrammatical illustration of the interaction of challenges, data and skills.Figure 3\\xa0 Graphical illustration of the CNN classification and assessment process.Algorithm 1\\xa0 Adaptation of the SMA Algorithm (Mwitondi et al. 2020) for Animation & Visualisation.2 Methodology\\nThe section hinges on addressing data randomness that characterises all learning models and it is organised as follows. Section 2.1 provides a data–driven generic framework for addressing societal challenges from an interdisciplinary perspective, using sophisticated data modelling tools. It is followed by a data description in Section 2.2 and an outline of the implementation strategy in Section 2.3.\\n\\n2.1 A Data-Driven Generic Framework\\nFigure 2 highlights the overlap of global challenges, data and relevant skills, from which the motivation of this work derives. It constitutes a logical relationship between the three categories that are fundamental in addressing cross–sectoral or global challenges, with its overlapping components forming the basis for addressing data randomness, as presented in Section 2.3. The intersections #1 through #4 are crucial as they resonate with the interdisciplinary approach to problem solving. For example, #1 and #4 relate to aspects of data science, while #2 and #4 may relate to specific knowledge domains. Similar interpretations can be made for #1, #2 and #4 or the other tripartites.\\nInterdisciplinary approaches to tackling global challenges, combining domain knowledge, data, tools and skills are well-documented. In recent years researchers have focused on integrating different sources of knowledge across the broad spectrum of SDG, with poverty, food security, gender equality, health, education, innovation and climate change standing out (Mwitondi et al. 2018b). One good example would be the ongoing debates on the role of disparate knowledge sets and expertise in managing the impact of climate change which expose cross-sectoral gaps in learning about data, national policies and various aspects of science as outlined in Pearce et al. (2018). COVID-19 delivers an even better example of the need for interdisciplinarity in tackling global challenges. Evidence of direct correlation between environmental pollution and contagion dynamics imply that interdisciplinarity is required in understanding the pandemic’s contagion diffusion patterns in relation to multiplicity of environmental, socio–economic as well as its geographical diversity (Bontempi et al. 2020). This is particularly important, since COVID-19 has generated arguments and counter-arguments on how it should be managed–from balancing societal health and economic aspects to vaccine uptakes and their ramifications on social interactions. Apparently, detaching the categories creates knowledge gaps and the more they overlap, the more cohesive knowledge is attained. These dynamics inevitably lead to data randomness, inherently affecting modelling results and hence the conclusions drawn from them. The setup naturally appeals to developing robust solutions for SDG challenges such as COVID–19, in which not only data variability abounds (Mwitondi et al. 2013), but also definitions and interpretations tend to vary over time, a phenomenon commonly referred to as concept drift (Tsymbal et al. 2008, Žliobaitė et al. 2016). Data randomness and concept drift present natural challenges to algorithmic learning, on which this paper focuses (Mwitondi & Said 2021).\\n\\n\\n2.2 Data Sources and Visualisation\\nIn the light of the impact of COVID–19 on SDG, data deluge and computing power, each SDG can reasonably be seen as a source of Big Data (Kharrazi 2017, Kruse et al. 2016, Yan et al. 2015, Mwitondi et al. 2018a, b). For the purpose of this work, structured data came from the European Centre for Disease Prevention and Control (ECDC) (ECDC 2020) and the UK Office of National Statistics website (ONS 2020). The former provided daily updates on cases and deaths per country based on a 14-day notification rate of new COVID-19 cases and deaths while the latter provided multiple data files on business, industry and trade as well as on the general economy and on the dynamics on the labour market before and during the pandemic. Preparation of structured data for animation and visualisation required re-arranging the data points in such a way that the adapted Algorithm 1, described below, could iterate across attributes. Table 1 lists a typical choice of variables of interest. Notice that while this list satisfies the requirements for the illustrations in this paper, it is by no means exhaustive. Its elements are dependent on the problem at hand and must carefully be selected based on the data-driven generic framework in Figure 2. In other words, variable selection is problem-dependent and it should be guided by expert knowledge in both the underlying domain and data analytics. Identifying the necessary skills and modelling techniques is also a function of the problem space. It is that multi-dimensional joint decision that defines the functionality of the framework in Figure 2.\\n\\n\\nTable 1\\nTypical variables of interest for animation and visualisation.\\n\\n\\n\\n\\nVARIABLES\\nNOTATION\\nDESCRIPTION AND RELEVANCE\\n\\n\\n\\nPopulation\\nδ\\nPopulation affected by a phenomenon: This may be a national, regional or city population from which other variables are obtained\\n\\n\\n\\nGDP\\nɣ\\nGross Domestic Product of a country: Vital for comparative purposes\\n\\n\\n\\nUnemployment\\nξ\\nUnemployment rate: Global, national, regional or city level\\n\\n\\n\\nLocation\\nλ\\nWhere a phenomenon happens: Useful for spatio–temporal comparisons\\n\\n\\n\\nTime\\nτ\\nYear, month, week, day etc: Useful for spatio–temporal comparisons\\n\\n\\n\\nCOVID–19\\nκ\\nDeaths, infections, hospitalisation rates, variants\\n\\n\\n\\nPPE\\nπ\\nPersonal Protective Equipment: Associated with COVID–19 etc.\\n\\n\\n\\n\\n\\nThe unstructured dataset is a large COVID–19 X–Ray collection of 1840 image files, downloaded from GitHub (Cohen et al. 2020) and 1341 normal X–Ray image files obtained from from Kaggle (Kaggle 2020). Sources of both structured and unstructured data used in this research are regularly updated, which makes it possible for the paper’s modelling results to be reproduced and updated. The adopted implementation strategy is based on a two–fold adaptation of the SMA algorithm as outlined below.\\n\\n\\n2.3 Implementation Strategy\\nAdaptation of the SMA algorithm is two–fold. The first modification is for animation and visualisation, in search of informative COVID–19 patterns from multiple attributes in a two-dimensional space. The second is for the classification of unstructured data using the Convolutional Neural Network (CNN) model as described in (LeCun, Jackel, Boser, Denker, Graf, Guyon, Henderson, Howard & Hubbard 1989), (LeCun, Boser, Denker, Henderson, Howard, Hubbard & Jackel 1989) and (Fukushima 1980), which is also used to carry out multiple sampling of COVID–19 imagery data. Both adaptations have the potential for providing crucial information to decision makers.\\n\\n2.3.1 SMA Adaptation for Animation and Visualisation\\nThis adaptation is designed for carrying out animation and graphical data visualisation of selected variables, to reflect the multi-dimensional impact of COVID–19 in a two–dimensional space. Its specific applications will vary and must typically be guided by the framework in Figure 2. For example, the choice of attributes to be animated and/or visualised will depend on the intended purpose of the study. Which variables to display and which cut-off points to trigger which alarms are decisions that require underlying domain knowledge and not a purely data science problem. Algorithm 1 represents a simple variant of the SMA algorithm. It is designed to display multiple variables in a two–dimensional space, comparing relevant parameters and firing a message on meeting pre-specified criteria. Its mechanics are illustrated below, using the notation in Table 1, collectively featuring a super set of data sources Γ.\\nThe subset ø ⊆ Γ contains variables of interest, based on which the algorithm iteratively displays multi-dimensional data in a two-dimensional space, triggering alarms in accordance with pre-set conditions. For example, as the unemployment rate in a particular borough in England reaches a specific level, e.g. ξ ≥ 3.5% while death rates are above 1000 per day, at time τ = t*, the Chancellor of the Exchequer may need to consider taking action on the furlough scheme, say. Presenting structured data in both visual static and animated forms, provides clear insights to stakeholders in addressing societal challenges such as COVID–19. The main focus is on both Γ and ø which will always need to be adapted to handle new cases. Practical illustrations of the algorithm’s mechanics are given in Section 3.1.\\n\\n\\n2.3.2 SMA Adaptation for Convolutional Neural Networks\\nFigure 3 provides a graphical illustration of our adaptation of the SMA algorithm in addressing data randomness via multiple model training, testing and assessing. The data repository is a large data source from which multiple training and testing samples are drawn, with or without replacement. Its data contents can be either structured or unstructured.\\nAt the preparatory level, the investigator examines the overall behaviour of the data through visualisation, animation or other methods of inspection, such as outlier detection and missing values, in order to ascertain its validity for applying the adopted modelling technique. A machine learning model trained and tested on different training samples will typically yield different outcomes. Performance assessment is made on the basis of specific metrics generated and assessed via the two algorithms, as described in Section 2.3. Given a dataset with class labels, y, the SMA algorithm applies a learning model which, without loss of generality, we can define as in Equation 1\\n\\n(1)\\nFϕ=P︸x,y~Dϕx≠y\\nM1\\n\\\\documentclass[10pt]{article}\\n\\\\usepackage{wasysym}\\n\\\\usepackage[substack]{amsmath}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage[mathscr]{eucal}\\n\\\\usepackage{mathrsfs}\\n\\\\usepackage{pmc}\\n\\\\usepackage[Euler]{upgreek}\\n\\\\pagestyle{empty}\\n\\\\oddsidemargin -1.0in\\n\\\\begin{document}\\n\\\\[F\\\\left(\\\\phi \\\\right) = \\\\underbrace P_{x,y\\\\sim {\\\\mathcal{D}}}\\\\left[ {\\\\phi \\\\left(x \\\\right) \\\\ne y} \\\\right]\\\\]\\n\\\\end{document}\\n\\n\\n\\nwhere DM12\\n\\\\documentclass[10pt]{article}\\n\\\\usepackage{wasysym}\\n\\\\usepackage[substack]{amsmath}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage[mathscr]{eucal}\\n\\\\usepackage{mathrsfs}\\n\\\\usepackage{pmc}\\n\\\\usepackage[Euler]{upgreek}\\n\\\\pagestyle{empty}\\n\\\\oddsidemargin -1.0in\\n\\\\begin{document}\\n\\\\[\\n{\\\\cal D}\\n \\\\]\\n\\\\end{document}\\n is the underlying distribution and P[ø(x) ≠ y] is the probability of disparity between the predicted and actual values. By repeatedly sampling from the provided data source, modelling and carrying out a comparative assessment of the results, the SMA algorithm provides a unifying environment with the potential to yield consistent results across samples. For classification problems, it proceeds by training and validating the model in Equation 1 on random samples, keeping the samples stateless across all iterations. Thus, multiple machine learning models are fitted, compared and updated over several iterations, finally selecting the best performing model based on the probability\\n\\n(2)\\nPΨD,POP≥ΨB,POP=1⇔EΨD,POP−ΨB,POP=EΔ≥0\\nM2\\n\\\\documentclass[10pt]{article}\\n\\\\usepackage{wasysym}\\n\\\\usepackage[substack]{amsmath}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage[mathscr]{eucal}\\n\\\\usepackage{mathrsfs}\\n\\\\usepackage{pmc}\\n\\\\usepackage[Euler]{upgreek}\\n\\\\pagestyle{empty}\\n\\\\oddsidemargin -1.0in\\n\\\\begin{document}\\n\\\\[P\\\\left({{\\\\Psi _{D,POP}} \\\\ge {\\\\Psi _{B,POP}}} \\\\right) = 1 \\\\Leftrightarrow {\\\\mathbb{E}}\\\\left[ {{\\\\Psi _{D,POP}} - {\\\\Psi _{B,POP}}} \\\\right] = {\\\\mathbb{E}}\\\\left[ \\\\Delta \\\\right] \\\\ge 0\\\\]\\n\\\\end{document}\\n\\n\\n\\nwhere 𝔼[Δ] is the estimated difference between the population error ψD,POP and the validation error ψB,POP. Adaptation of the SMA algorithm is illustrated via Convolutional Neural Network (CNN)–a machine learning technique, typically used for classifying image data such as the X–Ray data, in this case. Its original ideas derive from the work of a Japanese Scientist, Kunihiko Fukushima (Fukushima 1980), on neocognitron–a basic image recognition neural network and developed through the work of LeCun, Jackel, Boser, Denker, Graf, Guyon, Henderson, Howard & Hubbard (1989), LeCun, Boser, Denker, Henderson, Howard, Hubbard & Jackel (1989) into the modern day CNN via the ImageNet data challenge Krizhevsky et al. (2012).\\nA CNN model performs classification based on image inputs and a target variable of known classes of the images. It is typically composed of multiple layers of artificial neurons, imitating biological neurons, as graphically illustrated in Figure 4. It processes the convolution computing for the input multichannel extracting features on its plane.\\n\\n\\nFigure 4\\xa0 An architecture of a CNN model.\\n\\nEach convolutional kernel is convolved across the width and height of 2D input volumes from the previous layer, computing the dot product between the kernel and the input. If we let X be an n × n data matrix and W a k × k matrix of weights, which is a 2-dimensional filter with k ≤ n (see Figure 5), then\\n\\n\\nFigure 5\\xa0 Convolutional values are obtained by sliding the kernel over data.\\n\\n\\n(3)\\nXk(i,j)=xi,jxi,j+1xi,j+2…xi,j+k−1xi+1,jxi+1,j+1xi+1,j+2…xi+1,j+k−1xi+2,jxi+2,j+1xi+2,j+2…xi+2,j+k−1……………xi+k−1,jxi+k−1,j+1xi+k−1,j+2…xi+k−1,j+k−1\\nM3\\n\\\\documentclass[1'"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_8RIb1B_igZ"
      },
      "source": [
        "for i in range(len(ds_codata.Content)):\n",
        "  ds_codata.Content[i] = str(ds_codata.Content[i]).replace('Abstract\\r\\n', '')\n",
        "  # ds_codata.Content[i] = re.sub(r'[^a-zA-Z.]', ' ', str(ds_codata.Content[i]))\n",
        "  # ds_codata.Content[i] = ds_codata.Content[i].replace('\\n', '')\n",
        "  # ds_codata.Content[i] = ds_codata.Content[i].replace('  ', '')"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlxrqwDXAQr9"
      },
      "source": [
        "ds_codata.Content[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv4Kf0sOAR4u"
      },
      "source": [
        "ds_codata_list = dataset_divider(ds_codata, 'Content', 7)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvB8Jg54AzhZ",
        "outputId": "dc14fea4-539d-4a3b-986f-2168ac305bd1"
      },
      "source": [
        "len(ds_codata_list)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "269"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGYGU7pPBMeT"
      },
      "source": [
        "# g = []\n",
        "# for i in ds_codata.Content:\n",
        "#   g.append(len(i))\n",
        "\n",
        "# print(max(g))\n",
        "# print(min(g))"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pMhxRPkAmJC",
        "outputId": "f0107ced-03b8-42b9-ee7c-a3d8572e4b47"
      },
      "source": [
        "ds_codata_translated = Translator(ds_codata_list, 'ar')"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry_ZTJ8WAmLe",
        "outputId": "f9f87646-583a-4df8-f411-f672b20bfacb"
      },
      "source": [
        "len(ds_codata_translated)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "269"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "sgS5ICVeIp7e",
        "outputId": "5fef18fc-09e5-4a47-8312-942cbd5aae97"
      },
      "source": [
        "ds_codata_translated[0]"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1 المقدمة\\nتم تصميم أهداف الأمم المتحدة السبعة عشر للتنمية المستدامة (SDG) لمواجهة التحديات العالمية - الفقر ، والصحة ، وعدم المساواة ، وتغير المناخ ، والابتكار ، والتدهور البيئي ، والسلام والعدالة ، منذ إنشائها في عام 2015 ، في قلب استراتيجيات التنمية من أجل الحكومات المركزية والمحلية والشركات والمؤسسات في جميع أنحاء العالم (الأمم المتحدة 2015). يظهر تأثير COVID-19 في القطاعات والمجالات التي تصف أهداف التنمية المستدامة (Rothan & Byrareddy 2020). وبينما تتطلب معالجة التحديات العالمية من هذا النوع جهودًا من مختلف التخصصات والقطاعات والحدود والتشريعات ؛ يستمر العمل في الصومعة في الهيمنة على المبادرات البحثية في العديد من المجالات ، مما يؤدي باستمرار إلى خلق فجوات معرفية. لقد ذكّرنا جائحة COVID-19 - وهو مثال نموذجي لتحدي عالمي ، بمثل هذه الفجوات في معرفتنا ، والتي تتطلب مبادرات تعاونية ومتعددة التخصصات تعتمد على البيانات لسدها. على الرغم من تأثيره المدمر على أساليب حياتنا ، فقد قيل إن COVID-19 قد قدم لنا فرصة ممتازة لتسريع تحقيق أهداف التنمية المستدامة من خلال التقنيات التي تعتمد على البيانات (Pan & Zhang 2020) ، لا سيما بسبب حدوث COVID-19 وسط طوفان البيانات والقدرات المتزايدة في التعامل مع البيانات الضخمة (Wang et al. 2020). لقد تعاملت بلدان مختلفة مع الوباء باستخدام استراتيجيات مختلفة ، ولم تكن الحاجة إلى مشاركة البيانات عبر الحدود الجغرافية أكبر من أي وقت مضى.\\nإحدى القضايا الرئيسية التي يواجهها الباحثون وسيستمرون في مواجهتها في المستقبل هي الاختلافات المكانية والزمانية وتأثيرها على الاستنتاجات التي نتوصل إليها بشأن الحلول القائمة على البيانات. على الرغم من الآثار المدمرة ، فإن الاختلافات المكانية والزمانية لـ COVID-19 تقدم فرصة ممتازة لمجتمع البحث لسد الفجوات المعرفية في معالجة التحديات المجتمعية من خلال نمذجة البيانات متعددة التخصصات. نظرًا لأن الحلول التي تعتمد على البيانات تعتمد على استقرار افتراضات نمذجة البيانات الأساسية ، فإن الفجوات المعرفية تنشأ حتمًا عند انتهاك الافتراضات. نقدم إطارًا عامًا لملء هذه الفجوات ، استنادًا إلى خوارزميتين تعتمدان على البيانات تجمعان البيانات والتعلم الآلي والتخصصات المتعددة لسد الفجوات المعرفية المجتمعية من خلال تسليط الضوء على توقيت وظروف التدخلات. استخدام بيانات منظمة COVID-19 المنظمة التي تم الحصول عليها من المركز الأوروبي للوقاية من الأمراض ومكافحتها (ECDC) ؛ بيانات عن تأثيرها ، تم الحصول عليها من مكتب الإحصاءات الوطنية بالمملكة المتحدة ، وصور غير منظمة COVID-19 X تم الحصول عليها من GitHub و Kaggle ، نقدم خوارزميتين - واحدة للرسوم المتحركة والتصور والأخرى لتحسين التصنيف على أساس التكيف العصبي التلافيفي نموذج الشبكة (سي إن إن).\\nتم تضمين حداثة الورقة في الخوارزميتين - كلاهما مقتبس من خوارزمية أخذ العينات والقياس والتقييم (SMA) لمعالجة عشوائية البيانات ، والتي تم تطويرها في الأصل بواسطة Mwitondi et al. (2018 أ ، ب ، 2020) لنمذجة البيانات المنظمة بناءً على ملاءمة النموذج الإحصائي وتقييمه. التكيف في القسم 2 ، له صدى مع المناقشات البحثية متعددة التخصصات في معالجة التحديات العالمية. ويتم تنظيم هذه الورقة على النحو التالي. يقدم القسم 1 مقدمة ودوافع وسؤال بحث وأهداف. يقدم القسم 2 الأساليب - الإطار ومصادر البيانات وتقنيات النمذجة. يقدم القسم 3 التحليلات ويختتم القسم 4 العمل ويسلط الضوء على المسارات الاتجاهية الجديدة للبحث.\\n\\n1.1 الأعمال ذات الصلة\\nكما هو مذكور أعلاه ، كان الدافع وراء هذا العمل هو نمذجة البيانات الضخمة لأهداف التنمية المستدامة (BDMSDG) (Mwitondi وآخرون. .2020 ، بارتيك وآخرون 2020). تمثل التفاعلات المعقدة لأهداف التنمية المستدامة ، وحجم وديناميكيات سمات بياناتها ، فضلاً عن الاختلافات الاجتماعية والاقتصادية والثقافية العميقة والواسعة في جميع أنحاء العالم تحديًا وفرصة لمشروع SDG. تؤثر هذه السمات على الحلول القائمة على البيانات لأنها لا تساهم في عشوائية البيانات فحسب ، بل تساهم أيضًا في الاختلافات في علاقات البيانات والتعريفات الأساسية بمرور الوقت ، والمعروفة باسم انحراف المفهوم (Zenisek et al. 2019). لذلك ، من المعقول مواءمة الاختلافات المكانية والزمانية لتأثير COVID-19 مع إمكانية سد الفجوات المعرفية المجتمعية واكتساب فهم أفضل للتحديات التي نواجهها من خلال الحلول القائمة على البيانات. تمت دراسة الاختلافات في البيانات على نطاق واسع ويستمد هذا العمل من تقنيات النمذجة الحالية مثل المتغيرات القياسية للتحقق المتبادل (Bo et al. 2006 ، Xu & Goodacre 2018) وأهمية ميزة التقليب (Galkin et al. 2018). يُستمد العمل من نماذج إحصائية مثل التعبئة والتعبئة ، والتي تعتمد إما على تجميع المصنفات أو تمثيل العينة (Mwitondi et al. 2019). يكمن تفوق خوارزمية SMA في آلياتها المدمجة للتعامل بكفاءة مع عشوائية البيانات (Mwitondi et al. 2019 ، 2020.).\\nمنذ ظهور جائحة COVID-19 ، أصبحت أدوات تصور البيانات شائعة بشكل متزايد في جميع أنحاء العالم. قامت العديد من لوحات المعلومات الموجودة مسبقًا مثل Our World in Data (Roser et al. 2018) ومجموعة البنك الدولي (WBGroup 2018) ومركز موارد فيروس كورونا بجامعة جونز هوبكنز (CRC 2021) ومعهد الألفية (MI 2021) بتطوير أدوات لرسم الخرائط الوباء في جميع أنحاء العالم ، بعضها في الوقت الفعلي تقريبًا. الشكل 1 ، المأخوذ من لوحة معلومات COVID-19 بجامعة جونز هوبكنز في 7 يوليو 2021 في الساعة 17:21 ، يعرض الحالات والوفيات وجرعات اللقاح التي تُدار حسب البلد بالإضافة إلى سمات البيانات الأخرى عبر عناصر القائمة. في حين أن هذا النوع من تصور الأنماط مفيد للاتجاه الذي يتخذه الوباء ، تمامًا مثل الأدوات المذكورة أعلاه ، فهو في الأساس مولد إحصائي وصفي محسّن. إن اعتمادها على دقة البيانات الخاصة بكل بلد يترك العديد من الأسئلة دون إجابة. على سبيل المثال ، هل يعكس حقًا جمع البيانات وإعداد التقارير في جميع البلدان المعروضة؟ هل يوفر فهمًا أفضل للتحديات التي نواجهها؟ للحصول على إجابات لهذه الأسئلة والعديد من الأسئلة العامة الأخرى ، Zhang et al. (2014) يوصي باتباع نهج من القاعدة إلى القمة. تمت دراسة الدقة والاكتمال والاتساق والجوانب الأخرى لجودة البيانات على نطاق واسع ولا تزال نقطة محورية في العديد من المجالات (Cai & Zhu 2015، Zhang et al. 2017).\\n\\n\\nالشكل 1 لوحة معلومات مرئيات Johns Hopkins COVID-19.\\n(المصدر: https://coronavirus.jhu.edu/map.html)\\n\\nأدى ظهور لوحات المعلومات إلى مزيد من التفكير في كيفية استخدامها لتحسين عمليات صنع القرار من خلال زيادة الشفافية والمساءلة ومشاركة أصحاب المصلحة والحوكمة والترتيبات المؤسسية Matheus et al. (2020). يركز عملنا على كيفية استكمال البيانات الوصفية التي يمكن الوصول إليها ، من خلال لوحات المعلومات أو غير ذلك ، من خلال تقنيات النمذجة لدعم عمليات صنع القرار. إنه يسترشد بالتغيرات المكانية والزمانية في اكتساب رؤى حول كيفية تأثر المجتمعات المختلفة بالوباء.\\nتقترح هذه الورقة نهجًا متعدد التخصصات لمعالجة الأسئلة العامة السابقة بناءً على أساليب نمذجة البيانات المنظمة وغير المنظمة. الأول عبارة عن أداة رسوم متحركة تفاعلية للبيانات والتخيل مع قدرة مضمنة على إطلاق تنبيهات التحذير ، بينما يوفر الأخير قوة تنبؤية باستخدام بيانات الصور.\\n\\n\\n1.2 سؤال البحث والأهداف\\nالاختلافات المكانية والزمانية وعشوائية البيانات هي بعض العوامل الرئيسية المعروفة بأنها تؤثر على الاستنتاجات التي نستخلصها من الحلول القائمة على البيانات (Mwitondi & Said 2013). يجمع هذا العمل بين قوة البيانات الضخمة والتعلم الآلي والتخصصات المتعددة لمعالجة هذه المشكلات. باستخدام أمثلة من الحياة الواقعية استنادًا إلى بيانات جائحة COVID-19 ، ندرس كيف تتناسب المناهج الخاصة بكل بلد لمواجهة التحديات العالمية مع المنظور العالمي للحلول القائمة على البيانات. نسعى للإجابة على السؤال التالي: كيف يتردد صدى الاختلافات المكانية والزمانية مع المعالجة متعددة التخصصات للتحديات العالمية؟ للإجابة على السؤال السابق ، حددنا الأهداف التالية.\\n\\nلتوضيح فعالية التصور متعدد الأبعاد على المستوى الوطني لتأثير COVID-19 على المجتمعات.\\nلإثبات فعالية الجمع بين البيانات والتقنيات والمهارات في سياق متعدد التخصصات.\\nلاستخدام تصور البيانات متعدد الأبعاد في مساحة ثنائية الأبعاد لاتخاذ قرارات في الوقت المناسب بشأن تأثير الوباء والتحديات المجتمعية الأخرى.\\nلتوفير تطبيقات عملية لخوارزمية قوية للتعلم الآلي ، مع إمكانات مدمجة لاستيعاب المهارات متعددة التخصصات.\\nلتسليط الضوء على خارطة طريق لمواءمة الاستراتيجيات الوطنية مع المنشور العالمي للحلول القائمة على البيانات.\\n\\n\\n\\n1.3 المساهمة في المعرفة\\nتُستمد حداثة الورقة من الآليات التطبيقية للخوارزميات 1 و 2 ، في سياق إطار العمل المستند إلى البيانات في الشكل 2 وتدفق التنفيذ في الشكل 3. وتتوقف فكرتها الرئيسية على عشوائية البيانات التي تميز جميع نماذج التعلم باعتبارها سببًا رئيسيًا وراء الاختلافات المكانية والزمانية ، كما هو موضح في Mwitondi & Said (2013). باستخدام الرسوم التوضيحية لـ COVID-19 ، يسلط التطبيق الضوء على مسارات للجمع بين معرفة المجال والبيانات والأدوات والمهارات في مواجهة التحديات العالمية عبر طيف أهداف التنمية المستدامة. بناءً على الأدلة من الأدبيات ، نسلط الضوء على الجوانب التالية للمساهمة في المعرفة.\\n\\n\\nالشكل 2 توضيح تخطيطي لتفاعل التحديات والبيانات والمهارات.\\n\\n\\n\\nالشكل 3 رسم بياني لتصنيف CNN وعملية التقييم.\\n\\n\\n\\nمعالجة عشوائية البيانات: يؤدي إلى تقنيات النمذجة المحسنة لأنظمة دعم القرار.\\n\\nالرسوم المتحركة: التصور متعدد الأبعاد لسمات البيانات في مساحة ثنائية الأبعاد ، مما يسمح بالتدخل اليدوي أو الآلي عبر الخوارزمية 1 ، هو نظام دعم قرار محسّن يعتمد على البيانات ولا يتم توفيره بواسطة أي من الأدوات التي تمت مناقشتها في إطار العمل ذي الصلة في القسم 1.1 . الخوارزمية 1 قابلة للتكيف مع مجموعة واسعة من التطبيقات ويتم استخدام COVID-19 باعتباره s.حالة خاصة لتوضيح آلياتها.\\n\\nتحسين النموذج: بدلاً من مجرد حساب متوسط \\u200b\\u200bمجموعة من النماذج لتقليل التباين (كما في حالة التعبئة) أو تقييم النماذج البديلة ، تجمع الخوارزمية 2 بين التحقق المتقاطع والتعبئة والتقييم التدريجي بناءً على معلمات قابلة للتحديث (أوزان النموذج ، في هذه الحالة ) ، مما يعرض أداءً قويًا للخوارزمية. تطبيق CNN على بيانات COVID-19 لتوضيح الحلول القوية القائمة على البيانات للتحديات العالمية ، فإن الخوارزمية 2 تتكيف مع مجموعة واسعة من التقنيات - غير الخاضعة للإشراف والإشراف.\\n\\n\\n\\nالتطبيقات: نهج جديد للتطبيق في سياق مبادرات أهداف التنمية المستدامة.\\nيكمل البيانات الوصفية للوحة القيادة ، في سياق متعدد التخصصات كما هو موضح في الشكل 2.\\nتوفر الاختلافات المكانية والزمانية رؤى حول كيفية تأثر المجتمعات المختلفة بالوباء. يمكن للباحثين الذين يركزون على التحديات الأخرى المتعلقة بأهداف التنمية المستدامة أن يكيفوا بسهولة آليات الخوارزميتين والإطار العام المعتمد على البيانات لاحتياجاتهم الخاصة بحثًا عن أداء قوي.\\n\\n\\n\\n\\n\\nالخوارزمية 1 تكييف خوارزمية SMA (Mwitondi et al. 2020) للرسوم المتحركة والتصور.\\n\\nتعتمد منهجية الدراسة على بيانات منظمة وغير منظمة. أفكاره الأساسية موجودة في خوارزمية نموذج - قياس - تقييم (SMA) التي تم تطويرها في الأصل من أجل البيانات المنظمة (Mwitondi et al. 2020، Said & Mwitondi 2021).\\nكما هو مذكور أعلاه ، كان الدافع وراء هذا العمل هو نمذجة البيانات الضخمة لأهداف التنمية المستدامة (BDMSDG) (Mwitondi وآخرون. .2020 ، بارتيك وآخرون 2020). تمثل التفاعلات المعقدة لأهداف التنمية المستدامة ، وحجم وديناميكيات سمات بياناتها ، فضلاً عن الاختلافات الاجتماعية والاقتصادية والثقافية العميقة والواسعة في جميع أنحاء العالم تحديًا وفرصة لمشروع SDG. تؤثر هذه السمات على الحلول القائمة على البيانات لأنها لا تساهم في عشوائية البيانات فحسب ، بل تساهم أيضًا في الاختلافات في علاقات البيانات والتعريفات الأساسية بمرور الوقت ، والمعروفة باسم انحراف المفهوم (Zenisek et al. 2019). لذلك ، من المعقول مواءمة الاختلافات المكانية والزمانية لتأثير COVID-19 مع إمكانية سد الفجوات المعرفية المجتمعية واكتساب فهم أفضل للتحديات التي نواجهها من خلال الحلول القائمة على البيانات. تمت دراسة الاختلافات في البيانات على نطاق واسع ويستمد هذا العمل من تقنيات النمذجة الحالية مثل المتغيرات القياسية للتحقق المتبادل (Bo et al. 2006 ، Xu & Goodacre 2018) وأهمية ميزة التقليب (Galkin et al. 2018). يُستمد العمل من نماذج إحصائية مثل التعبئة والتعبئة ، والتي تعتمد إما على تجميع المصنفات أو تمثيل العينة (Mwitondi et al. 2019). يكمن تفوق خوارزمية SMA في آلياتها المدمجة للتعامل بكفاءة مع عشوائية البيانات (Mwitondi et al. 2019 ، 2020).\\nمنذ ظهور جائحة COVID-19 ، أصبحت أدوات تصور البيانات شائعة بشكل متزايد في جميع أنحاء العالم. قامت العديد من لوحات المعلومات الموجودة مسبقًا مثل Our World in Data (Roser et al. 2018) ومجموعة البنك الدولي (WBGroup 2018) ومركز موارد فيروس كورونا بجامعة جونز هوبكنز (CRC 2021) ومعهد الألفية (MI 2021) بتطوير أدوات لرسم الخرائط الوباء في جميع أنحاء العالم ، بعضها في الوقت الفعلي تقريبًا. الشكل 1 ، المأخوذ من لوحة معلومات COVID-19 بجامعة جونز هوبكنز في 7 يوليو 2021 في الساعة 17:21 ، يعرض الحالات والوفيات وجرعات اللقاح التي تُدار حسب البلد بالإضافة إلى سمات البيانات الأخرى عبر عناصر القائمة. في حين أن هذا النوع من تصور الأنماط مفيد للاتجاه الذي يتخذه الوباء ، تمامًا مثل الأدوات المذكورة أعلاه ، فهو في الأساس مولد إحصائي وصفي محسّن. إن اعتمادها على دقة البيانات الخاصة بكل بلد يترك العديد من الأسئلة دون إجابة. على سبيل المثال ، هل يعكس حقًا جمع البيانات وإعداد التقارير في جميع البلدان المعروضة؟ هل يوفر فهمًا أفضل للتحديات التي نواجهها؟ للحصول على إجابات لهذه الأسئلة والعديد من الأسئلة العامة الأخرى ، Zhang et al. (2014) يوصي باتباع نهج من القاعدة إلى القمة. تمت دراسة الدقة والاكتمال والاتساق والجوانب الأخرى لجودة البيانات على نطاق واسع ولا تزال نقطة محورية في العديد من المجالات (Cai & Zhu 2015، Zhang et al. 2017).\\n\\n\\nالشكل 1 لوحة معلومات مرئيات Johns Hopkins COVID-19.\\n(المصدر: https://coronavirus.jhu.edu/map.html)\\n\\nأدى ظهور لوحات المعلومات إلى مزيد من التفكير في كيفية استخدامها لتحسين عمليات صنع القرار من خلال زيادة الشفافية والمساءلة ومشاركة أصحاب المصلحة والحوكمة والترتيبات المؤسسية Matheus et al. (2020). يركز عملنا على كيفية استكمال البيانات الوصفية التي يمكن الوصول إليها ، من خلال لوحات المعلومات أو غير ذلك ، من خلال تقنيات النمذجة لدعم عمليات صنع القرار. إنه يسترشد بالتغيرات المكانية والزمانية في اكتساب رؤى حول كيفية تأثر المجتمعات المختلفة بالوباء.\\nتقترح هذه الورقة موافقة متعددة التخصصات.oach لمعالجة الأسئلة العامة السابقة بناءً على أساليب نمذجة البيانات المنظمة وغير المهيكلة. الأول عبارة عن أداة رسوم متحركة تفاعلية للبيانات والتصور مع قدرة مضمنة على إطلاق تنبيهات التحذير ، بينما يوفر الأخير قوة تنبؤية باستخدام بيانات الصور.\\n(المصدر: https://coronavirus.jhu.edu/map.html) 1.2 سؤال البحث والأهداف\\nالاختلافات المكانية والزمانية وعشوائية البيانات هي بعض العوامل الرئيسية المعروفة بأنها تؤثر على الاستنتاجات التي نستخلصها من الحلول القائمة على البيانات (Mwitondi & Said 2013). يجمع هذا العمل بين قوة البيانات الضخمة والتعلم الآلي والتخصصات المتعددة لمعالجة هذه المشكلات. باستخدام أمثلة من الحياة الواقعية استنادًا إلى بيانات جائحة COVID-19 ، ندرس كيف تتناسب المناهج الخاصة بكل بلد لمواجهة التحديات العالمية مع المنظور العالمي للحلول القائمة على البيانات. نسعى للإجابة على السؤال التالي: كيف يتردد صدى الاختلافات المكانية والزمانية مع المعالجة متعددة التخصصات للتحديات العالمية؟ للإجابة على السؤال السابق ، حددنا الأهداف التالية.\\n\\nلتوضيح فعالية التصور متعدد الأبعاد على المستوى الوطني لتأثير COVID-19 على المجتمعات.\\nلإثبات فعالية الجمع بين البيانات والتقنيات والمهارات في سياق متعدد التخصصات.\\nلاستخدام تصور البيانات متعدد الأبعاد في مساحة ثنائية الأبعاد لاتخاذ قرارات في الوقت المناسب بشأن تأثير الوباء والتحديات المجتمعية الأخرى.\\nلتوفير تطبيقات عملية لخوارزمية قوية للتعلم الآلي ، مع إمكانات مدمجة لاستيعاب المهارات متعددة التخصصات.\\nلتسليط الضوء على خارطة طريق لمواءمة الاستراتيجيات الوطنية مع المنشور العالمي للحلول القائمة على البيانات.\\nتُستمد حداثة الورقة من الآليات التطبيقية للخوارزميات 1 و 2 ، في سياق إطار العمل المستند إلى البيانات في الشكل 2 وتدفق التنفيذ في الشكل 3. وتتوقف فكرتها الرئيسية على عشوائية البيانات التي تميز جميع نماذج التعلم باعتبارها سببًا رئيسيًا وراء الاختلافات المكانية والزمانية ، كما هو موضح في Mwitondi & Said (2013). باستخدام الرسوم التوضيحية لـ COVID-19 ، يسلط التطبيق الضوء على مسارات للجمع بين معرفة المجال والبيانات والأدوات والمهارات في مواجهة التحديات العالمية عبر طيف أهداف التنمية المستدامة. بناءً على الأدلة من الأدبيات ، نسلط الضوء على الجوانب التالية للمساهمة في المعرفة.\\n\\n\\nالشكل 2 توضيح تخطيطي لتفاعل التحديات والبيانات والمهارات.\\n\\n\\n\\nالشكل 3 رسم بياني لتصنيف CNN وعملية التقييم.\\n\\n\\n\\nمعالجة عشوائية البيانات: يؤدي إلى تقنيات النمذجة المحسنة لأنظمة دعم القرار.\\n\\nالرسوم المتحركة: التصور متعدد الأبعاد لسمات البيانات في مساحة ثنائية الأبعاد ، مما يسمح بالتدخل اليدوي أو الآلي عبر الخوارزمية 1 ، هو نظام دعم قرار محسّن يعتمد على البيانات ولا يتم توفيره بواسطة أي من الأدوات التي تمت مناقشتها في إطار العمل ذي الصلة في القسم 1.1 . الخوارزمية 1 قابلة للتكيف مع مجموعة واسعة من التطبيقات ويتم استخدام COVID-19 كحالة خاصة لتوضيح آلياتها.\\n\\nتحسين النموذج: بدلاً من مجرد حساب متوسط \\u200b\\u200bمجموعة من النماذج لتقليل التباين (كما في حالة التعبئة) أو تقييم النماذج البديلة ، تجمع الخوارزمية 2 بين التحقق المتقاطع والتعبئة والتقييم التدريجي بناءً على معلمات قابلة للتحديث (أوزان النموذج ، في هذه الحالة ) ، مما يعرض أداءً قويًا للخوارزمية. تطبيق CNN على بيانات COVID-19 لتوضيح الحلول القوية القائمة على البيانات للتحديات العالمية ، فإن الخوارزمية 2 تتكيف مع مجموعة واسعة من التقنيات - غير الخاضعة للإشراف والإشراف.\\n\\n\\n\\nالتطبيقات: نهج جديد للتطبيق في سياق مبادرات أهداف التنمية المستدامة.\\nيكمل البيانات الوصفية للوحة القيادة ، في سياق متعدد التخصصات كما هو موضح في الشكل 2.\\nتوفر الاختلافات المكانية والزمانية رؤى حول كيفية تأثر المجتمعات المختلفة بالوباء. يمكن للباحثين الذين يركزون على التحديات الأخرى المتعلقة بأهداف التنمية المستدامة أن يكيفوا بسهولة آليات الخوارزميتين والإطار العام المعتمد على البيانات لاحتياجاتهم الخاصة بحثًا عن أداء قوي.\\n\\n\\n\\n\\n\\nالخوارزمية 1 تكييف خوارزمية SMA (Mwitondi et al. 2020) للرسوم المتحركة والتصور.\\n\\nتعتمد منهجية الدراسة على بيانات منظمة وغير منظمة. أفكاره الأساسية موجودة في خوارزمية نموذج - قياس - تقييم (SMA) التي تم تطويرها في الأصل للبيانات المهيكلة (Mwitondi et al. 2020، Said & Mwitondi 2021) الشكل 2 توضيح تخطيطي لتفاعل التحديات والبيانات والمهارات. توضيح رسومي لتصنيف CNN وعملية التقييم ، الخوارزمية 1 تكييف خوارزمية SMA (Mwitondi et al. 2020) للرسوم المتحركة والتصور .2 المنهجية\\nيتوقف القسم على معالجة البيانات العشوائية التي تميز جميع نماذج التعلم وهي منظمة على النحو التالي. يوفر القسم 2.1 إطارًا عامًا يعتمد على البيانات لمواجهة التحديات المجتمعية من منظور متعدد التخصصات ، باستخدام أدوات نمذجة البيانات المتطورة. يتبع ب.وصف البيانات في القسم 2.2 ومخطط إستراتيجية التنفيذ في القسم 2.3.\\n\\n2.1 إطار عام مستند إلى البيانات\\nيسلط الشكل 2 الضوء على تداخل التحديات العالمية والبيانات والمهارات ذات الصلة ، والتي يُستمد منها الدافع وراء هذا العمل. وهي تشكل علاقة منطقية بين الفئات الثلاث التي تعتبر أساسية في معالجة التحديات عبر القطاعات أو العالمية ، مع مكوناتها المتداخلة التي تشكل الأساس لمعالجة عشوائية البيانات ، كما هو موضح في القسم 2.3. تعد التقاطعات من رقم 1 إلى رقم 4 مهمة لأنها تتوافق مع النهج متعدد التخصصات لحل المشكلات. على سبيل المثال ، يرتبط الرقمان 1 و 4 بجوانب علم البيانات ، في حين أن الرقمين 2 و 4 قد يتعلقان بمجالات معرفة محددة. يمكن إجراء تفسيرات مماثلة للرقم 1 ورقم 2 ورقم 4 أو الثلاثية الأخرى.\\nمناهج متعددة التخصصات لمواجهة التحديات العالمية ، والجمع بين معرفة المجال والبيانات والأدوات والمهارات موثقة جيدًا. ركز الباحثون في السنوات الأخيرة على دمج مصادر المعرفة المختلفة عبر مجموعة واسعة من أهداف التنمية المستدامة ، مع إبراز الفقر والأمن الغذائي والمساواة بين الجنسين والصحة والتعليم والابتكار وتغير المناخ (Mwitondi et al. 2018b). قد يكون أحد الأمثلة الجيدة هو المناقشات الجارية حول دور مجموعات المعرفة المتباينة والخبرات في إدارة تأثير تغير المناخ والتي تكشف عن فجوات عبر القطاعات في التعرف على البيانات والسياسات الوطنية والجوانب المختلفة للعلم على النحو المبين في Pearce et al. (2018). يقدم COVID-19 مثالًا أفضل على الحاجة إلى تعددية التخصصات في مواجهة التحديات العالمية. تشير الأدلة على الارتباط المباشر بين التلوث البيئي وديناميات العدوى إلى أن التخصصات المتعددة مطلوبة في فهم أنماط انتشار العدوى للوباء فيما يتعلق بتنوع التنوع البيئي والاجتماعي والاقتصادي وكذلك الجغرافي (Bontempi et al. 2020). هذا مهم بشكل خاص ، لأن COVID-19 قد ولّد حججًا وحججًا مضادة حول كيفية إدارته - من موازنة الصحة المجتمعية والجوانب الاقتصادية إلى امتصاص اللقاح وتداعياته على التفاعلات الاجتماعية. على ما يبدو ، فإن فصل الفئات يخلق فجوات معرفية وكلما زاد تداخلها ، يتم الحصول على معرفة أكثر تماسكًا. تؤدي هذه الديناميكيات حتمًا إلى عشوائية البيانات ، مما يؤثر بطبيعته على نتائج النمذجة ومن ثم الاستنتاجات المستخلصة منها. يستدعي الإعداد بشكل طبيعي تطوير حلول قوية لتحديات أهداف التنمية المستدامة مثل COVID-19 ، والتي لا تكثر فيها تباين البيانات فقط (Mwitondi et al. 2013) ، ولكن أيضًا التعاريف والتفسيرات تميل إلى التباين بمرور الوقت ، وهي ظاهرة يشار إليها عادةً باسم المفهوم. الانجراف (Tsymbal et al. 2008، liobaitė et al. 2016). تمثل عشوائية البيانات وانحراف المفهوم تحديات طبيعية للتعلم الخوارزمي ، والتي تركز عليها هذه الورقة (Mwitondi & Said 2021).\\n\\n\\n2.2 مصادر البيانات والتصور\\nفي ضوء تأثير COVID-19 على أهداف التنمية المستدامة وطوفان البيانات وقوة الحوسبة ، يمكن اعتبار كل هدف من أهداف التنمية المستدامة بشكل معقول مصدرًا للبيانات الضخمة (خرازي 2017 ، كروس وآخرون 2016 ، يان وآخرون 2015 ، مويتوندي وآخرون 2018 أ ، ب). لغرض هذا العمل ، جاءت البيانات المنظمة من المركز الأوروبي للوقاية من الأمراض ومكافحتها (ECDC) (ECDC 2020) والموقع الإلكتروني لمكتب الإحصاء الوطني في المملكة المتحدة (ONS 2020). قدم الأول تحديثات يومية عن الحالات والوفيات في كل بلد بناءً على معدل الإخطار لمدة 14 يومًا لحالات COVID-19 الجديدة والوفيات بينما قدم الأخير ملفات بيانات متعددة حول الأعمال والصناعة والتجارة وكذلك عن الاقتصاد العام وعن ديناميات سوق العمل قبل الجائحة وأثناءها. يتطلب إعداد البيانات المنظمة للرسوم المتحركة والتصور إعادة ترتيب نقاط البيانات بطريقة يمكن للخوارزمية المكيفة 1 ، الموضحة أدناه ، تكرارها عبر السمات. يسرد الجدول 1 اختيارًا نموذجيًا للمتغيرات ذات الأهمية. لاحظ أنه في حين أن هذه القائمة تفي بمتطلبات الرسوم التوضيحية في هذه الورقة ، فهي ليست شاملة بأي حال من الأحوال. تعتمد عناصرها على المشكلة المطروحة ويجب اختيارها بعناية بناءً على إطار العمل العام المستند إلى البيانات في الشكل 2. وبعبارة أخرى ، يعتمد اختيار المتغير على المشكلة ويجب أن يسترشد بمعرفة الخبراء في كل من المجال الأساسي و تحليلات البيانات. تحديد المهارات الضرورية وتقنيات النمذجة هو أيضًا وظيفة لمساحة المشكلة. هذا القرار المشترك متعدد الأبعاد هو الذي يحدد وظائف الإطار في الشكل 2.\\n\\n\\nالجدول 1\\nالمتغيرات النموذجية ذات الأهمية للرسوم المتحركة والتصور.\\n\\n\\n\\n\\nالمتغيرات\\nالرموز\\nالوصف والصلة\\n\\n\\n\\nتعداد السكان\\nδ\\nالسكان المتأثرون بظاهرة: قد يكونون سكانًا محليين أو إقليميين أو سكان مدينة يتم الحصول على متغيرات أخرى منها\\n\\n\\n\\nالناتج المحلي الإجمالي\\nɣ\\nالناتج المحلي الإجمالي لبلد ما: حيوي لأغراض المقارنة\\n\\n\\n\\nالبطالة\\nξ\\nمعدل البطالة: على المستوى العالمي أو الوطني أو الإقليمي أو المدينة\\n\\n\\n\\nموقع\\nλ\\nأين.ظاهرة تحدث: مفيدة للمقارنات المكانية والزمانية\\n\\n\\n\\nزمن\\nτ\\nالسنة والشهر والأسبوع واليوم إلخ: مفيدة للمقارنات المكانية والزمانية\\n\\n\\n\\nكوفيد -19\\nκ\\nالوفيات ، العدوى ، معدلات الاستشفاء ، المتغيرات\\n\\n\\n\\nمعدات الوقاية الشخصية\\nπ\\nمعدات الحماية الشخصية: المرتبطة بـ COVID – 19 إلخ.\\n\\n\\n\\n\\n\\nمجموعة البيانات غير المهيكلة عبارة عن مجموعة كبيرة من COVID-19 X-Ray من 1840 ملف صورة ، تم تنزيلها من GitHub (Cohen et al. 2020) و 1341 ملف صور X-Ray عادي تم الحصول عليها من Kaggle (Kaggle 2020). يتم تحديث مصادر البيانات المنظمة وغير المهيكلة المستخدمة في هذا البحث بانتظام ، مما يجعل من الممكن إعادة إنتاج نتائج النمذجة بالورقة وتحديثها. تعتمد استراتيجية التنفيذ المعتمدة على تكييف مزدوج لخوارزمية SMA على النحو المبين أدناه.\\n\\n\\n2.3 استراتيجية التنفيذ\\nتكييف خوارزمية SMA ذو شقين. التعديل الأول للرسوم المتحركة والتصور ، بحثًا عن أنماط COVID – 19 مفيدة من سمات متعددة في فضاء ثنائي الأبعاد. والثاني هو لتصنيف البيانات غير المهيكلة باستخدام نموذج الشبكة العصبية التلافيفية (CNN) كما هو موصوف في (LeCun، Jackel، Boser، Denker، Graf، Guyon، Henderson، Howard & Hubbard 1989)، (LeCun، Boser، Denker، Henderson ، Howard، Hubbard & Jackel 1989) و (Fukushima 1980) ، والتي تُستخدم أيضًا لإجراء أخذ عينات متعددة من بيانات صور COVID-19. كلا التعديلين لديه القدرة على توفير المعلومات الهامة لصانعي القرار.\\n\\n2.3.1 تكييف SMA للرسوم المتحركة والتصور\\nتم تصميم هذا التكيف لتنفيذ الرسوم المتحركة وتصور البيانات الرسومية للمتغيرات المحددة ، لتعكس التأثير متعدد الأبعاد لـ COVID-19 في مساحة ثنائية الأبعاد. ستختلف تطبيقاتها المحددة ويجب أن تسترشد عادةً بإطار العمل في الشكل 2. على سبيل المثال ، سيعتمد اختيار السمات المراد تحريكها و / أو تصورها على الغرض المقصود من الدراسة. المتغيرات التي يجب عرضها وأي نقاط التوقف لإطلاق الإنذارات هي قرارات تتطلب معرفة المجال الأساسي وليست مشكلة علم بيانات بحتة. تمثل الخوارزمية 1 متغيرًا بسيطًا لخوارزمية SMA. إنه مصمم لعرض متغيرات متعددة في فضاء ثنائي الأبعاد ، ومقارنة المعلمات ذات الصلة وإطلاق رسالة عند تلبية المعايير المحددة مسبقًا. يتم توضيح آلياتها أدناه ، باستخدام الرموز الموجودة في الجدول 1 ، والتي تعرض بشكل جماعي مجموعة فائقة من مصادر البيانات.\\nتحتوي المجموعة الفرعية ø ⊆ Γ على متغيرات ذات أهمية ، بناءً عليها تعرض الخوارزمية بشكل متكرر بيانات متعددة الأبعاد في فضاء ثنائي الأبعاد ، مما يؤدي إلى تشغيل الإنذارات وفقًا للشروط المحددة مسبقًا. على سبيل المثال ، حيث يصل معدل البطالة في منطقة معينة في إنجلترا إلى مستوى معين ، على سبيل المثال ξ ≥ 3.5٪ بينما معدلات الوفيات أعلى من 1000 يوميًا ، في الوقت τ = t * ، قد يحتاج وزير الخزانة إلى التفكير في اتخاذ إجراء بشأن مخطط الإجازة ، على سبيل المثال. يوفر تقديم البيانات المنظمة في كل من الأشكال المرئية الثابتة والمتحركة رؤى واضحة لأصحاب المصلحة في معالجة التحديات المجتمعية مثل COVID-19. ينصب التركيز الرئيسي على كل من Γ و ø اللذين سيحتاجان دائمًا إلى التكيف للتعامل مع الحالات الجديدة. ترد الرسوم التوضيحية العملية لميكانيكا الخوارزمية في القسم 3.1.\\n\\n\\n2.3.2 تكييف SMA للشبكات العصبية التلافيفية\\nيقدم الشكل 3 توضيحًا رسوميًا لتكييفنا لخوارزمية SMA في معالجة عشوائية البيانات من خلال نماذج متعددة للتدريب والاختبار والتقييم. يعد مستودع البيانات مصدرًا كبيرًا للبيانات يتم من خلاله سحب عينات تدريب واختبار متعددة ، مع استبدال أو بدون استبدال. يمكن أن تكون محتويات البيانات الخاصة بها إما منظمة أو غير منظمة.\\nفي المستوى التحضيري ، يفحص المحقق السلوك العام للبيانات من خلال التصور أو الرسوم المتحركة أو طرق الفحص الأخرى ، مثل الكشف عن القيم الخارجية والقيم المفقودة ، من أجل التأكد من صلاحيتها لتطبيق تقنية النمذجة المعتمدة. نموذج التعلم الآلي الذي تم تدريبه واختباره على عينات تدريب مختلفة سيؤدي عادةً إلى نتائج مختلفة. يتم إجراء تقييم الأداء على أساس مقاييس محددة تم إنشاؤها وتقييمها عبر الخوارزميتين ، كما هو موضح في القسم 2.3. بالنظر إلى مجموعة البيانات التي تحتوي على تسميات الفصل ، y ، تطبق خوارزمية SMA نموذجًا تعليميًا ، بدون فقدان التعميم ، يمكننا تحديده كما في المعادلة 1\\n\\n(1)\\nFϕ = P︸x، y ~ Dϕx ≠ y\\nم 1\\n\\\\ documentclass [10pt] {article}\\n\\\\ usepackage {wasysym}\\n\\\\ usepackage [suback] {amsmath}\\n\\\\ usepackage {amsfonts}\\n\\\\ usepackage {amssymb}\\n\\\\ usepackage {amsbsy}\\n\\\\ usepackage [mathscr] {eucal}\\n\\\\ usepackage {mathrsfs}\\n\\\\ usepackage {pmc}\\n\\\\ usepackage [أويلر] {Upgreek}\\n\\\\ pagestyle {فارغ}\\n\\\\ oddsidemargin -1.0in\\n\\\\ ابدأ {مستند}\\n\\\\ [F \\\\ left (\\\\ phi \\\\ right) = \\\\ underbrace P_ {x، y \\\\ sim {\\\\ mathcal {D}}} \\\\ left [{\\\\ phi \\\\ left (x \\\\ right) \\\\ ne y} \\\\ right] \\\\ ]\\n\\\\ نهاية {المستند}\\n\\n\\n\\nحيث DM12\\n\\\\ documentclass [10pt] {article}\\n\\\\ usepackage {wasysym}\\n\\\\ usepackage [suback] {amsmath}\\n\\\\ usepackage {amsfonts}\\n\\\\ usepackage {amssymb}\\n\\\\ usepackage {amsbsy}\\n\\\\ usepackage [رياضيات.scr] {eucal}\\n\\\\ usepackage {mathrsfs}\\n\\\\ usepackage {pmc}\\n\\\\ usepackage [أويلر] {Upgreek}\\n\\\\ pagestyle {فارغ}\\n\\\\ oddsidemargin -1.0in\\n\\\\ ابدأ {مستند}\\n\\\\ [\\n{\\\\ كال د}\\n \\\\]\\n\\\\ نهاية {المستند}\\n هو التوزيع الأساسي و P [ø (x) ≠ y] هو احتمال التباين بين القيم المتوقعة والفعلية. من خلال أخذ العينات بشكل متكرر من مصدر البيانات المقدم ، والنمذجة وتنفيذ تقييم مقارن للنتائج ، توفر خوارزمية SMA بيئة موحدة مع إمكانية تحقيق نتائج متسقة عبر العينات. بالنسبة لمشاكل التصنيف ، يتم المضي قدمًا من خلال التدريب والتحقق من صحة النموذج في المعادلة 1 على عينات عشوائية ، مما يجعل العينات عديمة الجنسية في جميع التكرارات. وبالتالي ، يتم تركيب نماذج متعددة للتعلم الآلي ومقارنتها وتحديثها على مدى عدة تكرارات ، وفي النهاية يتم اختيار النموذج الأفضل أداءً بناءً على الاحتمالية\\n\\n(2)\\nPΨD، POP≥ΨB، POP = 1⇔EΨD، POP − ΨB، POP = EΔ≥0\\nم 2\\n\\\\ documentclass [10pt] {article}\\n\\\\ usepackage {wasysym}\\n\\\\ usepackage [suback] {amsmath}\\n\\\\ usepackage {amsfonts}\\n\\\\ usepackage {amssymb}\\n\\\\ usepackage {amsbsy}\\n\\\\ usepackage [mathscr] {eucal}\\n\\\\ usepackage {mathrsfs}\\n\\\\ usepackage {pmc}\\n\\\\ usepackage [أويلر] {Upgreek}\\n\\\\ pagestyle {فارغ}\\n\\\\ oddsidemargin -1.0in\\n\\\\ ابدأ {مستند}\\n\\\\ [P \\\\ left ({{\\\\ Psi _ {D، POP}} \\\\ ge {\\\\ Psi _ {B، POP}}} \\\\ right) = 1 \\\\ Leftrightarrow {\\\\ mathbb {E}} \\\\ left [{{\\\\ Psi _ {D، POP}} - {\\\\ Psi _ {B، POP}}} \\\\ right] = {\\\\ mathbb {E}} \\\\ left [\\\\ Delta \\\\ right] \\\\ ge 0 \\\\]\\n\\\\ نهاية {المستند}\\n\\n\\n\\nحيث 𝔼 [Δ] هو الفرق المقدر بين خطأ السكان ψD و POP وخطأ التحقق B ، POP. يتم توضيح تكيف خوارزمية SMA عبر الشبكة العصبية التلافيفية (CNN) - وهي تقنية للتعلم الآلي ، تُستخدم عادةً لتصنيف بيانات الصورة مثل بيانات الأشعة السينية ، في هذه الحالة. أفكاره الأصلية مستمدة من عمل عالم ياباني ، كونيهيكو فوكوشيما (فوكوشيما 1980) ، على نيوكونيترون - شبكة عصبية للتعرف على الصور الأساسية وتم تطويرها من خلال عمل LeCun و Jackel و Boser و Denker و Graf و Guyon و Henderson و Howard & Hubbard (1989) ، LeCun ، Boser ، Denker ، Henderson ، Howard ، Hubbard & Jackel (1989) في شبكة CNN الحديثة عبر تحدي بيانات ImageNet Krizhevsky et al. (2012).\\nيقوم نموذج CNN بالتصنيف بناءً على مدخلات الصورة والمتغير المستهدف للفئات المعروفة للصور. وهي تتكون عادةً من طبقات متعددة من الخلايا العصبية الاصطناعية ، تحاكي الخلايا العصبية البيولوجية ، كما هو موضح بيانياً في الشكل 4. وهي تعالج الحوسبة الالتفافية لميزات الاستخراج متعددة القنوات المدخلة على مستواها.\\n\\n\\nالشكل 4 الشكل المعماري لنموذج سي إن إن.\\n\\nيتم لف كل نواة تلافيفية عبر عرض وارتفاع أحجام الإدخال ثنائية الأبعاد من الطبقة السابقة ، ويتم حساب المنتج النقطي بين النواة والمدخلات. إذا جعلنا X عبارة عن مصفوفة بيانات n × n ومصفوفة W a k × k من الأوزان ، وهي عبارة عن مرشح ثنائي الأبعاد مع k ≤ n (انظر الشكل 5) ، إذن\\n\\n\\nالشكل 5 يتم الحصول على القيم التلافيفية عن طريق تحريك النواة فوق البيانات.\\n\\n\\n(3)\\nXk (i، j) = xi، jxi، j + 1xi، j + 2 ... xi، j + k − 1xi + 1، jxi + 1، j + 1xi + 1، j + 2 ... xi + 1، j + k −1xi + 2، jxi + 2، j + 1xi + 2، j + 2 ... xi + 2، j + k − 1 …………… xi + k − 1، jxi + k − 1، j + 1xi + k −1، j + 2 ... xi + k − 1، j + k − 1\\nم 3\\n\\\\ فئة المستندات [1'"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XerVmLopIgQ_"
      },
      "source": [
        "ds_codata['arabic_translated'] = ds_codata_translated"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dhVqm1vmJ1rv",
        "outputId": "d68101c6-8430-4b52-f1ac-2ed7f1128d88"
      },
      "source": [
        "ds_codata.head()"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Content</th>\n",
              "      <th>URL</th>\n",
              "      <th>arabic_translated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A Framework for Data-Driven Solutions with COV...</td>\n",
              "      <td>1 Introduction\\nDrawn to address global challe...</td>\n",
              "      <td>https://datascience.codata.org/articles/10.533...</td>\n",
              "      <td>1 المقدمة\\nتم تصميم أهداف الأمم المتحدة السبعة...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Interconnecting Systems Using Machine-Actionab...</td>\n",
              "      <td>1 Introduction\\nThe Data Management Plan (DMP)...</td>\n",
              "      <td>https://datascience.codata.org/articles/10.533...</td>\n",
              "      <td>1 المقدمة\\nتم تقديم خطة إدارة البيانات (DMP) ل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Keeping Track of Samples in Multidisciplinary ...</td>\n",
              "      <td>Introduction\\nThe EU directive on Public Secto...</td>\n",
              "      <td>https://datascience.codata.org/articles/10.533...</td>\n",
              "      <td>مقدمة\\nتم استبدال توجيه الاتحاد الأوروبي بشأن ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Towards Globally Unique Identification of Phys...</td>\n",
              "      <td>Introduction: Persistent Identifiers for Sampl...</td>\n",
              "      <td>https://datascience.codata.org/articles/10.533...</td>\n",
              "      <td>مقدمة: المعرفات الثابتة للعينات في البحث\\nالعي...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Application Profile for Machine-Actionable Dat...</td>\n",
              "      <td>1 Introduction\\nData Management Plans (DMPs) a...</td>\n",
              "      <td>https://datascience.codata.org/articles/10.533...</td>\n",
              "      <td>1 المقدمة\\nخطط إدارة البيانات (DMPs) هي وثائق ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        ArticleTitle  ...                                  arabic_translated\n",
              "0  A Framework for Data-Driven Solutions with COV...  ...  1 المقدمة\\nتم تصميم أهداف الأمم المتحدة السبعة...\n",
              "1  Interconnecting Systems Using Machine-Actionab...  ...  1 المقدمة\\nتم تقديم خطة إدارة البيانات (DMP) ل...\n",
              "2  Keeping Track of Samples in Multidisciplinary ...  ...  مقدمة\\nتم استبدال توجيه الاتحاد الأوروبي بشأن ...\n",
              "3  Towards Globally Unique Identification of Phys...  ...  مقدمة: المعرفات الثابتة للعينات في البحث\\nالعي...\n",
              "4  Application Profile for Machine-Actionable Dat...  ...  1 المقدمة\\nخطط إدارة البيانات (DMPs) هي وثائق ...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZiFsxngMQMh"
      },
      "source": [
        "ds_codata.to_csv('/content/datascience_codata_org_Translated_into_arabic.csv') "
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKrPNIdxMkBo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSOLthqWM5p5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtrjItX9NWWO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15P2r8NaNW_O"
      },
      "source": [
        "## Machine learning Mastry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx8uvHN3NW_P"
      },
      "source": [
        "MLM= pd.read_csv(r'/content/machine-learning-mastery-articles.csv')"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "gAcY5sk6NW_P",
        "outputId": "44592447-54b0-47b6-d1af-e996894dbe16"
      },
      "source": [
        "MLM.head()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "      <th>keywords</th>\n",
              "      <th>title</th>\n",
              "      <th>published date</th>\n",
              "      <th>article link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tweet Share Share\\n\\nLast Updated on November ...</td>\n",
              "      <td>Example of vanishing gradient problemTo illust...</td>\n",
              "      <td>['x', 'activation', 'sigmoid', 'model', 'probl...</td>\n",
              "      <td>Visualizing the vanishing gradient problem</td>\n",
              "      <td>2021-11-16 19:00:45+00:00</td>\n",
              "      <td>https://machinelearningmastery.com/visualizing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tweet Share Share\\n\\nLast Updated on November ...</td>\n",
              "      <td>In this tutorial, we are going to look at an e...</td>\n",
              "      <td>['prediction', 'x', 'y_pred', 'import', 'using...</td>\n",
              "      <td>Using CNN for financial time series prediction</td>\n",
              "      <td>2021-11-14 21:00:05+00:00</td>\n",
              "      <td>https://machinelearningmastery.com/using-cnn-f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tweet Share Share\\n\\nWe have already familiari...</td>\n",
              "      <td>After completing this tutorial, you will know:...</td>\n",
              "      <td>['input', 'model', 'sublayer', 'words', 'encod...</td>\n",
              "      <td>The Transformer Model</td>\n",
              "      <td>2021-11-03 19:00:23+00:00</td>\n",
              "      <td>https://machinelearningmastery.com/the-transfo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tweet Share Share\\n\\nBefore the introduction o...</td>\n",
              "      <td>We will first be focusing on the Transformer a...</td>\n",
              "      <td>['attention', 'right', 'left', 'values', 'dots...</td>\n",
              "      <td>The Transformer Attention Mechanism</td>\n",
              "      <td>2021-10-29 21:00:32+00:00</td>\n",
              "      <td>https://machinelearningmastery.com/the-transfo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tweet Share Share\\n\\nLast Updated on October 3...</td>\n",
              "      <td>In the paper they indeed provided the algorith...</td>\n",
              "      <td>['principal', 'component', 'picture', 'eigenfa...</td>\n",
              "      <td>Face Recognition using Principal Component Ana...</td>\n",
              "      <td>2021-10-27 17:00:36+00:00</td>\n",
              "      <td>https://machinelearningmastery.com/face-recogn...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                       article link\n",
              "0  Tweet Share Share\\n\\nLast Updated on November ...  ...  https://machinelearningmastery.com/visualizing...\n",
              "1  Tweet Share Share\\n\\nLast Updated on November ...  ...  https://machinelearningmastery.com/using-cnn-f...\n",
              "2  Tweet Share Share\\n\\nWe have already familiari...  ...  https://machinelearningmastery.com/the-transfo...\n",
              "3  Tweet Share Share\\n\\nBefore the introduction o...  ...  https://machinelearningmastery.com/the-transfo...\n",
              "4  Tweet Share Share\\n\\nLast Updated on October 3...  ...  https://machinelearningmastery.com/face-recogn...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJJVwVQNNW_Q"
      },
      "source": [
        "MLM.text[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZUDkU0_NW_Q"
      },
      "source": [
        "for i in range(len(MLM.Content)):\n",
        "  MLM.Content[i] = str(MLM.Content[i]).replace('Abstract\\r\\n', '')\n",
        "  MLM.Content[i] = re.sub(r'[^a-zA-Z.]', ' ', str(MLM.Content[i]))\n",
        "  MLM.Content[i] = MLM.Content[i].replace('\\n', '')\n",
        "  MLM.Content[i] = MLM.Content[i].replace('  ', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d24qSSH3NW_R"
      },
      "source": [
        "MLM.Content[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myHEAUurNW_R"
      },
      "source": [
        ""
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdhkJMtvQqSa"
      },
      "source": [
        "# def dataset_divider(data, column, no_of_chunks):\n",
        "#   final_list = []\n",
        "#   for par in data[column]: \n",
        "#     start = 0\n",
        "#     d = []\n",
        "#     # chunk = 4950\n",
        "#     n = [4950 for x in range(no_of_chunks)]\n",
        "#     for i in n:\n",
        "#         d.append(par[start:start+i])\n",
        "#         start += i\n",
        "#     d.append(par[start:])\n",
        "#     p = [x for x in d if len(x) > 1] \n",
        "#     final_list.append(p)\n",
        "\n",
        "#   return final_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueg47SnnRhx8"
      },
      "source": [
        "MLM_list = dataset_divider(MLM, 'text', 21)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB_b0JW_NW_R",
        "outputId": "4af4dab3-79cc-4478-c0ad-ab25a36010c7"
      },
      "source": [
        "len(MLM_list)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1141"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL2PwA5XNW_R"
      },
      "source": [
        "# w = []\n",
        "# for i in MLM.text:\n",
        "#   w.append(len(i))\n",
        "\n",
        "# print(max(w))\n",
        "# print(min(w))"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjKAlH60NW_S",
        "outputId": "3021c07c-d63f-483d-ee06-628a7c9674b6"
      },
      "source": [
        "MLM_translated = Translator(MLM_list, 'ar')"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYm13OchNW_T",
        "outputId": "29b5393e-6826-47ea-b994-475973660d57"
      },
      "source": [
        "len(MLM_translated)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1141"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KE0uHZENW_T"
      },
      "source": [
        "MLM_translated[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJoL_lzKNW_T"
      },
      "source": [
        "MLM['arabic_translated'] = MLM_translated"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "5ZhD92IgNJNV",
        "outputId": "580316f3-3cad-4d8a-b933-7dcaef9daae9"
      },
      "source": [
        "MLM.head()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "      <th>keywords</th>\n",
              "      <th>title</th>\n",
              "      <th>published date</th>\n",
              "      <th>article link</th>\n",
              "      <th>arabic_translated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tweet Share Share\\n\\nLast Updated on November ...</td>\n",
              "      <td>Example of vanishing gradient problemTo illust...</td>\n",
              "      <td>['x', 'activation', 'sigmoid', 'model', 'probl...</td>\n",
              "      <td>Visualizing the vanishing gradient problem</td>\n",
              "      <td>2021-11-16 19:00:45+00:00</td>\n",
              "      <td>https://machinelearningmastery.com/visualizing...</td>\n",
              "      <td>سقسقة شارك شارك\\n\\nتم التحديث الأخير في 20 نوف...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tweet Share Share\\n\\nLast Updated on November ...</td>\n",
              "      <td>In this tutorial, we are going to look at an e...</td>\n",
              "      <td>['prediction', 'x', 'y_pred', 'import', 'using...</td>\n",
              "      <td>Using CNN for financial time series prediction</td>\n",
              "      <td>2021-11-14 21:00:05+00:00</td>\n",
              "      <td>https://machinelearningmastery.com/using-cnn-f...</td>\n",
              "      <td>سقسقة شارك شارك\\n\\nتم التحديث الأخير في 20 نوف...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tweet Share Share\\n\\nWe have already familiari...</td>\n",
              "      <td>After completing this tutorial, you will know:...</td>\n",
              "      <td>['input', 'model', 'sublayer', 'words', 'encod...</td>\n",
              "      <td>The Transformer Model</td>\n",
              "      <td>2021-11-03 19:00:23+00:00</td>\n",
              "      <td>https://machinelearningmastery.com/the-transfo...</td>\n",
              "      <td>سقسقة شارك شارك\\n\\nلقد تعرفنا بالفعل على مفهوم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tweet Share Share\\n\\nBefore the introduction o...</td>\n",
              "      <td>We will first be focusing on the Transformer a...</td>\n",
              "      <td>['attention', 'right', 'left', 'values', 'dots...</td>\n",
              "      <td>The Transformer Attention Mechanism</td>\n",
              "      <td>2021-10-29 21:00:32+00:00</td>\n",
              "      <td>https://machinelearningmastery.com/the-transfo...</td>\n",
              "      <td>سقسقة شارك شارك\\n\\nقبل إدخال نموذج المحولات ، ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tweet Share Share\\n\\nLast Updated on October 3...</td>\n",
              "      <td>In the paper they indeed provided the algorith...</td>\n",
              "      <td>['principal', 'component', 'picture', 'eigenfa...</td>\n",
              "      <td>Face Recognition using Principal Component Ana...</td>\n",
              "      <td>2021-10-27 17:00:36+00:00</td>\n",
              "      <td>https://machinelearningmastery.com/face-recogn...</td>\n",
              "      <td>سقسقة شارك شارك\\n\\nتم التحديث الأخير في 30 أكت...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                  arabic_translated\n",
              "0  Tweet Share Share\\n\\nLast Updated on November ...  ...  سقسقة شارك شارك\\n\\nتم التحديث الأخير في 20 نوف...\n",
              "1  Tweet Share Share\\n\\nLast Updated on November ...  ...  سقسقة شارك شارك\\n\\nتم التحديث الأخير في 20 نوف...\n",
              "2  Tweet Share Share\\n\\nWe have already familiari...  ...  سقسقة شارك شارك\\n\\nلقد تعرفنا بالفعل على مفهوم...\n",
              "3  Tweet Share Share\\n\\nBefore the introduction o...  ...  سقسقة شارك شارك\\n\\nقبل إدخال نموذج المحولات ، ...\n",
              "4  Tweet Share Share\\n\\nLast Updated on October 3...  ...  سقسقة شارك شارك\\n\\nتم التحديث الأخير في 30 أكت...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mToHLQVHNJNX"
      },
      "source": [
        "MLM.to_csv('/content/machinelearningmastery_Translated_into_arabic.csv')"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxo8h2mSouV1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}