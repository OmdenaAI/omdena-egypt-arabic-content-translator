{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = Article(\n",
    "    'https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks', language='ar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/site-packages/cryptography/hazmat/backends/openssl/x509.py:15: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "article.download()\n",
    "article.parse()\n",
    "article.nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"مصفوفة الدقّة (confusion matrix) تستخدم مصفوفة الدقّة لأخذ تصور شامل عند تقييم أداء النموذج. وهي تعرّف كالتالي:\\n\\nفي سياق التصنيف الثنائي، هذه المقاييس (metrics) المهمة التي يجدر مراقبتها من أجل تقييم آداء النموذج.\\n\\nمنحنى دقّة الأداء (ROC) منحنى دقّة الآداء، ويطلق عليه ROC، هو رسمة لمعدل التصنيفات الإيجابية الصحيحة (TPR) مقابل معدل التصنيفات الإيجابية الخاطئة (FPR) باستخدام قيم حد (threshold) متغيرة. هذه المقاييس ملخصة في الجدول التالي:\\n\\nالمقاييس الأساسية المقاييس التالية تستخدم في العادة لتقييم أداء نماذج التصنيف:\\n\\nالمساحة تحت منحنى دقة الأداء (المساحة تحت المنحنى) (AUC) المساحة تحت منحنى دقة الأداء (المساحة تحت المنحنى)، ويطلق عليها AUC أو AUROC، هي المساحة تحت ROC كما هو موضح في الرسمة التالية:\\n\\nمقاييس الانحدار\\n\\nالمقاييس الأساسية إذا كان لدينا نموذج الانحدار $f$، فإن المقاييس التالية غالباً ما تستخدم لتقييم أداء النموذج:\\n\\nالمجموع الكلي للمربعات مجموع المربعات المُفسَّر مجموع المربعات المتبقي $\\\\displaystyle\\\\textrm{SS}_{\\\\textrm{tot}}=\\\\sum_{i=1}^m(y_i-\\\\overline{y})^2$ $\\\\displaystyle\\\\textrm{SS}_{\\\\textrm{reg}}=\\\\sum_{i=1}^m(f(x_i)-\\\\overline{y})^2$ $\\\\displaystyle\\\\textrm{SS}_{\\\\textrm{res}}=\\\\sum_{i=1}^m(y_i-f(x_i))^2$\\n\\nمُعامل التحديد (coefficient of determination) مُعامل التحديد، وغالباً يرمز له بـ $R^2$ أو $r^2$، يعطي قياس لمدى مطابقة النموذج للنتائج الملحوظة، ويعرف كما يلي:\\n\\n\\\\[\\\\boxed{R^2=1-\\\\frac{\\\\textrm{SS}_\\\\textrm{res}}{\\\\textrm{SS}_\\\\textrm{tot}}}\\\\]\\n\\nالمقاييس الرئيسية المقاييس التالية تستخدم غالباً لتقييم أداء نماذج الانحدار، وذلك بأن يتم الأخذ في الحسبان عدد المتغيرات $n$ المستخدمة فيها:\\n\\nمعيار معامل مالوس (Mallow's) AIC BIC Adjusted $R^2$ $\\\\displaystyle\\\\frac{\\\\textrm{SS}_{\\\\textrm{res}}+2(n+1)\\\\widehat{\\\\sigma}^2}{m}$ $\\\\displaystyle2\\\\Big[(n+2)-\\\\log(L)\\\\Big]$ $\\\\displaystyle\\\\log(m)(n+2)-2\\\\log(L)$ $\\\\displaystyle1-\\\\frac{(1-R^2)(m-1)}{m-n-1}$\\n\\nحيث $L$ هو الأرجحية، و $\\\\widehat{\\\\sigma}^2$ تقدير التباين الخاص بكل نتيجة.\\n\\nاختيار النموذج\\n\\nمفردات عند اختيار النموذج، نفرق بين 3 أجزاء من البيانات التي لدينا كالتالي:\\n\\nمجموعة تدريب مجموعة تحقق مجموعة اختبار • يتم تدريب النموذج\\n\\n• غالباً 80% من مجموعة البيانات • يتم تقييم النموذج\\n\\n• غالباً 20% من مجموعة البيانات\\n\\n• يطلق عليها كذلك المجموعة المُجنّبة أو مجموعة التطوير • النموذج يعطي التوقعات\\n\\n• بيانات لم يسبق رؤيتها من قبل\\n\\nبمجرد اختيار النموذج، يتم تدريبه على مجموعة البيانات بالكامل ثم يتم اختباره على مجموعة اختبار لم يسبق رؤيتها من قبل. كما هو موضح في الشكل التالي:\\n\\nالتحقق المتقاطع (cross-validation) التحقق المتقاطع، وكذلك يختصر بـ CV، هو طريقة تستخدم لاختيار نموذج بحيث لا يعتمد بشكل كبير على مجموعة بيانات التدريب المبدأية. أنواع التحقق المتقاطع المختلفة ملخصة في الجدول التالي:\\n\\nk-fold Leave-p-out • التدريب على $k-1$ جزء والتقييم باستخدام الجزء الباقي\\n\\n• بشكل عام $k=5$ أو 10 • التدريب على $n-p$ عينة والتقييم باستخدام الـ $p$ عينات المتبقية\\n\\n• الحالة $p=1$ يطلق عليها الإبقاء على واحد (leave-one-out)\\n\\nالطريقة الأكثر استخداماً يطلق عليها التحقق المتقاطع س جزء/أجزاء ($k$-fold)، ويتم فيها تقسيم البيانات إلى $k$ جزء، بحيث يتم تدريب النموذج باستخدام $k-1$ والتحقق باستخدام الجزء المتبقي، ويتم تكرار ذلك $k$ مرة. يتم بعد ذلك حساب معدل الأخطاء في الأجزاء $k$ ويسمى خطأ التحقق المتقاطع.\\n\\nضبط (regularization) عمليه الضبط تهدف إلى تفادي فرط التخصيص (overfit) للنموذج، وهو بذلك يتعامل مع مشاكل التباين العالي. الجدول التالي يلخص أنواع وطرق الضبط الأكثر استخداماً:\\n\\nLASSO Ridge Elastic Net • يقلص المُعاملات إلى 0\\n\\n• جيد لاختيار المتغيرات يجعل المُعاملات أصغر المفاضلة بين اختيار المتغيرات والمُعاملات الصغيرة $...+\\\\lambda||\\\\theta||_1$\\n\\n$\\\\lambda\\\\in\\\\mathbb{R}$ $...+\\\\lambda||\\\\theta||_2^2$\\n\\n$\\\\lambda\\\\in\\\\mathbb{R}$ $...+\\\\lambda\\\\Big[(1-\\\\alpha)||\\\\theta||_1+\\\\alpha||\\\\theta||_2^2\\\\Big]$\\n\\n$\\\\lambda\\\\in\\\\mathbb{R},\\\\alpha\\\\in[0,1]$\\n\\nالتشخيصات\\n\\nالانحياز (bias) الانحياز للنموذج هو الفرق بين التنبؤ المتوقع والنموذج الحقيقي الذي نحاول تنبؤه للبيانات المعطاة.\\n\\nالتباين (variance) تباين النموذج هو مقدار التغير في تنبؤ النموذج لنقاط البيانات المعطاة.\\n\\nموازنة الانحياز/التباين (bias/variance tradeoff) كلما زادت بساطة النموذج، زاد الانحياز، وكلما زاد تعقيد النموذج، زاد التباين.\\n\\n\\n\\nUnderfitting Just right Overfitting الأعراض • خطأ التدريب عالي\\n\\n• خطأ التدريب قريب من خطأ الاختبار\\n\\n• انحياز عالي • خطأ التدريب أقل بقليل من خطأ الاختبار • خطأ التدريب منخفض جداً\\n\\n• خطأ التدريب أقل بكثير من خطأ الاختبار\\n\\n• تباين عالي توضيح الانحدار توضيح التصنيف توضيح التعلم العميق العلاجات الممكنة • زيادة تعقيد النموذج\\n\\n• إضافة المزيد من الخصائص\\n\\n• تدريب لمدة أطول • إجراء الضبط (regularization)\\n\\n• الحصول على المزيد من البيانات\\n\\nتحليل الخطأ تحليل الخطأ هو تحليل السبب الرئيسي للفرق في الأداء بين النماذج الحالية والنماذج المثالية.\\n\\nتحليل استئصالي (ablative analysis) التحليل الاستئصالي هو تحليل السبب الرئيسي للفرق في الأداء بين النماذج الحالية والنماذج المبدئية (baseline).\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning Tips and Tricks Cheatsheet'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tips', 'tricks', 'learning', 'cheatsheet', 'machine']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html lang=en><head><base href=../../ ><title>CS 229 - Machine Learning Tips and Tricks Cheatsheet</title><meta charset=utf-8><meta content=\"Teaching page of Shervine Amidi, Graduate Student at Stanford University.\" name=description><meta content=\"teaching, shervine, shervine amidi, data science\" name=keywords><meta content=\"width=device-width, initial-scale=1\" name=viewport><link href=https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks rel=canonical><link href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css rel=stylesheet><link href=https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css rel=stylesheet><link crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq rel=stylesheet><link href=css/style.min.css?3587b1299365680430aa5634d6b49ffb rel=stylesheet type=text/css><link href=css/article.min.css?6b276411853b0aa728d3d16d218cc10d rel=stylesheet><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js></script><script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js></script><script defer src=https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.9.1/underscore-min.js type=text/javascript></script><script crossorigin=anonymous defer integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js></script><script crossorigin=anonymous defer integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js></script><script src=js/ga.min.js?973cf97267541a888198336699a55cfd></script><script defer src=js/article.min.js?d4501a257815c34bb5963984260f6b39></script><script defer src=js/lang.min.js?49b78d872651f24046a7aff7b8536db5></script><script async defer src=https://buttons.github.io/buttons.js></script></head> <body data-offset=50 data-spy=scroll data-target=.navbar> <nav class=\"navbar navbar-inverse navbar-static-top\"> <div class=container-fluid> <div class=navbar-header> <button class=navbar-toggle data-target=#myNavbar data-toggle=collapse type=button> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a class=navbar-brand href onclick=trackOutboundLink(this);> <img alt=Stanford src=images/stanford-logo.png?f7176222abba492681ca93190e078e48> </a> <p class=navbar-text><font color=#dddddd>Shervine Amidi</font></p> </div> <div class=\"collapse navbar-collapse\" id=myNavbar> <ul class=\"nav navbar-nav\"> <li><a href onclick=trackOutboundLink(this);>About</a></li> </ul> <ul class=\"nav navbar-nav navbar-center\"> <li><a href=projects onclick=trackOutboundLink(this);>Projects</a></li> <li class=active><a href=teaching onclick=trackOutboundLink(this);>Teaching</a></li> <li><a href=blog onclick=trackOutboundLink(this);>Blog</a></li> </ul> <div class=\"collapse navbar-collapse\" data-target=None id=HiddenNavbar> <ul class=\"nav navbar-nav navbar-right\"> <li><a href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this);>About</a></li> <p class=navbar-text><font color=#dddddd>Afshine Amidi</font></p> <a class=navbar-brand href=https://www.mit.edu/~amidi onclick=trackOutboundLink(this); style=\"padding: 0px;\"> <img alt=MIT src=images/mit-logo.png?4f7adbadc5c51293b439c17d7305f96b style=\"padding: 15px 15px; width: 70px; margin-left: 15px; margin-right: 5px;\"> </a> </ul> </div> </div> </div> </nav> <div id=wrapper> <div id=sidebar-wrapper> <div class=sidebar-top> <li class=sidebar-title> <a href=teaching/cs-229 onclick=trackOutboundLink(this);><img alt=Stanford src=images/stanford-logo.png?f7176222abba492681ca93190e078e48 style=\"width: 15px;\">\\xa0\\xa0\\xa0<b>CS 229 - Machine Learning</b></a> </li> <li class=sidebar-brand> <a href=#> <div> <span style=color:white>Tips and tricks</span> </div> </a> </li> </div> <ul class=sidebar-nav> <li> <div class=dropdown-btn><a href=#classification-metrics>Classification metrics</a></div> <div class=dropdown-container> <a href=#classification-metrics><span>Confusion matrix</span></a> <a href=#classification-metrics><span>Accuracy</span></a> <a href=#classification-metrics><span>Precision, recall</span></a> <a href=#classification-metrics><span>F1 score</span></a> <a href=#classification-metrics><span>ROC</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#regression-metrics>Regression metrics</a></div> <div class=dropdown-container> <a href=#regression-metrics><span>R squared</span></a> <a href=#regression-metrics><span>Mallow\\'s CP</span></a> <a href=#regression-metrics><span>AIC, BIC</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#model-selection>Model selection</a></div> <div class=dropdown-container> <a href=#model-selection><span>Cross-validation</span></a> <a href=#model-selection><span>Regularization</span></a> </div> </li> <li> <div class=dropdown-btn><a href=#diagnostics>Diagnostics</a></div> <div class=dropdown-container> <a href=#diagnostics><span>Bias/variance tradeoff</span></a> <a href=#diagnostics><span>Error/ablative analysis</span></a> </div> </li> </ul> <center> <div class=sidebar-footer> <li> <a href=https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-machine-learning-tips-and-tricks.pdf onclick=trackOutboundLink(this); style=\"color: white; text-decoration:none;\"> <i aria-hidden=false class=\"fa fa-github fa-fw\"></i> View PDF version on GitHub </a> </li> </div> </center> </div> <article class=\"markdown-body entry-content\" itemprop=text>\\n<div class=\"alert alert-primary\" role=alert>\\n  Would you like to see this cheatsheet in your native language? You can help us <a class=alert-link href=https://github.com/shervinea/cheatsheet-translation onclick=trackOutboundLink(this);>translating it</a> on GitHub!\\n</div>\\n<div class=title-lang>\\n  <a aria-hidden=true class=anchor-bis href=#cs-229---machine-learning id=cs-229---machine-learning></a><a href=teaching/cs-229 onclick=trackOutboundLink(this);>CS 229 - Machine Learning</a>\\n  \\n  <div style=float:right;>\\n    <div class=input-group>\\n      <select class=form-control onchange=changeLangAndTrack(this); onfocus=storeCurrentIndex(this);>\\n        <option value=ar>العربية</option>\\n        <option selected value=en>English</option>\\n        <option value=es>Español</option>\\n        <option value=fa>فارسی</option>\\n        <option value=fr>Français</option>\\n        <option value=ko>한국어</option>\\n        <option value=pt>Português</option>\\n        <option value=tr>Türkçe</option>\\n        <option value=vi>Tiếng Việt</option>\\n        <option value=zh>简中</option>\\n        <option value=zh-tw>繁中</option>\\n      </select>\\n      <div class=input-group-addon><i class=fa>\\uf0ac</i></div>\\n    </div>\\n  </div>\\n</div>\\n<br>\\n<div aria-label=... class=\"btn-group btn-group-justified\" role=group>\\n  <div class=btn-group role=group>\\n    <button class=\"btn btn-default\" onclick=\"location.href=\\'teaching/cs-229/cheatsheet-supervised-learning\\'\" type=button>Supervised Learning</button>\\n  </div>\\n  <div class=btn-group role=group>\\n    <button class=\"btn btn-default\" onclick=\"location.href=\\'teaching/cs-229/cheatsheet-unsupervised-learning\\'\" type=button>Unsupervised Learning</button>\\n  </div>\\n  <div class=btn-group role=group>\\n    <button class=\"btn btn-default\" onclick=\"location.href=\\'teaching/cs-229/cheatsheet-deep-learning\\'\" type=button>Deep Learning</button>\\n  </div>\\n  <div class=btn-group role=group>\\n    <button class=\"btn btn-default active\" onclick=\"location.href=\\'teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks\\'\" type=button><b>Tips and tricks</b></button>\\n  </div>\\n</div>\\n<h1>\\n  <a aria-hidden=true class=anchor-bis href=#cheatsheet id=user-content-cheatsheet></a>Machine Learning tips and tricks cheatsheet\\n  <div style=float:right><a aria-label=\"Star afshinea/stanford-cs-229-machine-learning on GitHub\" class=\"github-button fa-fw\" data-icon=octicon-star data-show-count=true href=https://github.com/afshinea/stanford-cs-229-machine-learning onclick=trackOutboundLink(this);>Star</a></div>\\n</h1>\\n<p>By <a href=https://twitter.com/afshinea onclick=trackOutboundLink(this);>Afshine Amidi</a> and <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);>Shervine Amidi</a></p>\\n<h2><a aria-hidden=true class=anchor href=#classification-metrics id=classification-metrics></a>Classification metrics</h2>\\n<p>In a context of a binary classification, here are the main metrics that are important to track in order to assess the performance of the model.</p>\\n<p><span class=\"new-item item-b\">Confusion matrix</span> The confusion matrix is used to have a more complete picture when assessing the performance of a model. It is defined as follows:</p>\\n<div class=mobile-container>\\n<table style=\"margin-left: 10%\">\\n<colgroup><col width=120px>\\n<col width=50px>\\n<col width=200px>\\n<col width=200px>\\n</colgroup><tbody>\\n<tr style=\"border-top: 0px\">\\n<td align=center colspan=2 rowspan=2 style=\"background: rgba(255, 255, 255, 1);border: 0px\"></td>\\n<td align=center colspan=2 style=\"background: rgba(255, 255, 255, 1);border: 0px\"><b>Predicted</b> class</td>\\n</tr>\\n<tr style=\"border-top: 0px\">\\n<td align=center style=\"background: rgba(255, 255, 255, 1);border: 0px\"><b>+</b></td>\\n<td align=center style=\"background: rgba(255, 255, 255, 1);border: 0px\"><b>-</b></td>\\n</tr>\\n<tr style=\"border-top: 0px\">\\n<td align=center rowspan=2 style=\"border: 0px\"><b>Actual</b> class</td>\\n<td align=center style=\"background: rgba(255, 255, 255, 1);border: 0px;\"><b>+</b></td>\\n<td align=center style=\"background: rgba(0, 255, 0, 0.15);border: 0px;\"><b>TP</b><br>True Positives</td>\\n<td align=center style=\"background: rgba(255, 0, 0, 0.25);border: 0px;\"><b>FN</b><br>False Negatives<br>Type II error</td>\\n</tr>\\n<tr style=\"border-top: 0px\">\\n<td align=center style=\"background: rgba(255, 255, 255, 1);border: 0px;\"><b>-</b></td>\\n<td align=center style=\"background: rgba(255, 0, 0, 0.25);border: 0px;\"><b>FP</b><br>False Positives<br>Type I error</td>\\n<td align=center style=\"background: rgba(0, 255, 0, 0.15);border: 0px;\"><b>TN</b><br>True Negatives</td>\\n</tr>\\n</tbody>\\n</table>\\n</div>\\n<br>\\n<p><span class=\"new-item item-r\">Main metrics</span> The following metrics are commonly used to assess the performance of classification models:</p>\\n<div class=mobile-container>\\n<center>\\n<table>\\n<tbody>\\n<tr>\\n<td align=center><b>Metric</b></td>\\n<td align=center><b>Formula</b></td>\\n<td align=center><b>Interpretation</b></td>\\n</tr>\\n<tr>\\n<td align=center>Accuracy</td>\\n<td align=center>$\\\\displaystyle\\\\frac{\\\\textrm{TP}+\\\\textrm{TN}}{\\\\textrm{TP}+\\\\textrm{TN}+\\\\textrm{FP}+\\\\textrm{FN}}$</td>\\n<td align=left>Overall performance of model</td>\\n</tr>\\n<tr>\\n<td align=center>Precision</td>\\n<td align=center>$\\\\displaystyle\\\\frac{\\\\textrm{TP}}{\\\\textrm{TP}+\\\\textrm{FP}}$</td>\\n<td align=left>How accurate the positive predictions are</td>\\n</tr>\\n<tr>\\n<td align=center>Recall<br>Sensitivity</td>\\n<td align=center>$\\\\displaystyle\\\\frac{\\\\textrm{TP}}{\\\\textrm{TP}+\\\\textrm{FN}}$</td>\\n<td align=left>Coverage of actual positive sample</td>\\n</tr>\\n<tr>\\n<td align=center>Specificity</td>\\n<td align=center>$\\\\displaystyle\\\\frac{\\\\textrm{TN}}{\\\\textrm{TN}+\\\\textrm{FP}}$</td>\\n<td align=left>Coverage of actual negative sample</td>\\n</tr>\\n<tr>\\n<td align=center>F1 score</td>\\n<td align=center>$\\\\displaystyle\\\\frac{2\\\\textrm{TP}}{2\\\\textrm{TP}+\\\\textrm{FP}+\\\\textrm{FN}}$</td>\\n<td align=left>Hybrid metric useful for unbalanced classes</td>\\n</tr>\\n</tbody>\\n</table>\\n</center>\\n</div>\\n<br>\\n<p><span class=\"new-item item-r\">ROC</span> The receiver operating curve, also noted ROC, is the plot of TPR versus FPR by varying the threshold. These metrics are are summed up in the table below:</p>\\n<div class=mobile-container>\\n<center>\\n<table>\\n<tbody>\\n<tr>\\n<td align=center><b>Metric</b></td>\\n<td align=center><b>Formula</b></td>\\n<td align=center><b>Equivalent</b></td>\\n</tr>\\n<tr>\\n<td align=center>True Positive Rate<br>TPR</td>\\n<td align=center>$\\\\displaystyle\\\\frac{\\\\textrm{TP}}{\\\\textrm{TP}+\\\\textrm{FN}}$</td>\\n<td align=left>Recall, sensitivity</td>\\n</tr>\\n<tr>\\n<td align=center>False Positive Rate<br>FPR</td>\\n<td align=center>$\\\\displaystyle\\\\frac{\\\\textrm{FP}}{\\\\textrm{TN}+\\\\textrm{FP}}$</td>\\n<td align=left>1-specificity</td>\\n</tr>\\n</tbody>\\n</table>\\n</center>\\n</div>\\n<br>\\n<p><span class=\"new-item item-r\">AUC</span> The area under the receiving operating curve, also noted AUC or AUROC, is the area below the ROC as shown in the following figure:</p>\\n<br>\\n<div class=mobile-container>\\n<center>\\n<img alt=\"ROC AUC\" class=img-responsive src=teaching/cs-229/illustrations/roc-auc-en.png?d2dda6ec6e15f3e05597c058de976702 style=width:100%;max-width:700px>\\n</center>\\n</div>\\n<br>\\n<h2><a aria-hidden=true class=anchor href=#regression-metrics id=regression-metrics></a>Regression metrics</h2>\\n<p><span class=\"new-item item-b\">Basic metrics</span> Given a regression model $f$, the following metrics are commonly used to assess the performance of the model:</p>\\n<div class=mobile-container>\\n<center>\\n<table>\\n<colgroup><col width=33%>\\n<col width=33%>\\n<col width=33%>\\n</colgroup><tbody>\\n<tr>\\n<td align=center><b>Total sum of squares</b></td>\\n<td align=center><b>Explained sum of squares</b></td>\\n<td align=center><b>Residual sum of squares</b></td>\\n</tr>\\n<tr>\\n<td align=center>$\\\\displaystyle\\\\textrm{SS}_{\\\\textrm{tot}}=\\\\sum_{i=1}^m(y_i-\\\\overline{y})^2$</td>\\n<td align=center>$\\\\displaystyle\\\\textrm{SS}_{\\\\textrm{reg}}=\\\\sum_{i=1}^m(f(x_i)-\\\\overline{y})^2$</td>\\n<td align=center>$\\\\displaystyle\\\\textrm{SS}_{\\\\textrm{res}}=\\\\sum_{i=1}^m(y_i-f(x_i))^2$</td>\\n</tr>\\n</tbody>\\n</table>\\n</center>\\n</div>\\n<br>\\n<p><span class=\"new-item item-b\">Coefficient of determination</span> The coefficient of determination, often noted $R^2$ or $r^2$, provides a measure of how well the observed outcomes are replicated by the model and is defined as follows:</p>\\n\\\\[\\\\boxed{R^2=1-\\\\frac{\\\\textrm{SS}_\\\\textrm{res}}{\\\\textrm{SS}_\\\\textrm{tot}}}\\\\]\\n<br>\\n<p><span class=\"new-item item-r\">Main metrics</span> The following metrics are commonly used to assess the performance of regression models, by taking into account the number of variables $n$ that they take into consideration:</p>\\n<div class=mobile-container>\\n<center>\\n<table>\\n<colgroup><col width=25%>\\n<col width=25%>\\n<col width=25%>\\n<col width=25%>\\n</colgroup><tbody>\\n<tr>\\n<td align=center><b>Mallow\\'s Cp</b></td>\\n<td align=center><b>AIC</b></td>\\n<td align=center><b>BIC</b></td>\\n<td align=center><b>Adjusted $R^2$</b></td>\\n</tr>\\n<tr>\\n<td align=center>$\\\\displaystyle\\\\frac{\\\\textrm{SS}_{\\\\textrm{res}}+2(n+1)\\\\widehat{\\\\sigma}^2}{m}$</td>\\n<td align=center>$\\\\displaystyle2\\\\Big[(n+2)-\\\\log(L)\\\\Big]$</td>\\n<td align=center>$\\\\displaystyle\\\\log(m)(n+2)-2\\\\log(L)$</td>\\n<td align=center>$\\\\displaystyle1-\\\\frac{(1-R^2)(m-1)}{m-n-1}$</td>\\n</tr>\\n</tbody>\\n</table>\\n</center>\\n</div>\\n<p>where $L$ is the likelihood and $\\\\widehat{\\\\sigma}^2$ is an estimate of the variance associated with each response.</p>\\n<br>\\n<h2><a aria-hidden=true class=anchor href=#model-selection id=model-selection></a>Model selection</h2>\\n<p><span class=\"new-item item-b\">Vocabulary</span> When selecting a model, we distinguish 3 different parts of the data that we have as follows:</p>\\n<div class=mobile-container>\\n<center>\\n<table>\\n<colgroup><col width=33%>\\n<col width=33%>\\n<col width=33%>\\n</colgroup><tbody>\\n<tr>\\n<td align=center><b>Training set</b></td>\\n<td align=center><b>Validation set</b></td>\\n<td align=center><b>Testing set</b></td>\\n</tr>\\n<tr>\\n<td align=left>• Model is trained<br>\\n                 • Usually 80% of the dataset</td>\\n<td align=left>• Model is assessed<br>\\n                 • Usually 20% of the dataset<br>\\n                 • Also called hold-out or development set</td>\\n<td align=left>• Model gives predictions<br>\\n                 • Unseen data</td>\\n</tr>\\n</tbody>\\n</table>\\n</center>\\n</div>\\n<p>Once the model has been chosen, it is trained on the entire dataset and tested on the unseen test set. These are represented in the figure below:</p>\\n<div class=mobile-container>\\n<center>\\n<img alt=\"Partition of the dataset\" class=img-responsive src=teaching/cs-229/illustrations/train-val-test-en.png?0949795ac868562e193efdc249ae1066 style=width:100%;max-width:550px>\\n</center>\\n</div>\\n<br>\\n<p><span class=\"new-item item-g\">Cross-validation</span> Cross-validation, also noted CV, is a method that is used to select a model that does not rely too much on the initial training set. The different types are summed up in the table below:</p>\\n<div class=mobile-container>\\n<center>\\n<table>\\n<colgroup><col width=50%>\\n<col width=50%>\\n</colgroup><tbody>\\n<tr>\\n<td align=center><b>k-fold</b></td>\\n<td align=center><b>Leave-p-out</b></td>\\n</tr>\\n<tr>\\n<td align=left>• Training on $k-1$ folds and assessment on the remaining one<br>\\n                 • Generally $k=5$ or $10$</td>\\n<td align=left>• Training on $n-p$ observations and assessment on the $p$ remaining ones<br>\\n                 • Case $p=1$ is called leave-one-out</td>\\n</tr>\\n</tbody>\\n</table>\\n</center>\\n</div>\\n<p>The most commonly used method is called $k$-fold cross-validation and splits the training data into $k$ folds to validate the model on one fold while training the model on the $k-1$ other folds, all of this $k$ times. The error is then averaged over the $k$ folds and is named cross-validation error.</p>\\n<div class=mobile-container>\\n<center>\\n<img alt=Cross-validation class=img-responsive src=teaching/cs-229/illustrations/cross-validation-en.png?0f7ada4dc141d0af6b12bb21cc431471 style=width:100%;max-width:770px>\\n</center>\\n</div>\\n<br>\\n<p><span class=\"new-item item-r\">Regularization</span> The regularization procedure aims at avoiding the model to overfit the data and thus deals with high variance issues. The following table sums up the different types of commonly used regularization techniques:</p>\\n<div class=mobile-container>\\n<center>\\n<table style=\"table-layout:fixed; width:100%; min-width:760px;\">\\n  <colgroup>\\n    <col style=width:33%>\\n    <col style=width:33%>\\n    <col style=width:33%>\\n  </colgroup>\\n<tbody>\\n<tr>\\n<td align=center><b>LASSO</b></td>\\n<td align=center><b>Ridge</b></td>\\n<td align=center><b>Elastic Net</b></td>\\n</tr>\\n<tr>\\n<td align=left>• Shrinks coefficients to 0<br>• Good for variable selection</td>\\n<td align=left>Makes coefficients smaller</td>\\n<td align=left>Tradeoff between variable selection and small coefficients</td>\\n</tr>\\n<tr>\\n<td align=center><img alt=Lasso class=img-responsive src=teaching/cs-229/illustrations/lasso.png?ad67282f00fc8b2a529e5b15a856f91b></td>\\n<td align=center><img alt=Ridge class=img-responsive src=teaching/cs-229/illustrations/ridge.png?77abafe4253433af93fb8ffc7d4f6bc7></td>\\n<td align=center><img alt=\"Elastic Net\" class=img-responsive src=teaching/cs-229/illustrations/elastic-net.png?8cd93eb9df1b6ae667d8eb69d20bf4a1></td>\\n</tr>\\n<tr>\\n<td align=left>$...+\\\\lambda||\\\\theta||_1$<br>$\\\\lambda\\\\in\\\\mathbb{R}$</td>\\n<td align=left>$...+\\\\lambda||\\\\theta||_2^2$<br>$\\\\lambda\\\\in\\\\mathbb{R}$</td>\\n<td align=left>$...+\\\\lambda\\\\Big[(1-\\\\alpha)||\\\\theta||_1+\\\\alpha||\\\\theta||_2^2\\\\Big]$<br>$\\\\lambda\\\\in\\\\mathbb{R},\\\\alpha\\\\in[0,1]$</td>\\n</tr>\\n</tbody>\\n</table>\\n</center>\\n</div>\\n<br>\\n<h2><a aria-hidden=true class=anchor href=#diagnostics id=diagnostics></a>Diagnostics</h2>\\n<p><span class=\"new-item item-b\">Bias</span> The bias of a model is the difference between the expected prediction and the correct model that we try to predict for given data points.</p>\\n<br>\\n<p><span class=\"new-item item-b\">Variance</span> The variance of a model is the variability of the model prediction for given data points.</p>\\n<br>\\n<p><span class=\"new-item item-r\">Bias/variance tradeoff</span> The simpler the model, the higher the bias, and the more complex the model, the higher the variance.</p>\\n<div class=mobile-container>\\n<center>\\n<br>\\n<table style=\"table-layout:fixed; width:100%; min-width:735px;\">\\n  <colgroup>\\n    <col style=width:19%>\\n    <col style=width:27%>\\n    <col style=width:27%>\\n    <col style=width:27%>\\n  </colgroup>\\n<tbody>\\n<tr>\\n<td align=center></td>\\n<td align=center><b>Underfitting</b></td>\\n<td align=center><b>Just right</b></td>\\n<td align=center><b>Overfitting</b></td>\\n</tr>\\n<tr>\\n<td align=center><b>Symptoms</b></td>\\n<td align=left>• High training error <br>• Training error close to test error <br>• High bias</td>\\n<td align=left style=vertical-align:top>• Training error slightly lower than test error</td>\\n<td align=left>• Very low training error <br>• Training error much lower than test error <br>• High variance</td>\\n</tr>\\n<tr>\\n<td align=center><b>Regression illustration</b></td>\\n<td align=center><img alt=\"Underfit in regression\" class=img-responsive src=teaching/cs-229/illustrations/regression-underfit.png?cd7fd5e4d334aa31d323b7d3f994a9df></td>\\n<td align=center><img alt=\"Right fit in regression\" class=img-responsive src=teaching/cs-229/illustrations/regression-just-right.png?4018112cca66bdc10de14e7e59702a32></td>\\n<td align=center><img alt=\"Overfit in regression\" class=img-responsive src=teaching/cs-229/illustrations/regression-overfit.png?4133157c77ba0baff93110d1d0e73382></td>\\n</tr>\\n<tr>\\n<td align=center><b>Classification illustration</b></td>\\n<td align=center><img alt=\"Underfit in classification\" class=img-responsive src=teaching/cs-229/illustrations/classification-underfit.png?1f014836b68bfc74da1d767ca32783a3></td>\\n<td align=center><img alt=\"Right fit in classification\" class=img-responsive src=teaching/cs-229/illustrations/classification-just-right.png?67a70eb8c5fb11dd75d8e5035714a435></td>\\n<td align=center><img alt=\"Overfit in classification\" class=img-responsive src=teaching/cs-229/illustrations/classification-overfit.png?64516ea4ed89dad5b422380ea1f8f350></td>\\n</tr>\\n<tr>\\n<td align=center><b>Deep learning illustration</b></td>\\n<td align=center><img alt=\"Underfit in deep learning\" class=img-responsive src=teaching/cs-229/illustrations/deep-learning-underfit-en.png?028697ac0c3ffff2a7a64edbab4dd61a></td>\\n<td align=center><img alt=\"Right fit in deep learning\" class=img-responsive src=teaching/cs-229/illustrations/deep-learning-just-right-en.png?5accb2fc5bfed4b0e328fa798b1dd47c></td>\\n<td align=center><img alt=\"Overfit in deep learning\" class=img-responsive src=teaching/cs-229/illustrations/deep-learning-overfit-en.png?026ff9256072273492de106ed7a9857f></td>\\n</tr>\\n<tr>\\n<td align=center><b>Possible remedies</b></td>\\n<td align=left>• Complexify model<br>• Add more features<br>• Train longer</td>\\n<td align=center style=\"background: rgba(234, 234, 234, 1);\"></td>\\n<td align=left>• Perform regularization<br>• Get more data</td>\\n</tr>\\n</tbody>\\n</table>\\n</center>\\n</div>\\n<br>\\n<p><span class=\"new-item item-b\">Error analysis</span> Error analysis is analyzing the root cause of the difference in performance between the current and the perfect models.</p>\\n<br>\\n<p><span class=\"new-item item-b\">Ablative analysis</span> Ablative analysis is analyzing the root cause of the difference in performance between the current and the baseline models.</p>\\n<br>\\n</article> </div> <footer class=footer> <div class=footer id=contact> <div class=container> <a href=https://twitter.com/shervinea onclick=trackOutboundLink(this);><i class=\"fa fa-twitter fa-3x fa-fw\"></i></a> <a href=https://linkedin.com/in/shervineamidi onclick=trackOutboundLink(this);><i class=\"fa fa-linkedin fa-3x fa-fw\"></i></a> <a href=https://github.com/shervinea onclick=trackOutboundLink(this);><i class=\"fa fa-github fa-3x fa-fw\"></i></a> <a href=\"https://scholar.google.com/citations?user=nMnMTm8AAAAJ\" onclick=trackOutboundLink(this);><i class=\"fa fa-google fa-3x fa-fw\"></i></a> <a class=crptdml data-domain=stanford data-name=shervine data-tld=edu href=#mail onclick=\"trackOutboundLink(this); window.location.href = \\'mailto:\\' + this.dataset.name + \\'@\\' + this.dataset.domain + \\'.\\' + this.dataset.tld\"><i class=\"fa fa-envelope fa-3x fa-fw\"></i></a> </div> </div> </footer> </body></html>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, title, summary, keywords, html, lang, publish_date = [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "def get_article_data(url: str, en: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Appends following lists containing the following:\n",
    "    - article_text\n",
    "    - article_title\n",
    "    - article_summary\n",
    "    - article_keywords\n",
    "    - article_html\n",
    "    - article_language\n",
    "    - article_publish_date\n",
    "    args:\n",
    "    - url: str - url of the article\n",
    "    - en: bool - True if the article is in english, False otherwise(Arabic)\n",
    "    \"\"\"\n",
    "    article = Article(url, language=('en' if en else 'ar'))\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "\n",
    "    text.append(article.text)\n",
    "    title.append(article.title)\n",
    "    summary.append(article.summary)\n",
    "    keywords.append(article.keywords)\n",
    "    html.append(article.html)\n",
    "    lang.append('en' if en else 'ar')\n",
    "    publish_date.append(article.publish_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [\n",
    "    'https://stanford.edu/~shervine/l/ar/teaching/cs-229/cheatsheet-supervised-learning',\n",
    "    'https://stanford.edu/~shervine/l/ar/teaching/cs-229/cheatsheet-unsupervised-learning',\n",
    "    'https://stanford.edu/~shervine/l/ar/teaching/cs-229/cheatsheet-deep-learning',\n",
    "    'https://stanford.edu/~shervine/l/ar/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks',\n",
    "    'https://stanford.edu/~shervine/l/ar/teaching/cs-229/refresher-probabilities-statistics',\n",
    "    'https://stanford.edu/~shervine/l/ar/teaching/cs-229/refresher-algebra-calculus'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(links: list) -> None:\n",
    "    \"\"\"\n",
    "    Scrapes the links in the list and appends the data to the lists\n",
    "    args:\n",
    "    - links: list - list of links to scrape\n",
    "    \"\"\"\n",
    "    for link in tqdm(links):\n",
    "        get_article_data(link, False)\n",
    "        en_link =  link.split('l/ar/')[0] + link.split('l/ar/')[1]\n",
    "        get_article_data(en_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5433079b9a8146f1be79a21ddc945baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scrape_links(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['تعلّم موجَّه',\n",
       " 'Supervised Learning Cheatsheet',\n",
       " 'تعلم غير موجَّه',\n",
       " 'Unsupervised Learning Cheatsheet',\n",
       " 'التعلم العميق',\n",
       " 'Deep Learning Cheatsheet',\n",
       " 'نصائح وحيل',\n",
       " 'Machine Learning Tips and Tricks Cheatsheet',\n",
       " 'تعلّم موجَّه',\n",
       " 'Supervised Learning Cheatsheet',\n",
       " 'تعلم غير موجَّه',\n",
       " 'Unsupervised Learning Cheatsheet',\n",
       " 'التعلم العميق',\n",
       " 'Deep Learning Cheatsheet',\n",
       " 'نصائح وحيل',\n",
       " 'Machine Learning Tips and Tricks Cheatsheet',\n",
       " 'الاحتمالات والإحصائيات',\n",
       " 'Probabilities and Statistics refresher',\n",
       " 'الجبر الخطي والتفاضل',\n",
       " 'Linear Algebra and Calculus refresher']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheats_df = pd.DataFrame({\n",
    "    'text' : text,\n",
    "    'summary' : summary,\n",
    "    'keywords' : keywords,\n",
    "    'lang' : lang,\n",
    "    'title' : title,\n",
    "    'html' :  html,\n",
    "    'publish_date' : publish_date\n",
    "})\n",
    "cheats_df.to_csv('cheats_stanford.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
